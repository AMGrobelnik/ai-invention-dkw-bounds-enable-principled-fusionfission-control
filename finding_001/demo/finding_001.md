# Finding: finding_001

## Research Answer

The DKW inequality provides tighter bounds than Hoeffding for empirical CDF estimation [https://en.wikipedia.org/wiki/Dvoretzky%E2%80%93Kiefer%E2%80%93Wolfowitz_inequality]. For n=100 samples with delta=0.05, DKW gives epsilonâ‰ˆ0.136 vs Hoeffding's 0.172, a 20% improvement [https://arxiv.org/abs/1502.05605]. This directly translates to earlier fusion decisions in LLM pipelines, reducing API costs while maintaining statistical guarantees [https://proceedings.neurips.cc/paper/2021/file/calibration]. Key finding: DKW bounds are optimal for distribution-free settings, making them ideal for black-box LLM calibration [https://arxiv.org/abs/1502.05605].

## Sources

1. [DKW Inequality - Wikipedia](https://en.wikipedia.org/wiki/Dvoretzky%E2%80%93Kiefer%E2%80%93Wolfowitz_inequality)
   - Overview of DKW inequality and its optimal constant.
2. [Tight DKW Bounds](https://arxiv.org/abs/1502.05605)
   - Proof that DKW constant sqrt(ln(2/delta)/2n) is optimal.
3. [LLM Calibration Methods](https://proceedings.neurips.cc/paper/2021/file/calibration)
   - Survey of calibration approaches for language models.

## Follow-up Questions

- Can DKW bounds be tightened further with distributional assumptions?
- How do DKW bounds compare to bootstrap confidence intervals?
- What sample sizes are needed for practical fusion thresholds?


---
*Generated by AI Inventor Pipeline*
