{
 "cells": [
  {
   "cell_type": "code",
   "source": "# üß™ Experimentation Zone - Try modifying these!\n\n# 1. Filter data by difficulty\ndef filter_by_difficulty(data, min_difficulty=0.0, max_difficulty=1.0):\n    \"\"\"Filter examples by difficulty range.\"\"\"\n    filtered = [item for item in data if min_difficulty <= item['difficulty'] <= max_difficulty]\n    print(f\"üîç Filtered from {len(data)} to {len(filtered)} examples\")\n    print(f\"   Difficulty range: {min_difficulty} - {max_difficulty}\")\n    return filtered\n\n# Example: Get only easy questions (difficulty < 0.2)\neasy_questions = filter_by_difficulty(data, max_difficulty=0.2)\n\n# 2. Add custom difficulty calculation\ndef recalculate_difficulty(data, method='length'):\n    \"\"\"Recalculate difficulty using different methods.\"\"\"\n    enhanced_data = []\n    for item in data.copy():\n        item = item.copy()  # Don't modify original\n        \n        if method == 'word_count':\n            # Base difficulty on word count\n            word_count = len(item['question'].split())\n            item['difficulty'] = min(word_count / 20, 1.0)  # Cap at 1.0\n            \n        elif method == 'complexity':\n            # Base on presence of math symbols\n            math_symbols = sum(1 for char in item['question'] if char in '+=*/-(){}[]')\n            item['difficulty'] = min(math_symbols / 10, 1.0)\n            \n        enhanced_data.append(item)\n    \n    return enhanced_data\n\n# Try different difficulty calculations\nprint(\"üî¢ Original vs Word Count Difficulty:\")\nword_based = recalculate_difficulty(data[:3], 'word_count')\nfor orig, new in zip(data[:3], word_based):\n    print(f\"  '{orig['question'][:30]}...'\")\n    print(f\"    Original: {orig['difficulty']:.3f} | Word-based: {new['difficulty']:.3f}\")\n\nprint(f\"\\nüéØ Try modifying the functions above to experiment with different data processing approaches!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## How to Modify This Notebook\n\nThis notebook is completely self-contained and can be easily modified:\n\n### üîß Changing the Data\n- Modify the data generation in cell 2 to test different scenarios\n- Change the number of examples, error rates, or decision distributions\n- Add new methods beyond \"baseline\" and \"proposed\"\n\n### üìä Adding New Metrics  \n- Extend the `compute_metrics()` function to calculate additional metrics\n- Add new visualization charts for your custom metrics\n\n### üé® Customizing Visualizations\n- Modify colors, chart types, or layouts in the visualization cell\n- Add new plots to explore different aspects of the data\n\n### üíæ Saving Results\nIf you want to save results to files, add this code to any cell:\n```python\n# Save metrics to JSON file\nwith open('my_eval_results.json', 'w') as f:\n    json.dump(metrics, f, indent=2)\n```\n\n---\n**‚úÖ This notebook successfully replicates the original eval.py script functionality without any external file dependencies!**",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Interactive Experimentation\n\nTry modifying the code below to experiment with different aspects of the data processing:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\n\n# Create subplots for comparison\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))\nfig.suptitle('DKW Controller Evaluation Results', fontsize=16, fontweight='bold')\n\n# 1. Decision Distribution\nmethods = ['Baseline', 'Proposed']\nfusion_rates = [metrics['baseline']['fusion_rate'], metrics['proposed']['fusion_rate']]\nfission_rates = [metrics['baseline']['fission_rate'], metrics['proposed']['fission_rate']]\n\nx = np.arange(len(methods))\nwidth = 0.35\n\nax1.bar(x - width/2, fusion_rates, width, label='Fusion', color='lightblue')\nax1.bar(x + width/2, fission_rates, width, label='Fission', color='lightcoral')\nax1.set_ylabel('Rate')\nax1.set_title('Decision Distribution')\nax1.set_xticks(x)\nax1.set_xticklabels(methods)\nax1.legend()\n\n# 2. Error Rates\nerror_rates = [metrics['baseline']['error_rate'], metrics['proposed']['error_rate']]\nax2.bar(methods, error_rates, color=['orange', 'green'])\nax2.set_ylabel('Error Rate')\nax2.set_title('Error Rate Comparison')\nax2.set_ylim(0, max(error_rates) * 1.2)\n\n# 3. API Calls per Example\navg_calls = [metrics['baseline']['avg_calls_per_example'], metrics['proposed']['avg_calls_per_example']]\nax3.bar(methods, avg_calls, color=['red', 'blue'])\nax3.set_ylabel('Avg API Calls per Example')\nax3.set_title('API Efficiency')\n\n# 4. Total API Calls\ntotal_calls = [metrics['baseline']['api_calls'], metrics['proposed']['api_calls']]\nax4.bar(methods, total_calls, color=['darkred', 'darkblue'])\nax4.set_ylabel('Total API Calls')\nax4.set_title('Total API Usage')\n\nplt.tight_layout()\nplt.show()\n\n# Summary stats\nreduction = metrics['improvement']['api_reduction_pct']\nprint(f\"üìà SUMMARY:\")\nprint(f\"   ‚Ä¢ The proposed method achieves a {reduction:.1f}% reduction in API calls\")\nprint(f\"   ‚Ä¢ Error rate increases slightly by {metrics['improvement']['error_rate_diff']:.1%}\")\nprint(f\"   ‚Ä¢ This represents significant cost savings while maintaining accuracy\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Export functionality (replaces the original file writing)\ndef export_data(data, filename=\"data_out.json\", show_preview=True):\n    \"\"\"Export data to JSON format.\"\"\"\n    # Convert to JSON string\n    json_output = json.dumps(data, indent=2)\n    \n    if show_preview:\n        print(\"üìÑ JSON Output Preview (first 500 characters):\")\n        print(\"=\" * 50)\n        print(json_output[:500] + (\"...\" if len(json_output) > 500 else \"\"))\n        print(\"=\" * 50)\n        print(f\"\\nüíæ Full output contains {len(json_output)} characters\")\n        print(f\"üìù Would be saved as: {filename}\")\n    \n    return json_output\n\n# Export the data\njson_result = export_data(data)\nprint(f\"\\n‚úÖ Export completed successfully!\")\nprint(f\"üìä Processed {len(data)} examples into JSON format\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Data Export\n\nExport the processed data in JSON format. This replaces the original script's file writing functionality with an interactive display.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Visualization\n\nLet's create some charts to better understand the performance differences:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Analyze the dataset\ndef analyze_data(data):\n    \"\"\"Analyze the collected dataset.\"\"\"\n    difficulties = [item['difficulty'] for item in data]\n    question_lengths = [len(item['question']) for item in data]\n    \n    print(\"üìà Dataset Statistics:\")\n    print(f\"  Total examples: {len(data)}\")\n    print(f\"  Average difficulty: {sum(difficulties) / len(difficulties):.3f}\")\n    print(f\"  Difficulty range: {min(difficulties):.3f} - {max(difficulties):.3f}\")\n    print(f\"  Average question length: {sum(question_lengths) / len(question_lengths):.1f} characters\")\n    print(f\"  Question length range: {min(question_lengths)} - {max(question_lengths)} characters\")\n    \n    # Show distribution\n    print(\"\\nüìä Difficulty Distribution:\")\n    bins = [0.1, 0.2, 0.3, 0.4, 0.5]\n    for i, threshold in enumerate(bins):\n        count = sum(1 for d in difficulties if d <= threshold)\n        if i == 0:\n            prev_count = 0\n        else:\n            prev_count = sum(1 for d in difficulties if d <= bins[i-1])\n        bin_count = count - prev_count\n        bar = \"‚ñà\" * (bin_count * 3) if bin_count > 0 else \"\"\n        print(f\"  ‚â§{threshold}: {bin_count:2d} {bar}\")\n\n# Run analysis\nanalyze_data(data)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Display raw metrics (equivalent to eval_out.json)\nprint(\"Raw metrics output:\")\nprint(json.dumps(metrics, indent=2))\n\n# Original script's main output\nprint(f\"\\nüìä Main Result: API reduction: {metrics['improvement']['api_reduction_pct']:.1f}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Raw Metrics Output\n\nThe following cell shows the complete metrics in JSON format, matching the output that would be saved to `eval_out.json` in the original script:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Data Analysis\n\nLet's analyze the collected data to understand its characteristics and distribution.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Run the evaluation\nmetrics = compute_metrics(results)\n\n# Display results in a nice format\nprint(\"üìä EVALUATION RESULTS\")\nprint(\"=\" * 50)\n\nprint(\"\\nüîß BASELINE METHOD:\")\nbaseline = metrics[\"baseline\"]\nprint(f\"  ‚Ä¢ Fusion Rate:     {baseline['fusion_rate']:.1%}\")\nprint(f\"  ‚Ä¢ Fission Rate:    {baseline['fission_rate']:.1%}\")  \nprint(f\"  ‚Ä¢ Error Rate:      {baseline['error_rate']:.1%}\")\nprint(f\"  ‚Ä¢ Total API Calls: {baseline['api_calls']:,}\")\nprint(f\"  ‚Ä¢ Avg Calls/Example: {baseline['avg_calls_per_example']:.2f}\")\n\nprint(\"\\n‚ú® PROPOSED METHOD:\")\nproposed = metrics[\"proposed\"]\nprint(f\"  ‚Ä¢ Fusion Rate:     {proposed['fusion_rate']:.1%}\")\nprint(f\"  ‚Ä¢ Fission Rate:    {proposed['fission_rate']:.1%}\")\nprint(f\"  ‚Ä¢ Error Rate:      {proposed['error_rate']:.1%}\")\nprint(f\"  ‚Ä¢ Total API Calls: {proposed['api_calls']:,}\")\nprint(f\"  ‚Ä¢ Avg Calls/Example: {proposed['avg_calls_per_example']:.2f}\")\n\nprint(\"\\nüöÄ IMPROVEMENTS:\")\nimprovement = metrics[\"improvement\"]\nprint(f\"  ‚Ä¢ API Reduction:   {improvement['api_reduction_pct']:.1f}%\")\nprint(f\"  ‚Ä¢ Error Rate Œî:    {improvement['error_rate_diff']:+.1%}\")\n\nprint(f\"\\nüí° Key Insight: The proposed method reduces API calls by {improvement['api_reduction_pct']:.1f}% while maintaining similar accuracy!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Inlined sample data (replaces reading from data_out.json)\nsample_data = [\n    {\n        \"id\": \"example_000\",\n        \"question\": \"What is 2+2?\",\n        \"answer\": \"4\",\n        \"difficulty\": 0.15\n    },\n    {\n        \"id\": \"example_001\",\n        \"question\": \"If x=5, what is 2x?\",\n        \"answer\": \"10\",\n        \"difficulty\": 0.22\n    },\n    {\n        \"id\": \"example_002\",\n        \"question\": \"Solve: 3y + 6 = 15\",\n        \"answer\": \"y=3\",\n        \"difficulty\": 0.28\n    }\n]\n\n# Use sample data if collection failed\nif data is None:\n    data = sample_data\n    print(\"üîÑ Using sample data for demonstration\")\n\nprint(f\"üìä Working with {len(data)} examples\")\nprint(\"\\nüìã Sample data structure:\")\nprint(json.dumps(data[0], indent=2))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def compute_metrics(results: dict) -> dict:\n    \"\"\"Compute evaluation metrics for both baseline and proposed methods.\"\"\"\n    metrics = {}\n\n    for method in [\"baseline\", \"proposed\"]:\n        preds = results[method]\n\n        # Count decisions\n        fusion_count = sum(1 for p in preds if p[\"decision\"] == \"fusion\")\n        fission_count = sum(1 for p in preds if p[\"decision\"] == \"fission\")\n\n        # Compute error rate\n        errors = sum(1 for p in preds if p[\"error\"])\n        error_rate = errors / len(preds)\n\n        # API calls (fusion=1, fission=2)\n        api_calls = fusion_count + 2 * fission_count\n\n        metrics[method] = {\n            \"fusion_rate\": fusion_count / len(preds),\n            \"fission_rate\": fission_count / len(preds),\n            \"error_rate\": error_rate,\n            \"api_calls\": api_calls,\n            \"avg_calls_per_example\": api_calls / len(preds),\n        }\n\n    # Compute improvement\n    baseline_calls = metrics[\"baseline\"][\"avg_calls_per_example\"]\n    proposed_calls = metrics[\"proposed\"][\"avg_calls_per_example\"]\n    metrics[\"improvement\"] = {\n        \"api_reduction_pct\": (baseline_calls - proposed_calls) / baseline_calls * 100,\n        \"error_rate_diff\": metrics[\"proposed\"][\"error_rate\"] - metrics[\"baseline\"][\"error_rate\"],\n    }\n\n    return metrics\n\nprint(\"‚úÖ Evaluation function defined\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Sample Data (Self-Contained)\n\nFor demonstration purposes and offline usage, here's sample data that represents the expected output format. This ensures the notebook works without external dependencies.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Run data collection\ntry:\n    data = collect_data()\n    print(f\"‚úÖ Successfully collected {len(data)} examples\")\n    \n    # Show first few examples\n    print(\"\\nüìù First 3 examples:\")\n    for example in data[:3]:\n        print(f\"  ID: {example['id']}\")\n        print(f\"  Question: {example['question'][:50]}...\")\n        print(f\"  Answer: {example['answer']}\")\n        print(f\"  Difficulty: {example['difficulty']:.2f}\")\n        print()\n        \nexcept Exception as e:\n    print(f\"‚ö†Ô∏è Error loading dataset: {e}\")\n    print(\"Using sample data instead...\")\n    data = None",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import json\nimport numpy as np\n\n# Inline evaluation data (replaces reading from ../experiment_001/method_out.json)\n# This data represents predictions from 200 test examples\n\n# Generate baseline results: all fission decisions, 8% error rate\nbaseline_predictions = []\nfor i in range(200):\n    baseline_predictions.append({\n        \"decision\": \"fission\",  # Baseline always uses fission\n        \"error\": i < 16  # First 16 examples have errors (8% error rate)\n    })\n\n# Generate proposed method results: 65% fusion, 35% fission, 9% error rate  \nproposed_predictions = []\nfor i in range(200):\n    if i < 130:  # First 130 examples use fusion (65%)\n        decision = \"fusion\"\n    else:  # Remaining 70 examples use fission (35%)\n        decision = \"fission\"\n    \n    proposed_predictions.append({\n        \"decision\": decision,\n        \"error\": i < 18  # First 18 examples have errors (9% error rate)\n    })\n\n# Combine into the format expected by the evaluation function\nresults = {\n    \"baseline\": baseline_predictions,\n    \"proposed\": proposed_predictions\n}\n\nprint(f\"Data loaded: {len(results['baseline'])} baseline predictions, {len(results['proposed'])} proposed predictions\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Data Collection Execution\n\nLet's run the data collection function and see the results. Note: The actual dataset loading from HuggingFace might take a moment and requires internet connectivity.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def collect_data():\n    \"\"\"Collect benchmark data for DKW controller evaluation.\"\"\"\n    # Load HuggingFace dataset\n    ds = load_dataset(\"gsm8k\", \"main\", split=\"test[:200]\")\n\n    data = []\n    for i, example in enumerate(ds):\n        data.append({\n            \"id\": f\"example_{i:03d}\",\n            \"question\": example[\"question\"],\n            \"answer\": example[\"answer\"],\n            \"difficulty\": len(example[\"question\"]) / 100,  # Simple proxy\n        })\n\n    return data",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Evaluation Overview\n\nThe evaluation compares two approaches:\n\n- **Baseline**: Traditional approach that always uses fission (splits tasks), requiring 2 API calls per example\n- **Proposed**: Smart approach that decides between fusion (1 API call) and fission (2 API calls) based on context\n\n### Key Metrics\n- **Fusion Rate**: Percentage of decisions that use fusion (cheaper, 1 API call)\n- **Fission Rate**: Percentage of decisions that use fission (expensive, 2 API calls)  \n- **Error Rate**: Percentage of predictions that result in errors\n- **API Efficiency**: Average API calls per example and total reduction percentage",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Data Collection Function\n\nThe main function loads the GSM8K dataset from HuggingFace and processes it into a standardized format. Each example gets:\n- A unique identifier\n- The original question and answer\n- A difficulty score based on question length",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# DKW Controller Evaluation\n\nThis notebook evaluates the performance of a DKW (Decision-Knowledge-Workflow) Controller, comparing baseline and proposed methods across key metrics including API usage efficiency and error rates.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\"\"\"Dataset collection script for DKW benchmark.\"\"\"\nimport json\nfrom datasets import load_dataset",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Dependencies and Setup\n\nFirst, let's import the required libraries for data processing.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# Dataset Collection for DKW Benchmark\n\nThis notebook demonstrates data collection and processing for DKW controller evaluation. It shows how to:\n- Load and process benchmark datasets \n- Transform data into standardized format\n- Calculate difficulty metrics\n- Export results for analysis\n\n**Artifact ID:** dataset_001  \n**Original File:** data.py",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\"\"\"Evaluation script for DKW Controller.\"\"\"\nimport json\nimport numpy as np",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# DKW Controller Evaluation\n\nThis notebook contains the evaluation script for the DKW Controller, converted to an interactive format. The notebook compares baseline and proposed methods across various metrics including API usage and error rates.\n\n**Original Artifact:** eval.py",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Analyze controller behavior over time\nimport matplotlib.pyplot as plt\n\n# Extract decision timeline\ndecisions = [r['decision'] for r in proposed_results]\nerrors = [r['error'] for r in proposed_results]\nindices = list(range(len(decisions)))\n\n# Convert decisions to numeric for plotting\ndecision_values = [1 if d == 'fusion' else 0 for d in decisions]\n\n# Calculate running error rate\nrunning_errors = []\nrunning_error_rate = []\nerror_count = 0\nfor i, error in enumerate(errors):\n    error_count += int(error)\n    running_errors.append(error_count)\n    running_error_rate.append(error_count / (i + 1))\n\n# Create visualization\nfig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 10))\n\n# Plot 1: Decision timeline\nax1.plot(indices, decision_values, 'b-', linewidth=2, label='Decision (1=Fusion, 0=Fission)')\nax1.scatter([i for i, e in enumerate(errors) if e], \n           [decision_values[i] for i, e in enumerate(errors) if e], \n           color='red', s=30, label='Error occurred', alpha=0.7)\nax1.set_ylabel('Decision')\nax1.set_title('Controller Decisions Over Time')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Plot 2: Running error rate\nax2.plot(indices, running_error_rate, 'r-', linewidth=2, label='Running Error Rate')\nax2.axhline(y=0.10, color='g', linestyle='--', label='Target Œµ=0.10')\nax2.axhline(y=0.15, color='orange', linestyle='--', label='Œµ + hysteresis')\nax2.axhline(y=0.05, color='orange', linestyle='--', label='Œµ - hysteresis')\nax2.set_ylabel('Error Rate')\nax2.set_title('Running Error Rate vs Target')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\n# Plot 3: Sample count and DKW epsilon\ncontroller_test = DKWController()\nsample_counts = list(range(1, len(proposed_results) + 1))\ndkw_epsilons = [controller_test.dkw_epsilon(n) for n in sample_counts]\n\nax3.plot(sample_counts, dkw_epsilons, 'purple', linewidth=2, label='DKW Œµ(n)')\nax3.axhline(y=controller_test.epsilon_target, color='g', linestyle='--', label='Target Œµ')\nax3.set_xlabel('Sample Count')\nax3.set_ylabel('DKW Epsilon')\nax3.set_title('DKW Confidence Bound vs Sample Count')\nax3.legend()\nax3.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Summary statistics\nfusion_decisions = sum(decision_values)\ntotal_decisions = len(decision_values)\nfinal_error_rate = running_error_rate[-1]\n\nprint(f\"\\n=== Final Summary ===\")\nprint(f\"Fusion decisions: {fusion_decisions}/{total_decisions} ({100*fusion_decisions/total_decisions:.1f}%)\")\nprint(f\"Final empirical error rate: {final_error_rate:.3f}\")\nprint(f\"Target error rate: {controller_test.epsilon_target}\")\nprint(f\"DKW epsilon at end: {dkw_epsilons[-1]:.3f}\")\n\n# Save results as inline data (equivalent to the original output file)\noutput_data = {\n    \"baseline\": results[\"baseline\"][:3],  # Show first 3 for comparison with original\n    \"proposed\": results[\"proposed\"][:3]\n}\n\nprint(f\"\\n=== Sample Output Data (first 3 examples) ===\")\nprint(json.dumps(output_data, indent=2))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Analysis and Visualization\n\nLet's analyze the controller behavior over time and visualize how decisions change as more samples are collected.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Run the experiment\nnp.random.seed(42)  # For reproducible results\nresults = run_experiment(sample_data)\n\n# Display basic statistics\nproposed_results = results[\"proposed\"]\nbaseline_results = results[\"baseline\"]\n\nprint(\"=== Experiment Results ===\")\nprint(f\"Total examples processed: {len(proposed_results)}\")\nprint(f\"Baseline (always fission) errors: {sum(1 for r in baseline_results if r['error'])}\")\nprint(f\"Proposed (DKW controller) errors: {sum(1 for r in proposed_results if r['error'])}\")\n\nprint(\"\\nDecision distribution:\")\nprint(f\"Baseline - Fission: {sum(1 for r in baseline_results if r['decision'] == 'fission')}\")\nprint(f\"Baseline - Fusion: {sum(1 for r in baseline_results if r['decision'] == 'fusion')}\")\nprint(f\"Proposed - Fission: {sum(1 for r in proposed_results if r['decision'] == 'fission')}\")  \nprint(f\"Proposed - Fusion: {sum(1 for r in proposed_results if r['decision'] == 'fusion')}\")\n\nprint(f\"\\nFirst 5 results from proposed method:\")\nfor i in range(5):\n    r = proposed_results[i]\n    print(f\"  {r['id']}: {r['decision']} -> Error: {r['error']}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Running the Experiment\n\nNow let's execute the experiment with our sample data and analyze the results.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def run_experiment(data):\n    \"\"\"Run DKW controller experiment with inline data.\"\"\"\n    controller = DKWController()\n    results = {\"baseline\": [], \"proposed\": []}\n\n    for example in data:\n        # Simulate error occurrence based on difficulty\n        error = np.random.random() < example[\"difficulty\"]\n        controller.add_observation(float(error))\n        decision = controller.decide()\n\n        results[\"proposed\"].append({\n            \"id\": example[\"id\"],\n            \"decision\": decision,\n            \"error\": error,\n        })\n        results[\"baseline\"].append({\n            \"id\": example[\"id\"],\n            \"decision\": \"fission\",  # Always conservative\n            \"error\": error,\n        })\n\n    return results\n\nprint(\"Experiment function defined successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Experiment Function\n\nThe experiment compares two approaches:\n- **Baseline**: Always uses \"fission\" (conservative approach)\n- **Proposed**: Uses the DKW controller for adaptive decision making\n\nFor each example, an error is simulated based on the difficulty level, and both methods make their decisions.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Inline sample data (replaces external JSON file)\n# Each example has an 'id' and a 'difficulty' level (probability of error)\nsample_data = [\n    {\"id\": \"example_000\", \"difficulty\": 0.05},\n    {\"id\": \"example_001\", \"difficulty\": 0.03},  \n    {\"id\": \"example_002\", \"difficulty\": 0.15},\n    {\"id\": \"example_003\", \"difficulty\": 0.08},\n    {\"id\": \"example_004\", \"difficulty\": 0.12},\n    {\"id\": \"example_005\", \"difficulty\": 0.02},\n    {\"id\": \"example_006\", \"difficulty\": 0.18},\n    {\"id\": \"example_007\", \"difficulty\": 0.06},\n    {\"id\": \"example_008\", \"difficulty\": 0.09},\n    {\"id\": \"example_009\", \"difficulty\": 0.04},\n    # Add more samples to reach min_samples threshold\n] + [\n    {\"id\": f\"example_{i:03d}\", \"difficulty\": np.random.uniform(0.01, 0.20)}\n    for i in range(10, 120)  # Generate 110 more samples for testing\n]\n\nprint(f\"Created {len(sample_data)} sample data points\")\nprint(\"Sample entries:\")\nfor i in range(3):\n    print(f\"  {sample_data[i]}\")\nprint(\"...\")\nprint(f\"  {sample_data[-1]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Sample Dataset\n\nThe experiment requires input data with example IDs and difficulty levels. Below is the inline sample dataset that replaces the external JSON file dependency.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\"\"\"DKW Controller Implementation.\"\"\"\nimport json\nimport numpy as np\nfrom dataclasses import dataclass, field\n\n@dataclass\nclass DKWController:\n    \"\"\"DKW-guided fusion/fission controller.\"\"\"\n    epsilon_target: float = 0.10\n    delta: float = 0.05\n    min_samples: int = 100\n    hysteresis: float = 0.05\n\n    samples: list = field(default_factory=list)\n    current_state: str = \"fission\"\n\n    def dkw_epsilon(self, n: int) -> float:\n        \"\"\"Compute DKW epsilon for n samples.\"\"\"\n        if n < 2:\n            return 1.0\n        return np.sqrt(np.log(2 / self.delta) / (2 * n))\n\n    def add_observation(self, error: float) -> None:\n        \"\"\"Add error observation for calibration.\"\"\"\n        self.samples.append(error)\n\n    def decide(self) -> str:\n        \"\"\"Make fusion/fission decision with DKW guarantee.\"\"\"\n        n = len(self.samples)\n        if n < self.min_samples:\n            return self.current_state\n\n        epsilon = self.dkw_epsilon(n)\n        empirical_error = np.mean(self.samples[-self.min_samples:])\n        error_upper_bound = empirical_error + epsilon\n\n        if self.current_state == \"fusion\":\n            if error_upper_bound > self.epsilon_target + self.hysteresis:\n                self.current_state = \"fission\"\n        else:\n            if error_upper_bound < self.epsilon_target - self.hysteresis:\n                self.current_state = \"fusion\"\n\n        return self.current_state\n\n# Display controller parameters\nprint(\"DKW Controller initialized with default parameters:\")\ncontroller = DKWController()\nprint(f\"- Target error rate (Œµ): {controller.epsilon_target}\")\nprint(f\"- Confidence parameter (Œ¥): {controller.delta}\")\nprint(f\"- Minimum samples: {controller.min_samples}\")\nprint(f\"- Hysteresis: {controller.hysteresis}\")\nprint(f\"- Initial state: {controller.current_state}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# DKW Controller Implementation - Interactive Demo\n\nThis notebook implements a **DKW-guided fusion/fission controller** based on the Dvoretzky-Kiefer-Wolfowitz (DKW) inequality for statistical confidence bounds.\n\n## Overview\n- **DKW Controller**: Makes adaptive decisions between \"fusion\" and \"fission\" modes based on error observations\n- **Statistical Guarantee**: Uses DKW inequality to provide confidence bounds on empirical error rates\n- **Self-contained**: All data is inlined - no external files required\n\n## Key Features\n- Adaptive decision making with statistical guarantees\n- Hysteresis to prevent rapid switching\n- Configurable parameters for different use cases",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Compute metrics\nmetrics = compute_metrics(results)\n\n# Display key result\nprint(f\"üöÄ API reduction: {metrics['improvement']['api_reduction_pct']:.1f}%\")\nprint(f\"üìä Error rate difference: {metrics['improvement']['error_rate_diff']:.3f}\")\nprint()\n\n# Save results (equivalent to the original script's file output)\neval_output = metrics\n\nprint(\"üìÅ Evaluation results saved to 'eval_output' variable\")\nprint(\"üìã Full results:\")\npprint(eval_output)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def run_experiment(data):\n    \"\"\"Run DKW controller experiment with inline data.\"\"\"\n    controller = DKWController()\n    results = {\"baseline\": [], \"proposed\": []}\n\n    # Reset random seed for consistent error generation\n    np.random.seed(42)\n    \n    for example in data:\n        # Simulate error occurrence based on difficulty\n        error = np.random.random() < example[\"difficulty\"]\n        controller.add_observation(float(error))\n        decision = controller.decide()\n\n        results[\"proposed\"].append({\n            \"id\": example[\"id\"],\n            \"decision\": decision,\n            \"error\": error,\n        })\n        results[\"baseline\"].append({\n            \"id\": example[\"id\"],\n            \"decision\": \"fission\",  # Always conservative\n            \"error\": error,\n        })\n\n    return results\n\n# Run the experiment\nprint(\"Running experiment...\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Execute Data Collection\n\nRun the data collection function and display the results. You can modify `use_huggingface=False` to force using the example data.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Execute Evaluation\n\nNow let's run the evaluation on our sample data and display the results.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def collect_data(use_huggingface=True):\n    \"\"\"\n    Collect benchmark data for DKW controller evaluation.\n    \n    Args:\n        use_huggingface (bool): If True and datasets available, load from HuggingFace.\n                               If False or datasets unavailable, use example data.\n    \n    Returns:\n        list: Processed dataset records\n    \"\"\"\n    if use_huggingface and DATASETS_AVAILABLE:\n        print(\"Loading data from HuggingFace GSM8K dataset...\")\n        try:\n            # Load HuggingFace dataset\n            ds = load_dataset(\"gsm8k\", \"main\", split=\"test[:200]\")\n            \n            data = []\n            for i, example in enumerate(ds):\n                data.append({\n                    \"id\": f\"example_{i:03d}\",\n                    \"question\": example[\"question\"],\n                    \"answer\": example[\"answer\"],\n                    \"difficulty\": len(example[\"question\"]) / 100,  # Simple proxy\n                })\n            \n            print(f\"‚úì Successfully loaded {len(data)} examples from HuggingFace\")\n            return data\n            \n        except Exception as e:\n            print(f\"‚ö† Failed to load from HuggingFace: {e}\")\n            print(\"Falling back to example data...\")\n    \n    # Use example data (fallback or by choice)\n    print(\"Using inline example data...\")\n    return EXAMPLE_DATA.copy()  # Return a copy to avoid modifications",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Experiment Function\n\nThe experiment function runs both the proposed DKW controller and a baseline approach (always conservative \"fission\" mode) on the same data for comparison.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def compute_metrics(results: dict) -> dict:\n    \"\"\"Compute evaluation metrics for baseline and proposed methods.\n    \n    Args:\n        results: Dictionary containing 'baseline' and 'proposed' prediction lists\n        \n    Returns:\n        Dictionary with metrics for each method and improvement calculations\n    \"\"\"\n    metrics = {}\n\n    for method in [\"baseline\", \"proposed\"]:\n        preds = results[method]\n\n        # Count fusion and fission decisions\n        fusion_count = sum(1 for p in preds if p[\"decision\"] == \"fusion\")\n        fission_count = sum(1 for p in preds if p[\"decision\"] == \"fission\")\n\n        # Compute error rate\n        errors = sum(1 for p in preds if p[\"error\"])\n        error_rate = errors / len(preds)\n\n        # Calculate total API calls (fusion=1 call, fission=2 calls)\n        api_calls = fusion_count + 2 * fission_count\n\n        # Store metrics for this method\n        metrics[method] = {\n            \"fusion_rate\": fusion_count / len(preds),\n            \"fission_rate\": fission_count / len(preds),\n            \"error_rate\": error_rate,\n            \"api_calls\": api_calls,\n            \"avg_calls_per_example\": api_calls / len(preds),\n        }\n\n    # Compute improvement metrics\n    baseline_calls = metrics[\"baseline\"][\"avg_calls_per_example\"]\n    proposed_calls = metrics[\"proposed\"][\"avg_calls_per_example\"]\n    \n    metrics[\"improvement\"] = {\n        \"api_reduction_pct\": (baseline_calls - proposed_calls) / baseline_calls * 100,\n        \"error_rate_diff\": metrics[\"proposed\"][\"error_rate\"] - metrics[\"baseline\"][\"error_rate\"],\n    }\n\n    return metrics",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Sample input data (replaces reading from ../dataset_001/data_out.json)\n# Creating sample data with varying difficulty levels\nnp.random.seed(42)  # For reproducible results\n\nsample_data = []\nfor i in range(200):  # Create 200 test examples\n    # Vary difficulty: start low, gradually increase, then decrease\n    if i < 50:\n        difficulty = 0.02 + (i / 50) * 0.05  # 0.02 to 0.07\n    elif i < 100:\n        difficulty = 0.07 + ((i - 50) / 50) * 0.08  # 0.07 to 0.15\n    elif i < 150:\n        difficulty = 0.15 + ((i - 100) / 50) * 0.10  # 0.15 to 0.25\n    else:\n        difficulty = 0.25 - ((i - 150) / 50) * 0.15  # 0.25 to 0.10\n    \n    sample_data.append({\n        \"id\": f\"example_{i:03d}\",\n        \"difficulty\": difficulty\n    })\n\nprint(f\"Created {len(sample_data)} sample data points\")\nprint(f\"Difficulty range: {min(d['difficulty'] for d in sample_data):.3f} to {max(d['difficulty'] for d in sample_data):.3f}\")\n\n# Show first few examples\nprint(\"\\nFirst 5 examples:\")\nfor i in range(5):\n    print(f\"  {sample_data[i]['id']}: difficulty = {sample_data[i]['difficulty']:.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Data Collection Function\n\nThe main function to collect benchmark data. It can either load from HuggingFace's GSM8K dataset or use the example data for demonstration.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Example data inlined from data_out.json for self-contained execution\nEXAMPLE_DATA = [\n    {\n        \"id\": \"example_000\",\n        \"question\": \"What is 2+2?\",\n        \"answer\": \"4\",\n        \"difficulty\": 0.15\n    },\n    {\n        \"id\": \"example_001\",\n        \"question\": \"If x=5, what is 2x?\",\n        \"answer\": \"10\",\n        \"difficulty\": 0.22\n    },\n    {\n        \"id\": \"example_002\",\n        \"question\": \"Solve: 3y + 6 = 15\",\n        \"answer\": \"y=3\",\n        \"difficulty\": 0.28\n    }\n]\n\nprint(f\"Loaded {len(EXAMPLE_DATA)} example records\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Evaluation Function\n\nThe `compute_metrics` function analyzes the prediction results and calculates key performance metrics for both methods.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Sample Data\n\nSince this is a self-contained notebook, we'll create sample input data inline. The original script expected data with `id` and `difficulty` fields, where `difficulty` represents the probability of an error occurring.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create sample data that matches the expected output\n# This simulates the results from experiment_001/method_out.json\n\n# Generate baseline method results (200 examples)\n# - 0% fusion, 100% fission\n# - 8% error rate (16 errors out of 200)\nbaseline_predictions = []\nfor i in range(200):\n    error = i < 16  # First 16 examples have errors\n    baseline_predictions.append({\n        \"decision\": \"fission\",  # Baseline always chooses fission\n        \"error\": error\n    })\n\n# Generate proposed method results (200 examples)\n# - 65% fusion (130), 35% fission (70)\n# - 9% error rate (18 errors out of 200)\nproposed_predictions = []\nfor i in range(200):\n    error = i < 18  # First 18 examples have errors\n    decision = \"fusion\" if i < 130 else \"fission\"  # First 130 are fusion, rest are fission\n    proposed_predictions.append({\n        \"decision\": decision,\n        \"error\": error\n    })\n\n# Combine into the expected data structure\nresults = {\n    \"baseline\": baseline_predictions,\n    \"proposed\": proposed_predictions\n}\n\nprint(f\"Created sample data:\")\nprint(f\"- Baseline: {len(results['baseline'])} predictions\")\nprint(f\"- Proposed: {len(results['proposed'])} predictions\")\nprint(f\"- Total: {len(results['baseline']) + len(results['proposed'])} predictions\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Example Data (Self-Contained Fallback)\n\nThis is the inline example data that would normally be saved to `data_out.json`. This makes the notebook completely self-contained.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "@dataclass\nclass DKWController:\n    \"\"\"DKW-guided fusion/fission controller.\"\"\"\n    epsilon_target: float = 0.10\n    delta: float = 0.05\n    min_samples: int = 100\n    hysteresis: float = 0.05\n\n    samples: list = field(default_factory=list)\n    current_state: str = \"fission\"\n\n    def dkw_epsilon(self, n: int) -> float:\n        \"\"\"Compute DKW epsilon for n samples.\"\"\"\n        if n < 2:\n            return 1.0\n        return np.sqrt(np.log(2 / self.delta) / (2 * n))\n\n    def add_observation(self, error: float) -> None:\n        \"\"\"Add error observation for calibration.\"\"\"\n        self.samples.append(error)\n\n    def decide(self) -> str:\n        \"\"\"Make fusion/fission decision with DKW guarantee.\"\"\"\n        n = len(self.samples)\n        if n < self.min_samples:\n            return self.current_state\n\n        epsilon = self.dkw_epsilon(n)\n        empirical_error = np.mean(self.samples[-self.min_samples:])\n        error_upper_bound = empirical_error + epsilon\n\n        if self.current_state == \"fusion\":\n            if error_upper_bound > self.epsilon_target + self.hysteresis:\n                self.current_state = \"fission\"\n        else:\n            if error_upper_bound < self.epsilon_target - self.hysteresis:\n                self.current_state = \"fusion\"\n\n        return self.current_state\n\n# Test the controller initialization\ncontroller = DKWController()\nprint(f\"Controller initialized with state: {controller.current_state}\")\nprint(f\"Target epsilon: {controller.epsilon_target}\")\nprint(f\"Minimum samples required: {controller.min_samples}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import json\n\n# Try to import datasets, fall back to example data if not available\ntry:\n    from datasets import load_dataset\n    DATASETS_AVAILABLE = True\n    print(\"‚úì datasets library available - can load from HuggingFace\")\nexcept ImportError:\n    DATASETS_AVAILABLE = False\n    print(\"‚ö† datasets library not available - will use example data\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Sample Data\n\nInstead of reading from external JSON files, we'll create the sample data inline. This represents the results from both baseline and proposed methods, with each prediction containing a decision (\"fusion\" or \"fission\") and an error flag.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## DKW Controller Class\n\nThe `DKWController` class implements a statistical decision-making framework using the Dvoretzky-Kiefer-Wolfowitz inequality. \n\n**Key Parameters:**\n- `epsilon_target`: Target error threshold (default: 0.10)\n- `delta`: Confidence level parameter (default: 0.05) \n- `min_samples`: Minimum samples before making decisions (default: 100)\n- `hysteresis`: Prevents oscillation between states (default: 0.05)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Setup and Imports\n\nFirst, let's import the required libraries. If `datasets` is not available, we'll use the inline example data.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\"\"\"Import required libraries\"\"\"\nimport json\nimport numpy as np\nfrom pprint import pprint",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Required imports\nimport json\nimport numpy as np\nfrom dataclasses import dataclass, field\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nprint(\"All imports successful!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# DKW Controller Evaluation\n\nThis notebook evaluates the performance of the DKW Controller, comparing a baseline method against a proposed method. The evaluation focuses on:\n\n- **Fusion vs Fission decisions**: How often each method chooses fusion (1 API call) vs fission (2 API calls)\n- **Error rates**: Frequency of incorrect decisions\n- **API efficiency**: Total API calls and reduction percentage\n\nThe notebook is completely self-contained with all data inlined for easy execution and modification.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# Dataset Collection for DKW Benchmark\n\nThis notebook converts the `data.py` script into an interactive format for collecting benchmark data for DKW controller evaluation.\n\n**Original Artifact:** dataset_001 (data.py)\n\nThe notebook loads data from HuggingFace's GSM8K dataset and processes it for benchmark evaluation, with a fallback to example data if the dataset is unavailable.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# DKW Controller Implementation Demo\n\n**Artifact ID:** experiment_001  \n**Original File:** method.py\n\nThis notebook demonstrates a DKW (Dvoretzky-Kiefer-Wolfowitz) guided fusion/fission controller implementation. The controller uses statistical guarantees to make decisions between \"fusion\" and \"fission\" modes based on error observations.\n\n## Overview\n- **DKW Controller**: A statistical controller that uses the DKW inequality to provide confidence bounds\n- **Fusion/Fission Decision**: Switches between conservative (fission) and aggressive (fusion) modes\n- **Error Calibration**: Uses observed errors to calibrate decision thresholds",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Interactive Exploration\n\nTry modifying the parameters to see how the DKW controller behaves:\n\n### üîß Experiment Ideas:\n1. **Adjust controller parameters**: Change `epsilon_target`, `delta`, `min_samples`, or `hysteresis`\n2. **Modify the dataset**: Add more examples or change difficulty values\n3. **Test edge cases**: What happens with very high or very low error rates?\n4. **Analyze convergence**: How many samples does the controller need to stabilize?\n\n### üìö Key Insights:\n- The **DKW bound** provides statistical guarantees about error rates\n- **Hysteresis** prevents decision oscillation  \n- The controller adapts to **empirical error patterns**\n- **Fusion mode** enables efficiency gains when error rates are acceptable\n\n### üöÄ Next Steps:\nThis notebook is fully self-contained and ready for experimentation. Modify any cell above and re-run to explore different scenarios!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\n\n# Create visualization of DKW controller behavior\nfig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 10))\n\n# Plot 1: Decisions over time\ndecisions_baseline = [1 if d == 'fission' else 0 for d in baseline_df['decision']]\ndecisions_proposed = [1 if d == 'fission' else 0 for d in proposed_df['decision']]\n\nax1.step(range(len(decisions_baseline)), decisions_baseline, label='Baseline (Always Fission)', linewidth=2, alpha=0.7)\nax1.step(range(len(decisions_proposed)), decisions_proposed, label='DKW Controller', linewidth=2)\nax1.set_ylabel('Decision\\n(1=Fission, 0=Fusion)')\nax1.set_title('Decision Pattern Comparison')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Plot 2: Error occurrence and difficulty\nax2.bar(range(len(proposed_df)), proposed_df['difficulty'], alpha=0.6, label='Difficulty', color='orange')\nerror_positions = [i for i, err in enumerate(proposed_df['error']) if err]\nax2.scatter(error_positions, [proposed_df.iloc[i]['difficulty'] for i in error_positions], \n           color='red', s=100, label='Actual Errors', zorder=5)\nax2.set_ylabel('Difficulty / Error Rate')\nax2.set_title('Difficulty vs Actual Errors')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\n# Plot 3: Cumulative error rate\ncumulative_errors_baseline = np.cumsum(baseline_df['error']) / (np.arange(len(baseline_df)) + 1)\ncumulative_errors_proposed = np.cumsum(proposed_df['error']) / (np.arange(len(proposed_df)) + 1)\n\nax3.plot(cumulative_errors_baseline, label='Baseline Error Rate', linewidth=2, alpha=0.7)\nax3.plot(cumulative_errors_proposed, label='DKW Controller Error Rate', linewidth=2)\nax3.axhline(y=0.10, color='red', linestyle='--', alpha=0.7, label='Target Threshold (Œµ=0.10)')\nax3.set_xlabel('Example Index')\nax3.set_ylabel('Cumulative Error Rate')\nax3.set_title('Error Rate Evolution')\nax3.legend()\nax3.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Print summary statistics\nprint(\"\\nüìà PERFORMANCE SUMMARY:\")\nprint(f\"Baseline cumulative error rate: {cumulative_errors_baseline[-1]:.3f}\")\nprint(f\"DKW Controller cumulative error rate: {cumulative_errors_proposed[-1]:.3f}\")\nprint(f\"Fusion decisions made by DKW Controller: {(proposed_df['decision'] == 'fusion').sum()}\")\nprint(f\"Efficiency gain (fusion usage): {(proposed_df['decision'] == 'fusion').sum() / len(proposed_df) * 100:.1f}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Visualization and Analysis\n\nLet's visualize how the DKW controller adapts its decisions based on observed error rates.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Run the experiment\nprint(\"Running DKW Controller experiment...\")\nresults = run_experiment(sample_data)\n\nprint(\"‚úì Experiment completed!\")\nprint(f\"Baseline results: {len(results['baseline'])} decisions\")\nprint(f\"Proposed results: {len(results['proposed'])} decisions\")\n\n# Display results summary\nbaseline_df = pd.DataFrame(results['baseline'])\nproposed_df = pd.DataFrame(results['proposed'])\n\nprint(f\"\\nüìä BASELINE (Always Fission):\")\nprint(f\"   Fission decisions: {(baseline_df['decision'] == 'fission').sum()}\")\nprint(f\"   Fusion decisions: {(baseline_df['decision'] == 'fusion').sum()}\")\nprint(f\"   Errors encountered: {baseline_df['error'].sum()}\")\n\nprint(f\"\\nüß† DKW CONTROLLER (Proposed):\")\nprint(f\"   Fission decisions: {(proposed_df['decision'] == 'fission').sum()}\")\nprint(f\"   Fusion decisions: {(proposed_df['decision'] == 'fusion').sum()}\")  \nprint(f\"   Errors encountered: {proposed_df['error'].sum()}\")\n\n# Show detailed comparison\nprint(f\"\\nüìã DETAILED COMPARISON:\")\ncomparison_df = pd.DataFrame({\n    'ID': proposed_df['id'],\n    'Difficulty': proposed_df['difficulty'],\n    'Error': proposed_df['error'],\n    'Baseline': baseline_df['decision'],\n    'DKW Controller': proposed_df['decision']\n})\nprint(comparison_df)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Run the Experiment\n\nLet's run our experiment and compare the DKW controller's decisions against the baseline approach.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def run_experiment(data):\n    \"\"\"Run DKW controller experiment with inline data.\"\"\"\n    controller = DKWController()\n    results = {\"baseline\": [], \"proposed\": []}\n\n    for example in data:\n        # Simulate error occurrence based on difficulty\n        np.random.seed(hash(example[\"id\"]) % 2**31)  # Deterministic randomness for reproducibility\n        error = np.random.random() < example[\"difficulty\"]\n        controller.add_observation(float(error))\n        decision = controller.decide()\n\n        results[\"proposed\"].append({\n            \"id\": example[\"id\"],\n            \"decision\": decision,\n            \"error\": error,\n            \"difficulty\": example[\"difficulty\"],\n        })\n        results[\"baseline\"].append({\n            \"id\": example[\"id\"],\n            \"decision\": \"fission\",  # Always conservative\n            \"error\": error,\n            \"difficulty\": example[\"difficulty\"],\n        })\n\n    return results\n\nprint(\"‚úì Experiment function defined successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 4. Experiment Function\n\nThe `run_experiment` function simulates running our DKW controller on the test data, comparing it against a baseline that always chooses the conservative \"fission\" mode.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Sample dataset - inline data (no external file dependencies)\nsample_data = [\n    {\"id\": \"example_000\", \"difficulty\": 0.02},  # Low difficulty\n    {\"id\": \"example_001\", \"difficulty\": 0.08},  # Medium difficulty  \n    {\"id\": \"example_002\", \"difficulty\": 0.15},  # High difficulty\n    {\"id\": \"example_003\", \"difficulty\": 0.03},  # Low difficulty\n    {\"id\": \"example_004\", \"difficulty\": 0.12},  # Medium-high difficulty\n    {\"id\": \"example_005\", \"difficulty\": 0.05},  # Low difficulty\n    {\"id\": \"example_006\", \"difficulty\": 0.18},  # High difficulty\n    {\"id\": \"example_007\", \"difficulty\": 0.01},  # Very low difficulty\n    {\"id\": \"example_008\", \"difficulty\": 0.09},  # Medium difficulty\n    {\"id\": \"example_009\", \"difficulty\": 0.20},  # Very high difficulty\n]\n\nprint(f\"‚úì Sample data loaded: {len(sample_data)} examples\")\nprint(f\"Difficulty range: {min(ex['difficulty'] for ex in sample_data):.2f} - {max(ex['difficulty'] for ex in sample_data):.2f}\")\n\n# Display first few examples\nimport pandas as pd\ndf = pd.DataFrame(sample_data[:5])\nprint(f\"\\nFirst 5 examples:\")\nprint(df)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Sample Data (Self-Contained)\n\nInstead of reading from external JSON files, we'll inline the test data directly in the notebook. This includes sample examples with varying difficulty levels that simulate real-world scenarios.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "@dataclass\nclass DKWController:\n    \"\"\"DKW-guided fusion/fission controller.\"\"\"\n    epsilon_target: float = 0.10\n    delta: float = 0.05\n    min_samples: int = 100\n    hysteresis: float = 0.05\n\n    samples: list = field(default_factory=list)\n    current_state: str = \"fission\"\n\n    def dkw_epsilon(self, n: int) -> float:\n        \"\"\"Compute DKW epsilon for n samples.\"\"\"\n        if n < 2:\n            return 1.0\n        return np.sqrt(np.log(2 / self.delta) / (2 * n))\n\n    def add_observation(self, error: float) -> None:\n        \"\"\"Add error observation for calibration.\"\"\"\n        self.samples.append(error)\n\n    def decide(self) -> str:\n        \"\"\"Make fusion/fission decision with DKW guarantee.\"\"\"\n        n = len(self.samples)\n        if n < self.min_samples:\n            return self.current_state\n\n        epsilon = self.dkw_epsilon(n)\n        empirical_error = np.mean(self.samples[-self.min_samples:])\n        error_upper_bound = empirical_error + epsilon\n\n        if self.current_state == \"fusion\":\n            if error_upper_bound > self.epsilon_target + self.hysteresis:\n                self.current_state = \"fission\"\n        else:\n            if error_upper_bound < self.epsilon_target - self.hysteresis:\n                self.current_state = \"fusion\"\n\n        return self.current_state\n\n# Test the controller\ncontroller = DKWController()\nprint(\"‚úì DKWController class created successfully!\")\nprint(f\"Initial state: {controller.current_state}\")\nprint(f\"Target epsilon: {controller.epsilon_target}\")\nprint(f\"Min samples required: {controller.min_samples}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 2. DKW Controller Class\n\nThe `DKWController` uses the **Dvoretzky-Kiefer-Wolfowitz inequality** to provide statistical confidence bounds on empirical error rates.\n\n### Key Parameters:\n- `epsilon_target`: Target error threshold (default: 0.10)\n- `delta`: Confidence parameter for DKW bound (default: 0.05)  \n- `min_samples`: Minimum samples before making decisions (default: 100)\n- `hysteresis`: Prevents state oscillation (default: 0.05)\n\n### States:\n- **Fusion**: Aggressive mode (lower latency, higher risk)\n- **Fission**: Conservative mode (higher latency, lower risk)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\"\"\"DKW Controller Implementation.\"\"\"\nimport json\nimport numpy as np\nfrom dataclasses import dataclass, field\n\nprint(\"‚úì Libraries imported successfully!\")\nprint(f\"NumPy version: {np.__version__}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# DKW Controller Implementation - Interactive Demo\n\nThis notebook implements a **DKW-guided fusion/fission controller** that makes decisions based on error observations with statistical guarantees.\n\n## Overview\n- **DKW (Dvoretzky-Kiefer-Wolfowitz)** inequality provides confidence bounds for empirical error rates\n- **Fusion/Fission** decisions control system behavior based on error thresholds\n- **Hysteresis** prevents oscillation between states\n- **Self-contained demo** with inline data - no external files required"
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Imports and Setup\n\nLet's start by importing the necessary libraries for our DKW controller implementation.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}