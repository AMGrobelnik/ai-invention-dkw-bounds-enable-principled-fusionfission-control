{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## üõ†Ô∏è Customization & Next Steps\n\n### Easy Modifications:\n1. **Add more examples**: Extend the `sample_dataset` list with new questions/answers\n2. **Change difficulty calculation**: Modify the difficulty formula in `collect_data()`\n3. **Add metadata**: Include additional fields like topic, source, etc.\n4. **Filter examples**: Add conditions to process only certain types of questions\n\n### Example: Add a new question\n```python\nsample_dataset.append({\n    \"question\": \"What is 5 √ó 7?\",\n    \"answer\": \"5 √ó 7 = 35\"\n})\n```\n\n### Example: Custom difficulty metric\n```python\n# Replace: difficulty = len(example[\"question\"]) / 100\n# With: difficulty = len(example[\"question\"].split()) / 20  # Based on word count\n```\n\n---\n**üéâ Notebook Complete!** This self-contained version replaces the original script's HuggingFace dependency and file I/O with inline data and interactive output.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Display the JSON output (replaces writing to file)\nprint(\"üìÑ Final JSON structure (first 2 examples):\")\nprint(\"=\" * 50)\n\n# Show formatted JSON for first 2 examples\nsample_output = processed_data[:2]\nformatted_json = json.dumps(sample_output, indent=2)\nprint(formatted_json)\n\nprint(\"=\" * 50)\nprint(f\"\\nüí° In the original script, this data would be saved to 'data_out.json'\")\nprint(f\"üìà Total examples processed: {len(processed_data)}\")\n\n# Simulate the original file saving behavior (commented out)\n# with open(\"data_out.json\", \"w\") as f:\n#     json.dump(processed_data, f, indent=2)\n# print(f\"Collected {len(processed_data)} examples\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## üíæ Output Format\n\nThis shows the final JSON structure that would be saved to `data_out.json`:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Process the full dataset\nprocessed_data = collect_data()\n\nprint(f\"‚úÖ Collected {len(processed_data)} examples\")\nprint(\"\\nüìä Dataset Summary:\")\nprint(f\"   Total examples: {len(processed_data)}\")\nprint(f\"   Average difficulty: {sum(item['difficulty'] for item in processed_data) / len(processed_data):.3f}\")\nprint(f\"   Min difficulty: {min(item['difficulty'] for item in processed_data):.3f}\")\nprint(f\"   Max difficulty: {max(item['difficulty'] for item in processed_data):.3f}\")\n\nprint(\"\\nüîç Sample processed examples:\")\nfor i, item in enumerate(processed_data[:3]):\n    print(f\"\\n   [{i+1}] {item['id']}\")\n    print(f\"       Question: {item['question']}\")\n    print(f\"       Answer: {item['answer'][:60]}{'...' if len(item['answer']) > 60 else ''}\")\n    print(f\"       Difficulty: {item['difficulty']:.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## üöÄ Process Full Dataset\n\nNow let's process all examples and see the results!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def collect_data(dataset: List[Dict[str, Any]] = None) -> List[Dict[str, Any]]:\n    \"\"\"\n    Collect benchmark data for DKW controller evaluation.\n    \n    Args:\n        dataset: Input dataset examples (uses sample_dataset if None)\n    \n    Returns:\n        List of processed examples with IDs, questions, answers, and difficulty scores\n    \"\"\"\n    if dataset is None:\n        dataset = sample_dataset\n    \n    data = []\n    for i, example in enumerate(dataset):\n        data.append({\n            \"id\": f\"example_{i:03d}\",\n            \"question\": example[\"question\"],\n            \"answer\": example[\"answer\"], \n            \"difficulty\": len(example[\"question\"]) / 100,  # Simple proxy metric\n        })\n    \n    return data\n\n# Test the function with a single example\ntest_example = [{\"question\": \"What is 1+1?\", \"answer\": \"1+1=2\"}]\ntest_result = collect_data(test_example)\nprint(\"üß™ Function test successful!\")\nprint(f\"   Input: {test_example[0]['question']}\")\nprint(f\"   Output ID: {test_result[0]['id']}\")\nprint(f\"   Difficulty: {test_result[0]['difficulty']:.2f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## üîß Data Processing Function\n\nThe `collect_data()` function processes raw examples and adds:\n- **Unique IDs** for tracking\n- **Difficulty scores** based on question length (simple proxy)\n- **Structured format** for benchmark evaluation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Sample dataset (simulates HuggingFace GSM8K dataset structure)\n# This replaces: ds = load_dataset(\"gsm8k\", \"main\", split=\"test[:200]\")\n\nsample_dataset = [\n    {\n        \"question\": \"What is 2+2?\",\n        \"answer\": \"To find 2+2, I add the numbers: 2 + 2 = 4\"\n    },\n    {\n        \"question\": \"If x=5, what is 2x?\", \n        \"answer\": \"If x = 5, then 2x = 2 * 5 = 10\"\n    },\n    {\n        \"question\": \"Solve: 3y + 6 = 15\",\n        \"answer\": \"To solve 3y + 6 = 15, I subtract 6 from both sides: 3y = 9. Then divide by 3: y = 3\"\n    },\n    {\n        \"question\": \"A store has 24 apples. If they sell 3/4 of them, how many apples are left?\",\n        \"answer\": \"3/4 of 24 apples = (3/4) * 24 = 18 apples sold. Remaining apples = 24 - 18 = 6 apples\"\n    },\n    {\n        \"question\": \"Calculate the area of a rectangle with length 8 cm and width 5 cm.\",\n        \"answer\": \"Area of rectangle = length √ó width = 8 cm √ó 5 cm = 40 square cm\"\n    }\n]\n\nprint(f\"üìã Loaded {len(sample_dataset)} sample examples\")\nprint(\"üîç Preview of first example:\")\nprint(f\"   Question: {sample_dataset[0]['question']}\")\nprint(f\"   Answer: {sample_dataset[0]['answer'][:50]}...\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## üìä Sample Dataset\n\nInstead of loading from HuggingFace (which would require internet access), we'll use inline sample data.  \nThis simulates the GSM8K dataset structure that would normally be loaded.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\"\"\"Dataset collection script for DKW benchmark.\"\"\"\nimport json\nfrom typing import List, Dict, Any\n\nprint(\"üì¶ Imports loaded successfully!\")\nprint(\"üéØ Ready to process dataset examples\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Overview\n\nThis notebook demonstrates how to:\n1. **Load and process dataset examples** for benchmark evaluation\n2. **Calculate difficulty metrics** based on question complexity\n3. **Structure data** for downstream DKW controller testing\n\n**Key Features:**\n- ‚úÖ Self-contained (no external files required)\n- ‚úÖ Interactive and modifiable\n- ‚úÖ Well-documented with examples\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# Dataset Collection for DKW Benchmark\n## data.py - Interactive Demo\n\nThis notebook demonstrates the dataset collection script for DKW controller evaluation. It has been converted from the original Python script to be completely self-contained and interactive.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}