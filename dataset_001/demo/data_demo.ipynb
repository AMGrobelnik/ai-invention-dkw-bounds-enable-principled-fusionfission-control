{
 "cells": [
  {
   "cell_type": "code",
   "source": "# Example extension: Filter data by difficulty threshold\ndef filter_by_difficulty(data, min_difficulty=0.0, max_difficulty=1.0):\n    \"\"\"Filter examples by difficulty range.\"\"\"\n    filtered = [\n        ex for ex in data \n        if min_difficulty <= ex['difficulty'] <= max_difficulty\n    ]\n    return filtered\n\n# Example usage\nprint(\"=== Filtering Example ===\")\neasy_questions = filter_by_difficulty(collected_data, max_difficulty=0.5)\nhard_questions = filter_by_difficulty(collected_data, min_difficulty=0.5)\n\nprint(f\"Total examples: {len(collected_data)}\")\nprint(f\"Easy questions (difficulty â‰¤ 0.5): {len(easy_questions)}\")\nprint(f\"Hard questions (difficulty > 0.5): {len(hard_questions)}\")\n\n# Show example of easy question\nif easy_questions:\n    print(f\"\\nExample easy question:\")\n    print(f\"  Question: {easy_questions[0]['question'][:50]}...\")\n    print(f\"  Difficulty: {easy_questions[0]['difficulty']:.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Customization and Extensions\n\nThis notebook is designed to be easily modified. Here are some ways you can customize it:\n\n### To use real HuggingFace data:\n1. Uncomment the `datasets` import and `load_dataset` call in the `collect_data()` function\n2. Replace the `simulated_examples` with actual dataset loading\n3. Ensure you have the `datasets` library installed: `pip install datasets`\n\n### To modify difficulty calculation:\n- The current difficulty is based on question length (`len(question) / 100`)\n- You could implement more sophisticated metrics like:\n  - Number of mathematical operations\n  - Presence of specific keywords\n  - Answer complexity\n\n### To add more data processing:\n- Add data validation\n- Implement filtering based on difficulty thresholds\n- Add data cleaning steps\n- Include additional metadata fields",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# In the original script, this would save to file:\n# with open(\"data_out.json\", \"w\") as f:\n#     json.dump(data, f, indent=2)\n\n# For demonstration, let's show what the JSON output would look like\njson_output = json.dumps(collected_data, indent=2)\nprint(\"=== JSON Output (first 500 characters) ===\")\nprint(json_output[:500] + \"...\" if len(json_output) > 500 else json_output)\n\nprint(f\"\\n=== Summary ===\")\nprint(f\"Collected {len(collected_data)} examples\")\nprint(\"JSON data structure ready for DKW benchmark evaluation\")\n\n# Optionally, uncomment the line below to actually save to file\n# with open(\"data_out.json\", \"w\") as f:\n#     json.dump(collected_data, f, indent=2)\n# print(\"Data saved to data_out.json\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Data Export and Saving\n\nThis section demonstrates how the original script would save data to a JSON file. In this self-contained version, we'll show the JSON output without actually writing to disk.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Explore the collected data\nprint(\"=== Data Structure ===\")\nprint(f\"Total examples: {len(collected_data)}\")\nprint(f\"Keys in each example: {list(collected_data[0].keys())}\")\n\nprint(\"\\n=== Sample Examples ===\")\nfor i, example in enumerate(collected_data):\n    print(f\"\\nExample {i+1}:\")\n    print(f\"  ID: {example['id']}\")\n    print(f\"  Question: {example['question'][:50]}...\")\n    print(f\"  Answer: {example['answer'][:50]}...\")\n    print(f\"  Difficulty: {example['difficulty']:.3f}\")\n\n# Calculate some basic statistics\ndifficulties = [ex['difficulty'] for ex in collected_data]\nprint(f\"\\n=== Statistics ===\")\nprint(f\"Average difficulty: {sum(difficulties)/len(difficulties):.3f}\")\nprint(f\"Min difficulty: {min(difficulties):.3f}\")\nprint(f\"Max difficulty: {max(difficulties):.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 4. Data Exploration and Analysis\n\nLet's explore the collected data and understand its structure and characteristics.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def collect_data():\n    \"\"\"Collect benchmark data for DKW controller evaluation.\n    \n    In the original script, this would load from HuggingFace dataset:\n    ds = load_dataset(\"gsm8k\", \"main\", split=\"test[:200]\")\n    \n    For this self-contained notebook, we'll simulate the process\n    with sample questions that represent the type of data we'd get.\n    \"\"\"\n    \n    # Simulated dataset examples (replacing HuggingFace dataset loading)\n    simulated_examples = [\n        {\"question\": \"Janet's ducks lay 16 eggs per day. She eats 3 for breakfast and bakes 4 for her friends. How many eggs does she have left?\", \"answer\": \"Janet's ducks lay 16 eggs per day. She eats 3 for breakfast, so she has 16 - 3 = 13 eggs left. She bakes 4 for her friends, so she has 13 - 4 = 9 eggs left. The answer is 9.\"},\n        {\"question\": \"A robe takes 2 bolts of blue fiber and half that much white fiber. How many bolts are needed total?\", \"answer\": \"The robe takes 2 bolts of blue fiber. It takes half that much white fiber, so it takes 2 / 2 = 1 bolt of white fiber. So the total is 2 + 1 = 3 bolts. The answer is 3.\"},\n        {\"question\": \"If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are there now?\", \"answer\": \"There were originally 3 cars. Then 2 more cars arrive. So there are 3 + 2 = 5 cars now. The answer is 5.\"},\n    ]\n\n    data = []\n    for i, example in enumerate(simulated_examples):\n        data.append({\n            \"id\": f\"example_{i:03d}\",\n            \"question\": example[\"question\"],\n            \"answer\": example[\"answer\"],\n            \"difficulty\": len(example[\"question\"]) / 100,  # Simple proxy based on question length\n        })\n\n    return data\n\n# Test the function\ncollected_data = collect_data()\nprint(f\"Collected {len(collected_data)} examples\")\nprint(f\"First example: {collected_data[0]['id']}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Data Collection Function\n\nThis function demonstrates how the original script would collect data from the GSM8K dataset. For demonstration purposes, we'll use simulated data to keep the notebook self-contained.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Sample data that would be generated from GSM8K dataset\n# This replaces the need to load external JSON files\nsample_data_output = [\n    {\n        \"id\": \"example_000\",\n        \"question\": \"What is 2+2?\",\n        \"answer\": \"4\",\n        \"difficulty\": 0.15\n    },\n    {\n        \"id\": \"example_001\",\n        \"question\": \"If x=5, what is 2x?\",\n        \"answer\": \"10\",\n        \"difficulty\": 0.22\n    },\n    {\n        \"id\": \"example_002\",\n        \"question\": \"Solve: 3y + 6 = 15\",\n        \"answer\": \"y=3\",\n        \"difficulty\": 0.28\n    }\n]\n\nprint(f\"Sample dataset contains {len(sample_data_output)} examples\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 2. Sample Output Data (Self-Contained)\n\nSince we want this notebook to be completely self-contained, we'll inline sample data that would typically be generated from the GSM8K dataset. This allows the notebook to run without external dependencies.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\"\"\"Dataset collection script for DKW benchmark.\"\"\"\nimport json\nfrom datasets import load_dataset",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Import Required Libraries\n\nWe'll import the necessary libraries for data manipulation and dataset loading.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# Dataset Collection Script - DKW Benchmark\n\n**Artifact ID:** dataset_001  \n**Name:** data.py  \n\nThis notebook demonstrates a dataset collection script for DKW benchmark controller evaluation. The script loads data from the GSM8K dataset and processes it for benchmarking purposes.\n\n## Overview\n- Loads HuggingFace dataset (GSM8K)\n- Processes examples with ID, question, answer, and difficulty metrics\n- Creates self-contained data structure for further analysis",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}