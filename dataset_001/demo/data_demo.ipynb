{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Collection Script for DKW Benchmark\n",
    "\n",
    "**Artifact Name:** data.py  \n",
    "**Description:** Dataset collection script for DKW benchmark evaluation\n",
    "\n",
    "This notebook demonstrates how to collect and process benchmark data for evaluating DKW controllers. The original script loads data from HuggingFace datasets, but this version is completely self-contained with inline sample data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook contains a self-contained version of the dataset collection script. Instead of loading data from external sources like HuggingFace datasets, we'll use inline sample data to demonstrate the data processing pipeline.\n",
    "\n",
    "### What this notebook does:\n",
    "1. Defines a data collection function\n",
    "2. Processes sample benchmark questions and answers\n",
    "3. Calculates difficulty metrics\n",
    "4. Outputs structured data for DKW evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Dataset collection script for DKW benchmark.\"\"\"\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Note: Original script used 'datasets' library to load from HuggingFace\n",
    "# This version is self-contained with inline sample data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Dataset\n",
    "\n",
    "Instead of loading from HuggingFace, we'll use inline sample data that represents the GSM8K benchmark format. This makes the notebook completely self-contained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inline sample data (replaces HuggingFace dataset loading)\n",
    "# This represents a subset of GSM8K-style math problems\n",
    "sample_dataset = [\n",
    "    {\n",
    "        \"question\": \"What is 2+2?\",\n",
    "        \"answer\": \"4\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"If x=5, what is 2x?\", \n",
    "        \"answer\": \"10\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Solve: 3y + 6 = 15\",\n",
    "        \"answer\": \"y=3\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"A store has 45 apples. If they sell 18 apples in the morning and 12 apples in the afternoon, how many apples do they have left?\",\n",
    "        \"answer\": \"45 - 18 - 12 = 15 apples\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Sarah has $20. She buys 3 pencils for $2 each and 2 notebooks for $4 each. How much money does she have left?\",\n",
    "        \"answer\": \"20 - (3 * 2) - (2 * 4) = 20 - 6 - 8 = $6\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(sample_dataset)} sample examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection Function\n",
    "\n",
    "This function processes the sample dataset and creates structured benchmark data with additional metadata like difficulty scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(dataset: List[Dict[str, str]] = None) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Collect benchmark data for DKW controller evaluation.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Optional dataset to process. If None, uses global sample_dataset.\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing processed benchmark data.\n",
    "    \"\"\"\n",
    "    # Use provided dataset or fall back to global sample_dataset\n",
    "    if dataset is None:\n",
    "        dataset = sample_dataset\n",
    "    \n",
    "    data = []\n",
    "    for i, example in enumerate(dataset):\n",
    "        # Process each example and add metadata\n",
    "        processed_example = {\n",
    "            \"id\": f\"example_{i:03d}\",\n",
    "            \"question\": example[\"question\"],\n",
    "            \"answer\": example[\"answer\"],\n",
    "            \"difficulty\": len(example[\"question\"]) / 100,  # Simple proxy for difficulty\n",
    "        }\n",
    "        data.append(processed_example)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Test the function with our sample data\n",
    "print(\"Data collection function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Data Collection\n",
    "\n",
    "Let's run the data collection function and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the data collection function\n",
    "collected_data = collect_data()\n",
    "\n",
    "print(f\"Collected {len(collected_data)} examples\")\n",
    "print(\"\\nFirst few examples:\")\n",
    "for i, example in enumerate(collected_data[:3]):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  ID: {example['id']}\")\n",
    "    print(f\"  Question: {example['question']}\")\n",
    "    print(f\"  Answer: {example['answer']}\")\n",
    "    print(f\"  Difficulty: {example['difficulty']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "\n",
    "Let's analyze the collected data to understand the difficulty distribution and other characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the collected data\n",
    "difficulties = [example['difficulty'] for example in collected_data]\n",
    "question_lengths = [len(example['question']) for example in collected_data]\n",
    "\n",
    "print(\"=== Data Analysis ===\")\n",
    "print(f\"Total examples: {len(collected_data)}\")\n",
    "print(f\"Average difficulty: {sum(difficulties) / len(difficulties):.3f}\")\n",
    "print(f\"Difficulty range: {min(difficulties):.3f} - {max(difficulties):.3f}\")\n",
    "print(f\"Average question length: {sum(question_lengths) / len(question_lengths):.1f} characters\")\n",
    "\n",
    "print(\"\\n=== Difficulty Distribution ===\")\n",
    "for i, example in enumerate(collected_data):\n",
    "    difficulty_bar = \"â–ˆ\" * int(example['difficulty'] * 50)  # Scale for visualization\n",
    "    print(f\"{example['id']}: {difficulty_bar} ({example['difficulty']:.3f})\")\n",
    "    \n",
    "# Show complete data structure for reference\n",
    "print(f\"\\n=== Complete Data Structure ===\")\n",
    "print(json.dumps(collected_data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results (Optional)\n",
    "\n",
    "Optionally save the processed data to a JSON file for external use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the collected data to a JSON file (equivalent to original script behavior)\n",
    "output_filename = \"data_out.json\"\n",
    "\n",
    "# Uncomment the following lines to save to file:\n",
    "# with open(output_filename, \"w\") as f:\n",
    "#     json.dump(collected_data, f, indent=2)\n",
    "# print(f\"Data saved to {output_filename}\")\n",
    "\n",
    "# For demonstration, let's show what would be saved:\n",
    "print(\"=== Data that would be saved to JSON file ===\")\n",
    "json_output = json.dumps(collected_data, indent=2)\n",
    "print(json_output)\n",
    "\n",
    "print(f\"\\nâœ… Notebook execution complete!\")\n",
    "print(f\"ðŸ“Š Processed {len(collected_data)} examples for DKW benchmark evaluation\")\n",
    "print(f\"ðŸ’¾ Data is ready for use (uncomment save lines to write to file)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage and Modification Notes\n",
    "\n",
    "This notebook is completely self-contained and can be run without any external files or dependencies (except for the `datasets` library if you want to use the original `collect_data()` function).\n",
    "\n",
    "### Key Changes from Original Script:\n",
    "- **Inlined JSON data**: The sample data is now embedded as a Python list instead of being read from an external JSON file\n",
    "- **Interactive exploration**: Added analysis and visualization of the dataset\n",
    "- **Self-contained**: No external file dependencies for the demo\n",
    "\n",
    "### To Modify:\n",
    "1. **Use real data**: Uncomment and run `data = collect_data()` to fetch from HuggingFace\n",
    "2. **Add more examples**: Extend the `sample_dataset` list with additional examples\n",
    "3. **Change difficulty calculation**: Modify the difficulty formula in the `collect_data()` function\n",
    "4. **Export results**: Save `collected_data` to a file using `json.dump()` if needed\n",
    "\n",
    "### Original Artifact:\n",
    "- **ID**: dataset_001\n",
    "- **Name**: data.py\n",
    "- **Purpose**: DKW benchmark dataset collection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}