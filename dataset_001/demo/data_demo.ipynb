{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## Usage Instructions & Customization\n\n### How to Use This Notebook\n1. **Run all cells sequentially** - Click \"Run All\" or execute each cell with Shift+Enter\n2. **Modify the data** - Edit the `sample_raw_data` list to add your own questions and answers\n3. **Adjust difficulty calculation** - Modify the difficulty formula in the `collect_data()` function\n4. **Export data** - The `json_output` variable contains the formatted JSON string\n\n### Customization Options\n- **Add more examples**: Extend the `sample_raw_data` list\n- **Change difficulty metric**: Modify the calculation in `collect_data()`\n- **Add new fields**: Extend the data structure with additional metadata\n- **Save to file**: Uncomment and modify the file writing code if needed\n\n### Original vs. Self-Contained Version\n- âœ… **Original**: Used HuggingFace datasets library and external files\n- âœ… **This version**: Uses inlined data for complete self-containment\n- âœ… **Maintained**: All core functionality and data structure\n- âœ… **Added**: Interactive analysis and visualization\n\nThis notebook is now completely **runnable without any external dependencies** beyond standard Python libraries!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Display all processed examples in a readable format\nprint(\"ðŸ“Š PROCESSED DATASET SUMMARY\")\nprint(\"=\" * 50)\n\nfor i, item in enumerate(data_output):\n    print(f\"\\nExample {i+1}: {item['id']}\")\n    print(f\"Question: {item['question']}\")\n    print(f\"Answer: {item['answer']}\")\n    print(f\"Difficulty: {item['difficulty']:.2f}\")\n    print(\"-\" * 30)\n\n# Statistics\ndifficulties = [item['difficulty'] for item in data_output]\nprint(f\"\\nðŸ“ˆ STATISTICS:\")\nprint(f\"Total examples: {len(data_output)}\")\nprint(f\"Min difficulty: {min(difficulties):.2f}\")\nprint(f\"Max difficulty: {max(difficulties):.2f}\")\nprint(f\"Average difficulty: {sum(difficulties)/len(difficulties):.2f}\")\n\n# Export as JSON string (equivalent to original file output)\njson_output = json.dumps(data_output, indent=2)\nprint(f\"\\nðŸ’¾ JSON OUTPUT (first 200 characters):\")\nprint(json_output[:200] + \"...\" if len(json_output) > 200 else json_output)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Data Analysis & Visualization\n\nLet's examine the processed data in detail and display it in a readable format.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Process the sample data\nprocessed_data = collect_data(sample_raw_data)\n\n# Display summary\nprint(f\"Successfully collected {len(processed_data)} examples\")\nprint(f\"Average difficulty: {sum(item['difficulty'] for item in processed_data) / len(processed_data):.2f}\")\n\n# Instead of saving to file, store in variable for analysis\ndata_output = processed_data\n\nprint(\"\\nâœ… Data processing complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Main Execution\n\nProcess the sample data and display the results. Instead of saving to an external file, we'll store the results in a variable for immediate analysis.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def collect_data(raw_data: List[Dict[str, str]]) -> List[Dict[str, Any]]:\n    \"\"\"\n    Collect and process benchmark data for DKW controller evaluation.\n    \n    Args:\n        raw_data: List of dictionaries with 'question' and 'answer' keys\n        \n    Returns:\n        List of processed examples with additional metadata\n    \"\"\"\n    data = []\n    for i, example in enumerate(raw_data):\n        data.append({\n            \"id\": f\"example_{i:03d}\",\n            \"question\": example[\"question\"],\n            \"answer\": example[\"answer\"],\n            \"difficulty\": len(example[\"question\"]) / 100,  # Simple proxy based on question length\n        })\n    \n    return data\n\n# Test the function with a single example\ntest_example = [{\"question\": \"Test question\", \"answer\": \"Test answer\"}]\ntest_result = collect_data(test_example)\nprint(\"Function test successful:\")\nprint(f\"Input: {test_example[0]}\")\nprint(f\"Output: {test_result[0]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Data Processing Function\n\nThe `collect_data()` function processes the raw data and adds additional metadata:\n- Assigns unique IDs to each example\n- Calculates a difficulty metric based on question length\n- Structures the data for benchmark evaluation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Sample data representing typical GSM8K math problems\n# This replaces the external HuggingFace dataset for demonstration\nsample_raw_data = [\n    {\n        \"question\": \"What is 2+2?\",\n        \"answer\": \"4\"\n    },\n    {\n        \"question\": \"If x=5, what is 2x?\", \n        \"answer\": \"10\"\n    },\n    {\n        \"question\": \"Solve: 3y + 6 = 15\",\n        \"answer\": \"y=3\"\n    },\n    {\n        \"question\": \"A store has 45 apples. If they sell 18 apples in the morning and 12 apples in the afternoon, how many apples do they have left?\",\n        \"answer\": \"45 - 18 - 12 = 15 apples\"\n    },\n    {\n        \"question\": \"Sarah has 3 times as many books as Tom. If Tom has 8 books, how many books does Sarah have?\",\n        \"answer\": \"3 Ã— 8 = 24 books\"\n    }\n]\n\nprint(f\"Loaded {len(sample_raw_data)} sample questions\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Sample Dataset\n\nInstead of loading from external files or HuggingFace, we'll use inlined sample data to make this notebook completely self-contained. This represents the kind of data that would be collected from the GSM8K dataset.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\"\"\"Dataset collection script for DKW benchmark.\"\"\"\nimport json\nfrom typing import List, Dict, Any\n\nprint(\"Libraries imported successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Imports\n\nRequired Python libraries for data processing and JSON handling.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# Dataset Collection for DKW Benchmark\n\n**Artifact:** dataset_001 (data.py)\n\nThis notebook demonstrates data collection and processing for the DKW controller evaluation benchmark. The original script has been converted to a self-contained format with inlined data to eliminate external dependencies.\n\n## Overview\n- Processes benchmark data for evaluation\n- Calculates difficulty metrics\n- Generates structured output for analysis",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}