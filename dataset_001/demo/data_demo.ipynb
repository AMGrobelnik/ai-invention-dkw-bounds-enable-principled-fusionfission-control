{
 "cells": [
  {
   "cell_type": "code",
   "source": "# Save data to JSON file (replicating original script behavior)\noutput_filename = \"data_out.json\"\n\nwith open(output_filename, \"w\") as f:\n    json.dump(working_data, f, indent=2)\n\nprint(f\"üíæ Saved {len(working_data)} examples to {output_filename}\")\nprint(\"‚ú® Dataset collection complete!\")\n\n# Show the JSON content as it would appear in the file\nprint(f\"\\nüìÑ Content of {output_filename}:\")\nprint(json.dumps(working_data, indent=2))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Export Data (Optional)\n\nIf you want to save the data to a JSON file (replicating the original script behavior), run the cell below:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Analyze the data\ndifficulties = [item[\"difficulty\"] for item in working_data]\navg_difficulty = sum(difficulties) / len(difficulties)\nmin_difficulty = min(difficulties)\nmax_difficulty = max(difficulties)\n\nprint(\"üìä Dataset Statistics:\")\nprint(f\"Total examples: {len(working_data)}\")\nprint(f\"Average difficulty: {avg_difficulty:.3f}\")\nprint(f\"Difficulty range: {min_difficulty:.3f} - {max_difficulty:.3f}\")\n\nprint(\"\\nüìù Sample questions by difficulty:\")\nsorted_data = sorted(working_data, key=lambda x: x[\"difficulty\"])\nprint(f\"Easiest: {sorted_data[0]['question'][:50]}... (difficulty: {sorted_data[0]['difficulty']:.3f})\")\nprint(f\"Hardest: {sorted_data[-1]['question'][:50]}... (difficulty: {sorted_data[-1]['difficulty']:.3f})\")\n\nprint(\"\\nüîç All examples:\")\nfor item in working_data:\n    print(f\"ID: {item['id']}, Difficulty: {item['difficulty']:.3f}\")\n    print(f\"Q: {item['question']}\")\n    print(f\"A: {item['answer']}\")\n    print(\"-\" * 50)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Data Analysis\n\nLet's analyze our collected data to understand its characteristics:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Attempt to collect live data from HuggingFace\ntry:\n    live_data = collect_data()\n    print(f\"‚úÖ Successfully collected {len(live_data)} examples from HuggingFace\")\n    print(\"\\nFirst live example:\")\n    print(json.dumps(live_data[0], indent=2))\n    \n    # Use live data for further processing\n    working_data = live_data\n    \nexcept Exception as e:\n    print(f\"‚ùå Could not connect to HuggingFace: {e}\")\n    print(\"üìù Using sample data instead...\")\n    \n    # Fall back to sample data\n    working_data = sample_data\n\nprint(f\"\\nWorking with {len(working_data)} examples\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Live Data Collection\n\nRun the cell below to collect fresh data from HuggingFace. This requires:\n- Internet connection\n- `datasets` library installed\n- Possible HuggingFace authentication for some datasets\n\n**Note:** If you just want to experiment with the data format, use the `sample_data` above instead!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Sample data (inlined from data_out.json)\nsample_data = [\n    {\n        \"id\": \"example_000\",\n        \"question\": \"What is 2+2?\",\n        \"answer\": \"4\",\n        \"difficulty\": 0.15\n    },\n    {\n        \"id\": \"example_001\",\n        \"question\": \"If x=5, what is 2x?\",\n        \"answer\": \"10\",\n        \"difficulty\": 0.22\n    },\n    {\n        \"id\": \"example_002\",\n        \"question\": \"Solve: 3y + 6 = 15\",\n        \"answer\": \"y=3\",\n        \"difficulty\": 0.28\n    }\n]\n\nprint(f\"Sample dataset contains {len(sample_data)} examples\")\nprint(\"\\nFirst example:\")\nprint(json.dumps(sample_data[0], indent=2))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Sample Data (Self-Contained)\n\nFor immediate experimentation, here's sample data that represents the expected output format. This makes the notebook completely self-contained - you can work with the data without needing to connect to HuggingFace or download anything.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def collect_data():\n    \"\"\"Collect benchmark data for DKW controller evaluation.\"\"\"\n    # Load HuggingFace dataset\n    ds = load_dataset(\"gsm8k\", \"main\", split=\"test[:200]\")\n\n    data = []\n    for i, example in enumerate(ds):\n        data.append({\n            \"id\": f\"example_{i:03d}\",\n            \"question\": example[\"question\"],\n            \"answer\": example[\"answer\"],\n            \"difficulty\": len(example[\"question\"]) / 100,  # Simple proxy\n        })\n\n    return data",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Data Collection Function\n\nThe `collect_data()` function connects to HuggingFace to load the GSM8K dataset and processes it for our benchmark. It:\n\n- Loads the first 200 test examples from GSM8K\n- Extracts question and answer pairs  \n- Calculates a difficulty score based on question length\n- Returns structured data with unique IDs",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\"\"\"Dataset collection script for DKW benchmark.\"\"\"\nimport json\nfrom datasets import load_dataset",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Overview\n\nThis notebook provides two ways to work with the dataset:\n\n1. **Live Data Collection**: Connect to HuggingFace to collect fresh data from the GSM8K dataset\n2. **Sample Data**: Use pre-processed sample data for immediate experimentation\n\nThe sample data has been inlined to make this notebook completely self-contained - no external files required!",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# Dataset Collection Script for DKW Benchmark\n\n**Artifact ID:** dataset_001  \n**Name:** data.py\n\nThis notebook demonstrates how to collect and process benchmark data for DKW controller evaluation. The original script has been converted into an interactive format with sample data included for easy experimentation.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}