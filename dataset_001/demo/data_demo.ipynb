{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## How to Modify and Extend\n\n### To use with real HuggingFace data:\n1. Install required packages: `pip install datasets`\n2. Uncomment the line `data = collect_data()` in the data collection section\n3. Comment out or replace the `sample_data` with `data`\n\n### To customize the dataset:\n- **Change dataset source:** Modify the `load_dataset()` call in `collect_data()`\n- **Adjust sample size:** Change `\"test[:200]\"` to your desired split\n- **Modify difficulty calculation:** Update the difficulty formula in the data processing loop\n- **Add new fields:** Extend the data dictionary with additional metadata\n\n### Example modifications:\n\n```python\n# Use a different dataset\nds = load_dataset(\"squad\", split=\"validation[:100]\")\n\n# More sophisticated difficulty calculation\ndifficulty = (len(example[\"question\"]) + len(example[\"answer\"])) / 200\n\n# Add additional metadata\ndata.append({\n    \"id\": f\"example_{i:03d}\",\n    \"question\": example[\"question\"],\n    \"answer\": example[\"answer\"], \n    \"difficulty\": difficulty,\n    \"word_count\": len(example[\"question\"].split()),\n    \"created_at\": datetime.now().isoformat()\n})\n```\n\nThis notebook is now completely self-contained and ready to run!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Save the data to JSON file (optional)\n# Uncomment the lines below if you want to save the data\n\n# with open(\"data_out.json\", \"w\") as f:\n#     json.dump(sample_data, f, indent=2)\n# print(f\"Data saved to data_out.json\")\n\n# For demonstration, let's show what the JSON would look like\nprint(\"JSON representation of the data:\")\nprint(json.dumps(sample_data, indent=2))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Data Export (Optional)\n\nThe original script saved data to `data_out.json`. Here's how you could save the processed data if needed:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Analyze the dataset characteristics\ntotal_examples = len(sample_data)\ndifficulties = [example[\"difficulty\"] for example in sample_data]\navg_difficulty = sum(difficulties) / len(difficulties)\nmin_difficulty = min(difficulties)\nmax_difficulty = max(difficulties)\n\nprint(\"Dataset Statistics:\")\nprint(f\"Total examples: {total_examples}\")\nprint(f\"Average difficulty: {avg_difficulty:.2f}\")\nprint(f\"Difficulty range: {min_difficulty:.2f} - {max_difficulty:.2f}\")\n\nprint(\"\\nAll examples:\")\nfor example in sample_data:\n    print(f\"ID: {example['id']}\")\n    print(f\"Question: {example['question']}\")\n    print(f\"Answer: {example['answer']}\")\n    print(f\"Difficulty: {example['difficulty']:.2f}\")\n    print(\"-\" * 50)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Data Analysis\n\nLet's analyze the collected data to understand its characteristics.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# For demonstration purposes, we'll use sample data instead of calling HuggingFace\n# This makes the notebook completely self-contained\n\n# Simulate the data collection with sample data\n# (In a real scenario, uncomment the line below to collect from HuggingFace)\n# data = collect_data()\n\n# Sample data that would typically be loaded from the dataset\nsample_data = [\n    {\n        \"id\": \"example_000\",\n        \"question\": \"What is 2+2?\",\n        \"answer\": \"4\", \n        \"difficulty\": 0.15\n    },\n    {\n        \"id\": \"example_001\",\n        \"question\": \"If x=5, what is 2x?\",\n        \"answer\": \"10\",\n        \"difficulty\": 0.22\n    },\n    {\n        \"id\": \"example_002\", \n        \"question\": \"Solve: 3y + 6 = 15\",\n        \"answer\": \"y=3\",\n        \"difficulty\": 0.28\n    }\n]\n\nprint(f\"Collected {len(sample_data)} examples\")\nprint(\"Sample data structure:\")\nfor i, example in enumerate(sample_data[:2]):\n    print(f\"Example {i+1}: {example}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Data Collection and Processing\n\nNow let's run the data collection function and examine the results. In the original script, this data would be saved to `data_out.json`.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def collect_data():\n    \"\"\"Collect benchmark data for DKW controller evaluation.\"\"\"\n    # Load HuggingFace dataset\n    ds = load_dataset(\"gsm8k\", \"main\", split=\"test[:200]\")\n\n    data = []\n    for i, example in enumerate(ds):\n        data.append({\n            \"id\": f\"example_{i:03d}\",\n            \"question\": example[\"question\"],\n            \"answer\": example[\"answer\"],\n            \"difficulty\": len(example[\"question\"]) / 100,  # Simple proxy\n        })\n\n    return data",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Data Collection Function\n\nThe main function loads data from the HuggingFace GSM8K dataset and processes it for benchmark evaluation. \n\n**Key features:**\n- Loads the first 200 examples from the test split\n- Adds unique IDs for each example  \n- Calculates a simple difficulty metric based on question length\n- Returns structured data ready for evaluation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\"\"\"Dataset collection script for DKW benchmark.\"\"\"\nimport json\nfrom datasets import load_dataset",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Setup and Imports\n\nFirst, let's import the necessary libraries for data collection and processing.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# Dataset Collection for DKW Benchmark\n\nThis notebook demonstrates dataset collection and processing for the DKW controller evaluation benchmark. \n\n**Original script:** `data.py`  \n**Purpose:** Collect and process benchmark data from HuggingFace datasets",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}