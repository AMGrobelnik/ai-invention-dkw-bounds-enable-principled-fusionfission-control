{
 "cells": [
  {
   "cell_type": "code",
   "source": "# Uncomment to save data to file\n# with open(\"data_out.json\", \"w\") as f:\n#     json.dump(processed_data, f, indent=2)\n# print(\"Data saved to data_out.json\")\n\n# Or create a more detailed export\ndef export_data(filename=\"benchmark_data.json\"):\n    \"\"\"Export processed data with metadata.\"\"\"\n    export_dict = {\n        \"metadata\": {\n            \"source\": \"DKW Benchmark Dataset\",\n            \"total_examples\": len(processed_data),\n            \"avg_difficulty\": sum(item['difficulty'] for item in processed_data) / len(processed_data)\n        },\n        \"data\": processed_data\n    }\n    \n    with open(filename, \"w\") as f:\n        json.dump(export_dict, f, indent=2)\n    print(f\"Enhanced data exported to {filename}\")\n\n# Uncomment to run the enhanced export\n# export_data()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Optional: Save to File\n\nUncomment the code below if you want to save the processed data to a JSON file:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Display the processed data\nprint(\"Processed Data:\")\nprint(json.dumps(processed_data, indent=2))\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"Data Summary:\")\nfor item in processed_data:\n    print(f\"ID: {item['id']}\")\n    print(f\"Question: {item['question']}\")\n    print(f\"Answer: {item['answer']}\")\n    print(f\"Difficulty: {item['difficulty']:.2f}\")\n    print(\"-\" * 30)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Results Display\n\nLet's examine the processed data structure and optionally save it to JSON format for use in other applications.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Execute the main data collection process\nprocessed_data = collect_data()\nprint(f\"Collected {len(processed_data)} examples\")\n\n# Display basic statistics\ntotal_length = sum(len(item['question']) for item in processed_data)\navg_length = total_length / len(processed_data)\nprint(f\"Average question length: {avg_length:.1f} characters\")\nprint(f\"Difficulty range: {min(item['difficulty'] for item in processed_data):.2f} - {max(item['difficulty'] for item in processed_data):.2f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Execute Data Collection\n\nNow let's run the data collection process and save the results. Instead of writing to a file, we'll store the data in memory for inspection.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def collect_data():\n    \"\"\"Collect benchmark data for DKW controller evaluation.\"\"\"\n    # Process our inlined sample dataset instead of loading from HuggingFace\n    data = []\n    for i, example in enumerate(sample_dataset):\n        data.append({\n            \"id\": f\"example_{i:03d}\",\n            \"question\": example[\"question\"],\n            \"answer\": example[\"answer\"],\n            \"difficulty\": len(example[\"question\"]) / 100,  # Simple proxy metric\n        })\n    \n    return data",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Data Processing Function\n\nThe `collect_data()` function processes the raw dataset and adds:\n- Unique identifiers for each example\n- Difficulty scores based on question length (simple proxy metric)\n- Structured formatting for downstream processing",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Sample dataset representing GSM8K questions and answers\n# This replaces the original load_dataset(\"gsm8k\", \"main\", split=\"test[:200]\") call\nsample_dataset = [\n    {\n        \"question\": \"What is 2+2?\",\n        \"answer\": \"4\"\n    },\n    {\n        \"question\": \"If x=5, what is 2x?\", \n        \"answer\": \"10\"\n    },\n    {\n        \"question\": \"Solve: 3y + 6 = 15\",\n        \"answer\": \"y=3\"\n    }\n]\n\nprint(f\"Loaded {len(sample_dataset)} sample questions\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Inlined Dataset\n\nInstead of loading from external sources, we've inlined sample data to make this notebook completely self-contained. This represents the type of data that would be loaded from the HuggingFace GSM8K dataset.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\"\"\"Dataset collection script for DKW benchmark.\"\"\"\nimport json",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Imports\n\nWe'll use the standard `json` library for data handling. The original script used `datasets` from HuggingFace, but we've inlined the data to make this notebook self-contained.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# Dataset Collection Script for DKW Benchmark\n\nThis notebook demonstrates dataset collection and processing for DKW controller evaluation. The original script has been converted to be completely self-contained with inlined data.\n\n**Original Artifact**: `dataset_001` - `data.py`\n\n## Overview\n- Loads and processes benchmark data\n- Calculates difficulty metrics based on question length\n- Outputs structured data for evaluation purposes",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}