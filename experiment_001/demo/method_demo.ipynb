{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## Output Data Structure\n\nThe final results follow the same structure as the original `method_out.json` file. Here's a sample of the output for reference:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Interactive parameter tuning - modify these values and re-run to see the effect!\nprint(\"=== INTERACTIVE PARAMETER TUNING ===\")\nprint(\"Modify the parameters below and re-run this cell to experiment:\")\nprint()\n\n# Adjustable parameters - users can modify these\nEPSILON_TARGET = 0.08  # Try values like 0.05, 0.10, 0.15\nDELTA = 0.01          # Try values like 0.01, 0.05, 0.10  \nMIN_SAMPLES = 50      # Try values like 20, 50, 100, 200\nHYSTERESIS = 0.03     # Try values like 0.01, 0.03, 0.05\n\ndef run_custom_experiment(epsilon_target, delta, min_samples, hysteresis):\n    \"\"\"Run experiment with custom parameters.\"\"\"\n    controller = DKWController(\n        epsilon_target=epsilon_target,\n        delta=delta,\n        min_samples=min_samples,\n        hysteresis=hysteresis\n    )\n    \n    results = {\"baseline\": [], \"proposed\": []}\n    \n    for example in sample_data:\n        error = np.random.random() < example[\"difficulty\"]\n        controller.add_observation(float(error))\n        decision = controller.decide()\n\n        results[\"proposed\"].append({\n            \"id\": example[\"id\"],\n            \"decision\": decision,\n            \"error\": error,\n        })\n        results[\"baseline\"].append({\n            \"id\": example[\"id\"],\n            \"decision\": \"fission\",\n            \"error\": error,\n        })\n    \n    return results\n\n# Run experiment with custom parameters\nprint(f\"Running experiment with custom parameters:\")\nprint(f\"  â€¢ epsilon_target = {EPSILON_TARGET}\")\nprint(f\"  â€¢ delta = {DELTA} (confidence = {1-DELTA:.1%})\")\nprint(f\"  â€¢ min_samples = {MIN_SAMPLES}\")\nprint(f\"  â€¢ hysteresis = {HYSTERESIS}\")\nprint()\n\ncustom_results = run_custom_experiment(EPSILON_TARGET, DELTA, MIN_SAMPLES, HYSTERESIS)\n\n# Quick analysis\nproposed = custom_results[\"proposed\"]\nbaseline = custom_results[\"baseline\"]\n\nproposed_fusion = sum(1 for r in proposed if r[\"decision\"] == \"fusion\")\nproposed_errors = sum(1 for r in proposed if r[\"error\"])\nbaseline_fusion = sum(1 for r in baseline if r[\"decision\"] == \"fusion\")\nbaseline_errors = sum(1 for r in baseline if r[\"error\"])\n\ntotal = len(proposed)\n\nprint(f\"Results with custom parameters:\")\nprint(f\"  Proposed: {proposed_fusion/total:.1%} fusion, {proposed_errors/total:.3f} error rate\")\nprint(f\"  Baseline: {baseline_fusion/total:.1%} fusion, {baseline_errors/total:.3f} error rate\")\nprint(f\"  Improvement: {(proposed_fusion - baseline_fusion)} more fusion decisions\")\nprint()\nprint(\"ðŸ’¡ Try modifying the parameters above and re-running to see different behaviors!\")\nprint(\"ðŸ’¡ Lower epsilon_target = more aggressive fusion\")\nprint(\"ðŸ’¡ Lower delta = higher confidence requirements\") \nprint(\"ðŸ’¡ Higher min_samples = more conservative initially\")\nprint(\"ðŸ’¡ Higher hysteresis = less frequent switching\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Interactive Experimentation\n\nTry modifying the controller parameters below to see how they affect the decision-making behavior. This makes the notebook interactive and educational!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create visualization of the controller's behavior\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\nfig.suptitle('DKW Controller Analysis', fontsize=16)\n\n# 1. Decision timeline\ndecisions = experiment_results[\"tracking\"][\"decisions\"]\ndecision_numeric = [1 if d == \"fusion\" else 0 for d in decisions]\nax1.plot(decision_numeric, linewidth=2, color='blue', alpha=0.7)\nax1.fill_between(range(len(decision_numeric)), decision_numeric, alpha=0.3, color='blue')\nax1.set_title('Decision Timeline (1=Fusion, 0=Fission)')\nax1.set_xlabel('Example #')\nax1.set_ylabel('Decision')\nax1.set_ylim(-0.1, 1.1)\nax1.grid(True, alpha=0.3)\n\n# 2. Error rate evolution  \nerror_rates = experiment_results[\"tracking\"][\"error_rates\"]\nvalid_indices = [i for i, rate in enumerate(error_rates) if rate is not None]\nvalid_rates = [rate for rate in error_rates if rate is not None]\n\nif valid_rates:\n    ax2.plot(valid_indices, valid_rates, 'r-', linewidth=2, label='Empirical Error Rate')\n    ax2.axhline(y=0.10, color='black', linestyle='--', label='Target (Îµ=0.10)')\n    ax2.axhline(y=0.15, color='red', linestyle=':', alpha=0.7, label='Upper Threshold')\n    ax2.axhline(y=0.05, color='green', linestyle=':', alpha=0.7, label='Lower Threshold')\n    ax2.set_title('Error Rate Evolution')\n    ax2.set_xlabel('Example #')\n    ax2.set_ylabel('Error Rate')\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n\n# 3. Decision distribution comparison\nmethods = ['Baseline\\n(Always Fission)', 'Proposed\\n(DKW Controller)']\nfusion_rates = [metrics['baseline_fusion_rate'], metrics['proposed_fusion_rate']]\nerror_rates_bar = [metrics['baseline_error_rate'], metrics['proposed_error_rate']]\n\nx = np.arange(len(methods))\nwidth = 0.35\n\nbars1 = ax3.bar(x - width/2, fusion_rates, width, label='Fusion Rate', alpha=0.8, color='blue')\nbars2 = ax3.bar(x + width/2, error_rates_bar, width, label='Error Rate', alpha=0.8, color='red')\n\nax3.set_xlabel('Method')\nax3.set_ylabel('Rate')\nax3.set_title('Performance Comparison')\nax3.set_xticks(x)\nax3.set_xticklabels(methods)\nax3.legend()\nax3.grid(True, alpha=0.3)\n\n# Add value labels on bars\nfor bars in [bars1, bars2]:\n    for bar in bars:\n        height = bar.get_height()\n        ax3.annotate(f'{height:.3f}',\n                    xy=(bar.get_x() + bar.get_width() / 2, height),\n                    xytext=(0, 3),  # 3 points vertical offset\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom', fontsize=10)\n\n# 4. Difficulty vs Error correlation\ndifficulties = [r[\"difficulty\"] for r in experiment_results[\"proposed\"]]\nactual_errors = [1 if r[\"error\"] else 0 for r in experiment_results[\"proposed\"]]\n\nax4.scatter(difficulties, actual_errors, alpha=0.6, s=20)\nax4.set_xlabel('Example Difficulty')\nax4.set_ylabel('Actual Error (1=Error, 0=Success)')\nax4.set_title('Difficulty vs Actual Errors')\nax4.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\\\n=== Key Insights ===\")\nprint(f\"â€¢ Controller achieved {metrics['proposed_fusion_rate']:.1%} fusion usage vs {metrics['baseline_fusion_rate']:.1%} baseline\")\nprint(f\"â€¢ Error rates: {metrics['proposed_error_rate']:.3f} (proposed) vs {metrics['baseline_error_rate']:.3f} (baseline)\")\nif len(valid_rates) > 0:\n    print(f\"â€¢ Final empirical error rate: {valid_rates[-1]:.3f}\")\nprint(f\"â€¢ Total decision switches: {sum(1 for i in range(1, len(decisions)) if decisions[i] != decisions[i-1])}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Visualization\n\nLet's visualize the controller's behavior over time to understand how it adapts to the changing error patterns.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Calculate performance metrics\ndef analyze_results(results):\n    proposed = results[\"proposed\"]\n    baseline = results[\"baseline\"]\n    \n    # Count decisions and errors\n    proposed_fusion_count = sum(1 for r in proposed if r[\"decision\"] == \"fusion\")\n    proposed_fission_count = sum(1 for r in proposed if r[\"decision\"] == \"fission\")\n    proposed_error_count = sum(1 for r in proposed if r[\"error\"])\n    \n    baseline_fusion_count = sum(1 for r in baseline if r[\"decision\"] == \"fusion\")\n    baseline_fission_count = sum(1 for r in baseline if r[\"decision\"] == \"fission\")\n    baseline_error_count = sum(1 for r in baseline if r[\"error\"])\n    \n    total_examples = len(proposed)\n    \n    print(\"=== EXPERIMENT RESULTS ===\")\n    print(f\"Total examples processed: {total_examples}\")\n    print()\n    \n    print(\"PROPOSED (DKW Controller):\")\n    print(f\"  Fusion decisions: {proposed_fusion_count} ({proposed_fusion_count/total_examples:.1%})\")\n    print(f\"  Fission decisions: {proposed_fission_count} ({proposed_fission_count/total_examples:.1%})\")\n    print(f\"  Total errors: {proposed_error_count} ({proposed_error_count/total_examples:.3f} error rate)\")\n    print()\n    \n    print(\"BASELINE (Always Fission):\")\n    print(f\"  Fusion decisions: {baseline_fusion_count} ({baseline_fusion_count/total_examples:.1%})\")\n    print(f\"  Fission decisions: {baseline_fission_count} ({baseline_fission_count/total_examples:.1%})\")\n    print(f\"  Total errors: {baseline_error_count} ({baseline_error_count/total_examples:.3f} error rate)\")\n    print()\n    \n    # Performance comparison\n    fusion_advantage = proposed_fusion_count - baseline_fusion_count\n    print(f\"Performance advantage: {fusion_advantage} more fusion decisions\")\n    print(f\"Error rate difference: {(proposed_error_count - baseline_error_count)/total_examples:.3f}\")\n    \n    return {\n        \"proposed_fusion_rate\": proposed_fusion_count / total_examples,\n        \"proposed_error_rate\": proposed_error_count / total_examples,\n        \"baseline_fusion_rate\": baseline_fusion_count / total_examples,\n        \"baseline_error_rate\": baseline_error_count / total_examples\n    }\n\nmetrics = analyze_results(experiment_results)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Results Analysis\n\nLet's analyze the results comparing the DKW controller (proposed method) against the baseline conservative approach.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def run_experiment(data):\n    \"\"\"Run DKW controller experiment with inline data.\"\"\"\n    controller = DKWController()\n    results = {\"baseline\": [], \"proposed\": []}\n    decisions_over_time = []\n    error_rates_over_time = []\n    \n    for example in data:\n        # Simulate error occurrence based on difficulty\n        error = np.random.random() < example[\"difficulty\"]\n        controller.add_observation(float(error))\n        decision = controller.decide()\n        \n        # Track decisions and error rates over time\n        decisions_over_time.append(decision)\n        if len(controller.samples) >= controller.min_samples:\n            recent_error_rate = np.mean(controller.samples[-controller.min_samples:])\n            error_rates_over_time.append(recent_error_rate)\n        else:\n            error_rates_over_time.append(None)\n\n        results[\"proposed\"].append({\n            \"id\": example[\"id\"],\n            \"decision\": decision,\n            \"error\": error,\n            \"difficulty\": example[\"difficulty\"]\n        })\n        results[\"baseline\"].append({\n            \"id\": example[\"id\"],\n            \"decision\": \"fission\",  # Always conservative\n            \"error\": error,\n            \"difficulty\": example[\"difficulty\"]\n        })\n    \n    # Add tracking information for visualization\n    results[\"tracking\"] = {\n        \"decisions\": decisions_over_time,\n        \"error_rates\": error_rates_over_time,\n        \"samples_count\": list(range(1, len(data) + 1))\n    }\n    \n    return results\n\n# Run the experiment\nprint(\"Running DKW controller experiment...\")\nexperiment_results = run_experiment(sample_data)\nprint(f\"âœ“ Processed {len(experiment_results['proposed'])} examples\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Experiment Function\n\nThe main experiment function that runs the DKW controller on the sample data and compares it against a baseline conservative approach.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Sample data that replaces the need for external JSON files\n# This simulates a dataset with examples of varying difficulty\nsample_data = [\n    {\"id\": f\"example_{i:03d}\", \"difficulty\": difficulty}\n    for i, difficulty in enumerate([\n        # Easy examples (low error probability)\n        0.02, 0.01, 0.03, 0.02, 0.01, 0.04, 0.02, 0.01, 0.03, 0.02,\n        0.01, 0.02, 0.03, 0.01, 0.02, 0.04, 0.01, 0.02, 0.03, 0.01,\n        \n        # Medium difficulty examples\n        0.08, 0.07, 0.09, 0.08, 0.06, 0.10, 0.07, 0.08, 0.09, 0.07,\n        0.06, 0.08, 0.09, 0.07, 0.08, 0.10, 0.06, 0.07, 0.09, 0.08,\n        \n        # Gradually increasing difficulty\n        0.12, 0.11, 0.13, 0.12, 0.10, 0.14, 0.11, 0.12, 0.13, 0.11,\n        0.15, 0.14, 0.16, 0.15, 0.13, 0.17, 0.14, 0.15, 0.16, 0.14,\n        \n        # High difficulty examples (higher error probability) \n        0.20, 0.19, 0.21, 0.20, 0.18, 0.22, 0.19, 0.20, 0.21, 0.19,\n        0.25, 0.24, 0.26, 0.25, 0.23, 0.27, 0.24, 0.25, 0.26, 0.24,\n        \n        # Very challenging examples\n        0.30, 0.29, 0.31, 0.30, 0.28, 0.32, 0.29, 0.30, 0.31, 0.29,\n        0.35, 0.34, 0.36, 0.35, 0.33, 0.37, 0.34, 0.35, 0.36, 0.34,\n        \n        # Additional samples to reach minimum required\n        *[0.1 + 0.01 * (i % 20) for i in range(40)]\n    ])\n]\n\nprint(f\"Created {len(sample_data)} sample examples\")\nprint(\"First 5 examples:\")\nfor i in range(5):\n    print(f\"  {sample_data[i]}\")\nprint(\"...\")\nprint(\"Last 5 examples:\")\nfor i in range(-5, 0):\n    print(f\"  {sample_data[i]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Sample Data\n\nInstead of reading from external files, we'll create sample data inline. This represents examples with varying difficulty levels that influence the probability of errors occurring.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "@dataclass\nclass DKWController:\n    \"\"\"DKW-guided fusion/fission controller.\"\"\"\n    epsilon_target: float = 0.10\n    delta: float = 0.05\n    min_samples: int = 100\n    hysteresis: float = 0.05\n\n    samples: list = field(default_factory=list)\n    current_state: str = \"fission\"\n\n    def dkw_epsilon(self, n: int) -> float:\n        \"\"\"Compute DKW epsilon for n samples.\"\"\"\n        if n < 2:\n            return 1.0\n        return np.sqrt(np.log(2 / self.delta) / (2 * n))\n\n    def add_observation(self, error: float) -> None:\n        \"\"\"Add error observation for calibration.\"\"\"\n        self.samples.append(error)\n\n    def decide(self) -> str:\n        \"\"\"Make fusion/fission decision with DKW guarantee.\"\"\"\n        n = len(self.samples)\n        if n < self.min_samples:\n            return self.current_state\n\n        epsilon = self.dkw_epsilon(n)\n        empirical_error = np.mean(self.samples[-self.min_samples:])\n        error_upper_bound = empirical_error + epsilon\n\n        if self.current_state == \"fusion\":\n            if error_upper_bound > self.epsilon_target + self.hysteresis:\n                self.current_state = \"fission\"\n        else:\n            if error_upper_bound < self.epsilon_target - self.hysteresis:\n                self.current_state = \"fusion\"\n\n        return self.current_state\n\n# Create a controller instance for demonstration\ncontroller = DKWController()\nprint(f\"Initial state: {controller.current_state}\")\nprint(f\"Target error rate: {controller.epsilon_target}\")\nprint(f\"Confidence level: {1 - controller.delta}\")\nprint(f\"Minimum samples needed: {controller.min_samples}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## DKW Controller Class\n\nThe core controller that implements the DKW-guided decision making logic. It maintains a history of error observations and uses statistical bounds to make fusion/fission decisions.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\"\"\"Required imports for the DKW Controller.\"\"\"\nimport json\nimport numpy as np\nfrom dataclasses import dataclass, field\nimport matplotlib.pyplot as plt\n\n# Set random seed for reproducible results\nnp.random.seed(42)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Overview\n\nThe **Dvoretzky-Kiefer-Wolfowitz (DKW) inequality** provides statistical bounds on the difference between empirical and true distributions. In this implementation:\n\n- **Fusion**: Aggressive strategy that may have higher error rates but better performance\n- **Fission**: Conservative strategy with lower error rates but potentially slower performance\n- **DKW Controller**: Uses statistical guarantees to switch between strategies based on observed error rates\n\n### Key Parameters:\n- `epsilon_target`: Target error rate threshold\n- `delta`: Confidence level (1-Î´ confidence)\n- `min_samples`: Minimum samples before making decisions\n- `hysteresis`: Buffer to prevent rapid switching between states",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# DKW Controller Implementation\n\n**Artifact ID:** experiment_001  \n**Name:** method.py\n\nThis notebook demonstrates a DKW-guided fusion/fission controller implementation. The controller uses the Dvoretzky-Kiefer-Wolfowitz (DKW) inequality to make statistically-sound decisions about when to use fusion vs fission strategies based on error observations.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}