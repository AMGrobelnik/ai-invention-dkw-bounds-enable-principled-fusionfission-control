{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## Conclusion and Next Steps\n\nThis notebook demonstrates how the DKW controller adapts its decisions based on observed error rates while providing statistical guarantees. Key observations:\n\n1. **Adaptive Behavior**: The controller switches between fusion and fission based on error observations\n2. **Statistical Guarantees**: Uses DKW inequality to bound estimation error\n3. **Hysteresis**: Prevents oscillation between states\n\n### Experiment with Parameters\n\nYou can modify the controller parameters to see different behaviors:\n\n```python\n# Create a new controller with different parameters\ncustom_controller = DKWController(\n    epsilon_target=0.05,    # Tighter error tolerance\n    delta=0.01,            # Higher confidence\n    min_samples=50,        # Faster adaptation\n    hysteresis=0.02        # Less hysteresis\n)\n```\n\n### Modify the Data\n\nChange the `sample_data` generation to test different scenarios:\n- Different difficulty patterns\n- More or fewer examples\n- Varying error rates over time\n\n### Save Results\n\nUncomment the save block in the previous cell to save results to a JSON file.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Display sample results (first 5 examples)\nsample_results = {\n    \"baseline\": results[\"baseline\"][:5],\n    \"proposed\": results[\"proposed\"][:5]\n}\n\nprint(\"Sample Results (first 5 examples):\")\nprint(json.dumps(sample_results, indent=2))\n\n# Optionally save full results to file\n# with open(\"method_out.json\", \"w\") as f:\n#     json.dump(results, f, indent=2)\n# print(f\"\\nFull results saved to method_out.json\")\n\nprint(f\"\\nTotal examples in full results: {len(results['baseline'])}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Sample Output\n\nHere's a sample of the results in the same format as the original `method_out.json` file:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create visualization of decision patterns over time\nfig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 10))\n\n# Extract decision sequences\nproposed_decisions = [1 if r['decision'] == 'fusion' else 0 for r in results['proposed']]\nerrors = [1 if r['error'] else 0 for r in results['proposed']]\nexample_ids = list(range(len(results['proposed'])))\n\n# Plot 1: Decision patterns over time\nax1.plot(example_ids, proposed_decisions, 'b-', alpha=0.7, linewidth=2, label='DKW Controller (1=Fusion, 0=Fission)')\nax1.fill_between(example_ids, proposed_decisions, alpha=0.3, color='blue')\nax1.set_ylabel('Decision')\nax1.set_title('DKW Controller Decisions Over Time')\nax1.set_ylim(-0.1, 1.1)\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Plot 2: Error occurrences\nax2.scatter(example_ids, errors, alpha=0.6, c='red', s=20)\nax2.set_ylabel('Error Occurred')\nax2.set_title('Error Occurrences Over Time')\nax2.set_ylim(-0.1, 1.1)\nax2.grid(True, alpha=0.3)\n\n# Plot 3: Running error rate\nwindow_size = 20\nrunning_error_rate = []\nfor i in range(len(errors)):\n    start_idx = max(0, i - window_size + 1)\n    window_errors = errors[start_idx:i+1]\n    running_error_rate.append(np.mean(window_errors))\n\nax3.plot(example_ids, running_error_rate, 'g-', linewidth=2, label='Running Error Rate (20-sample window)')\nax3.axhline(y=0.10, color='red', linestyle='--', alpha=0.7, label='Target Error Rate (0.10)')\nax3.set_xlabel('Example Index')\nax3.set_ylabel('Error Rate')\nax3.set_title('Running Error Rate vs Target')\nax3.legend()\nax3.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Experiment: What if we had a different proposed method?\n# Modify these parameters to see different scenarios\n\ndef create_experimental_results(n_examples=200, fusion_rate=0.8, error_rate=0.05):\n    \"\"\"Create experimental results with custom parameters.\"\"\"\n    results = []\n    n_fusion = int(n_examples * fusion_rate)\n    n_errors = int(n_examples * error_rate)\n    \n    for i in range(n_examples):\n        decision = \"fusion\" if i < n_fusion else \"fission\"\n        error = i < n_errors\n        results.append({\"decision\": decision, \"error\": error})\n    \n    return results\n\n# Try different scenarios\nexperimental_scenarios = {\n    \"baseline\": baseline_results,  # Keep baseline the same\n    \"high_fusion_low_error\": create_experimental_results(fusion_rate=0.9, error_rate=0.03),\n    \"medium_fusion\": create_experimental_results(fusion_rate=0.5, error_rate=0.06),\n    \"original_proposed\": proposed_results\n}\n\nprint(\"Comparing different scenarios:\\n\")\nfor scenario_name, scenario_results in experimental_scenarios.items():\n    if scenario_name == \"baseline\":\n        continue\n    \n    scenario_data = {\"baseline\": baseline_results, \"proposed\": scenario_results}\n    scenario_metrics = compute_metrics(scenario_data)\n    \n    print(f\"{scenario_name.upper()}:\")\n    print(f\"  API reduction: {scenario_metrics['improvement']['api_reduction_pct']:.1f}%\")\n    print(f\"  Error rate change: {scenario_metrics['improvement']['error_rate_diff']:+.1%}\")\n    print(f\"  Fusion rate: {scenario_metrics['proposed']['fusion_rate']:.1%}\")\n    print()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Results Analysis\n\nLet's analyze the behavior of both methods and visualize how the DKW controller adapts over time.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Experiment with Different Scenarios\n\nYou can modify the parameters below to see how different controller behaviors would affect the metrics:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Run the experiment\nresults = run_experiment(sample_data)\n\n# Display basic statistics\nprint(\"Experiment Results:\")\nprint(f\"Total examples processed: {len(results['baseline'])}\")\n\n# Count decisions for each method\nbaseline_fission = sum(1 for r in results['baseline'] if r['decision'] == 'fission')\nbaseline_fusion = sum(1 for r in results['baseline'] if r['decision'] == 'fusion')\n\nproposed_fission = sum(1 for r in results['proposed'] if r['decision'] == 'fission')\nproposed_fusion = sum(1 for r in results['proposed'] if r['decision'] == 'fusion')\n\nprint(f\"\\nBaseline method:\")\nprint(f\"  Fission decisions: {baseline_fission}\")\nprint(f\"  Fusion decisions: {baseline_fusion}\")\n\nprint(f\"\\nProposed DKW method:\")\nprint(f\"  Fission decisions: {proposed_fission}\")\nprint(f\"  Fusion decisions: {proposed_fusion}\")\n\n# Count errors for each method\nbaseline_errors = sum(1 for r in results['baseline'] if r['error'])\nproposed_errors = sum(1 for r in results['proposed'] if r['error'])\n\nprint(f\"\\nError counts:\")\nprint(f\"  Baseline: {baseline_errors} errors\")\nprint(f\"  Proposed: {proposed_errors} errors\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Compute metrics\nmetrics = compute_metrics(results)\n\n# Display results in a formatted way\nprint(\"=\"*50)\nprint(\"DKW CONTROLLER EVALUATION RESULTS\")\nprint(\"=\"*50)\n\nfor method in [\"baseline\", \"proposed\"]:\n    print(f\"\\n{method.upper()} METHOD:\")\n    m = metrics[method]\n    print(f\"  Fusion rate:     {m['fusion_rate']:.1%}\")\n    print(f\"  Fission rate:    {m['fission_rate']:.1%}\") \n    print(f\"  Error rate:      {m['error_rate']:.1%}\")\n    print(f\"  Total API calls: {m['api_calls']}\")\n    print(f\"  Avg calls/example: {m['avg_calls_per_example']:.2f}\")\n\nprint(f\"\\nIMPROVEMENT:\")\nprint(f\"  API reduction: {metrics['improvement']['api_reduction_pct']:.1f}%\")\nprint(f\"  Error rate change: {metrics['improvement']['error_rate_diff']:+.1%}\")\n\n# Also save as JSON (inline output)\nprint(f\"\\nFull metrics as JSON:\")\nprint(json.dumps(metrics, indent=2))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Run Experiment\n\nLet's execute the experiment and collect results from both the proposed DKW controller and the baseline always-fission approach.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def run_experiment(data):\n    \"\"\"Run DKW controller experiment.\"\"\"\n    controller = DKWController()\n    results = {\"baseline\": [], \"proposed\": []}\n\n    for example in data:\n        # Simulate error occurrence based on difficulty\n        error = np.random.random() < example[\"difficulty\"]\n        controller.add_observation(float(error))\n        decision = controller.decide()\n\n        results[\"proposed\"].append({\n            \"id\": example[\"id\"],\n            \"decision\": decision,\n            \"error\": error,\n        })\n        results[\"baseline\"].append({\n            \"id\": example[\"id\"],\n            \"decision\": \"fission\",  # Always conservative\n            \"error\": error,\n        })\n\n    return results",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Run Evaluation\n\nNow let's compute the metrics and display the results:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def compute_metrics(results: dict) -> dict:\n    \"\"\"Compute evaluation metrics.\"\"\"\n    metrics = {}\n\n    for method in [\"baseline\", \"proposed\"]:\n        preds = results[method]\n\n        # Count decisions\n        fusion_count = sum(1 for p in preds if p[\"decision\"] == \"fusion\")\n        fission_count = sum(1 for p in preds if p[\"decision\"] == \"fission\")\n\n        # Compute error rate\n        errors = sum(1 for p in preds if p[\"error\"])\n        error_rate = errors / len(preds)\n\n        # API calls (fusion=1, fission=2)\n        api_calls = fusion_count + 2 * fission_count\n\n        metrics[method] = {\n            \"fusion_rate\": fusion_count / len(preds),\n            \"fission_rate\": fission_count / len(preds),\n            \"error_rate\": error_rate,\n            \"api_calls\": api_calls,\n            \"avg_calls_per_example\": api_calls / len(preds),\n        }\n\n    # Compute improvement\n    baseline_calls = metrics[\"baseline\"][\"avg_calls_per_example\"]\n    proposed_calls = metrics[\"proposed\"][\"avg_calls_per_example\"]\n    metrics[\"improvement\"] = {\n        \"api_reduction_pct\": (baseline_calls - proposed_calls) / baseline_calls * 100,\n        \"error_rate_diff\": metrics[\"proposed\"][\"error_rate\"] - metrics[\"baseline\"][\"error_rate\"],\n    }\n\n    return metrics",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Experiment Function\n\nThe `run_experiment` function simulates the controller's behavior over a sequence of examples, comparing:\n- **Proposed method**: Uses DKW controller for adaptive decisions\n- **Baseline method**: Always uses conservative \"fission\" mode\n\nFor each example, we simulate whether an error occurs based on the difficulty level.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Sample data - normally read from \"../dataset_001/data_out.json\"\n# Create synthetic data with varying difficulty levels\nsample_data = []\n\n# Generate 300 examples with varying difficulty\nfor i in range(300):\n    # Create examples with different difficulty patterns\n    if i < 100:\n        difficulty = 0.05  # Easy examples (low error rate)\n    elif i < 200:\n        difficulty = 0.15  # Medium examples (moderate error rate)\n    else:\n        difficulty = 0.08  # Harder examples (but still manageable)\n    \n    sample_data.append({\n        \"id\": f\"example_{i:03d}\",\n        \"difficulty\": difficulty\n    })\n\nprint(f\"Created {len(sample_data)} sample examples\")\nprint(f\"Sample data preview: {sample_data[:3]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Evaluation Metrics Function\n\nThe `compute_metrics` function calculates several key performance indicators:\n\n- **Fusion/Fission rates**: Proportion of decisions for each strategy\n- **Error rate**: Percentage of incorrect decisions\n- **API calls**: Total API usage (fusion = 1 call, fission = 2 calls)\n- **Efficiency improvements**: Comparison between methods",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Sample Data\n\nWe'll create sample data to simulate the input that would normally be read from a JSON file. Each example has:\n- `id`: Unique identifier\n- `difficulty`: Probability of error occurring (0.0 to 1.0)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create sample experimental results\n# This data represents the decisions made by baseline vs proposed methods\n\n# Baseline method: always chooses fission, 8% error rate\nbaseline_results = []\nfor i in range(200):\n    baseline_results.append({\n        \"decision\": \"fission\",\n        \"error\": i < 16  # First 16 examples have errors (8% of 200)\n    })\n\n# Proposed method: 65% fusion, 35% fission, 9% error rate  \nproposed_results = []\nfor i in range(200):\n    if i < 130:  # First 130 examples use fusion (65% of 200)\n        decision = \"fusion\"\n    else:  # Remaining 70 examples use fission (35% of 200)\n        decision = \"fission\"\n    \n    proposed_results.append({\n        \"decision\": decision,\n        \"error\": i < 18  # First 18 examples have errors (9% of 200)\n    })\n\n# Combine into results structure\nresults = {\n    \"baseline\": baseline_results,\n    \"proposed\": proposed_results\n}\n\nprint(f\"Generated {len(baseline_results)} baseline results and {len(proposed_results)} proposed results\")\nprint(f\"Baseline decisions: {sum(1 for r in baseline_results if r['decision'] == 'fusion')} fusion, {sum(1 for r in baseline_results if r['decision'] == 'fission')} fission\")\nprint(f\"Proposed decisions: {sum(1 for r in proposed_results if r['decision'] == 'fusion')} fusion, {sum(1 for r in proposed_results if r['decision'] == 'fission')} fission\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "@dataclass\nclass DKWController:\n    \"\"\"DKW-guided fusion/fission controller.\"\"\"\n    epsilon_target: float = 0.10\n    delta: float = 0.05\n    min_samples: int = 100\n    hysteresis: float = 0.05\n\n    samples: list = field(default_factory=list)\n    current_state: str = \"fission\"\n\n    def dkw_epsilon(self, n: int) -> float:\n        \"\"\"Compute DKW epsilon for n samples.\"\"\"\n        if n < 2:\n            return 1.0\n        return np.sqrt(np.log(2 / self.delta) / (2 * n))\n\n    def add_observation(self, error: float) -> None:\n        \"\"\"Add error observation for calibration.\"\"\"\n        self.samples.append(error)\n\n    def decide(self) -> str:\n        \"\"\"Make fusion/fission decision with DKW guarantee.\"\"\"\n        n = len(self.samples)\n        if n < self.min_samples:\n            return self.current_state\n\n        epsilon = self.dkw_epsilon(n)\n        empirical_error = np.mean(self.samples[-self.min_samples:])\n        error_upper_bound = empirical_error + epsilon\n\n        if self.current_state == \"fusion\":\n            if error_upper_bound > self.epsilon_target + self.hysteresis:\n                self.current_state = \"fission\"\n        else:\n            if error_upper_bound < self.epsilon_target - self.hysteresis:\n                self.current_state = \"fusion\"\n\n        return self.current_state",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Sample Data\n\nInstead of reading from external JSON files, we'll create sample data inline that represents the experimental results from both baseline and proposed methods.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## DKW Controller Class\n\nThe core controller that implements the DKW-guided decision making algorithm.\n\n**Parameters:**\n- `epsilon_target`: Target error threshold (0.10)\n- `delta`: Confidence parameter for DKW bound (0.05)\n- `min_samples`: Minimum samples before making decisions (100)\n- `hysteresis`: Buffer to prevent oscillation (0.05)\n\n**Key Methods:**\n- `dkw_epsilon()`: Computes the DKW confidence interval width\n- `add_observation()`: Records error observations\n- `decide()`: Makes fusion/fission decision with statistical guarantees",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import json\nimport numpy as np",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# DKW Controller Evaluation\n\nThis notebook evaluates the performance of a DKW Controller, comparing baseline and proposed methods. The evaluation focuses on:\n- **Fusion vs Fission decisions**: The controller can choose to fuse or split operations\n- **API efficiency**: Fusion requires 1 API call, fission requires 2 API calls\n- **Error rates**: How often the controller makes incorrect decisions\n\nThe goal is to measure the improvement in API efficiency while maintaining acceptable error rates.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\"\"\"DKW Controller Implementation.\"\"\"\nimport json\nimport numpy as np\nfrom dataclasses import dataclass, field\nimport matplotlib.pyplot as plt\n\n# Set random seed for reproducibility\nnp.random.seed(42)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# DKW Controller Implementation Demo\n\nThis notebook demonstrates a **DKW-guided fusion/fission controller** that makes adaptive decisions based on error observations with statistical guarantees.\n\n## Overview\nThe DKW (Dvoretzky-Kiefer-Wolfowitz) inequality provides a way to bound the difference between empirical and true error rates, enabling principled decision-making between fusion and fission states.\n\n**Key Features:**\n- Statistical guarantees via DKW inequality\n- Adaptive switching between fusion/fission modes\n- Hysteresis to prevent oscillation\n- Real-time error calibration",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Collection for DKW Benchmark\n\n",
    "This notebook demonstrates dataset collection for DKW controller evaluation using the GSM8K dataset from HuggingFace. The script processes mathematical word problems and creates structured benchmark data with difficulty estimates.\n\n",
    "**Artifact ID:** dataset_001  \n",
    "**Original file:** data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n\n",
    "We'll need the `datasets` library to load data from HuggingFace and `json` for data serialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Dataset collection script for DKW benchmark.\"\"\"\n",
    "import json\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection Function\n\n",
    "The `collect_data()` function loads the GSM8K dataset from HuggingFace and processes it into a structured format suitable for benchmarking. Each example includes:\n",
    "- **id**: Unique identifier for the example\n",
    "- **question**: The mathematical word problem\n",
    "- **answer**: The correct answer\n",
    "- **difficulty**: A simple difficulty estimate based on question length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data():\n",
    "    \"\"\"Collect benchmark data for DKW controller evaluation.\"\"\"\n",
    "    # Load HuggingFace dataset\n",
    "    ds = load_dataset(\"gsm8k\", \"main\", split=\"test[:200]\")\n\n",
    "    data = []\n",
    "    for i, example in enumerate(ds):\n",
    "        data.append({\n",
    "            \"id\": f\"example_{i:03d}\",\n",
    "            \"question\": example[\"question\"],\n",
    "            \"answer\": example[\"answer\"],\n",
    "            \"difficulty\": len(example[\"question\"]) / 100,  # Simple proxy\n",
    "        })\n\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Data Collection\n\n",
    "Run the data collection function and display the results. This will download 200 examples from the GSM8K test set and process them into our benchmark format.\n\n",
    "**Note:** This is completely self-contained - no external files are needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the data\n",
    "data = collect_data()\n\n",
    "# Display results\n",
    "print(f\"Collected {len(data)} examples\")\n",
    "print(f\"\\nFirst 3 examples:\")\n",
    "for i in range(min(3, len(data))):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  ID: {data[i]['id']}\")\n",
    "    print(f\"  Question: {data[i]['question'][:100]}...\")\n",
    "    print(f\"  Answer: {data[i]['answer']}\")\n",
    "    print(f\"  Difficulty: {data[i]['difficulty']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data to JSON File\n\n",
    "Optionally save the collected data to a JSON file for later use. This mimics the original script's behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to JSON file (optional)\n",
    "with open(\"data_out.json\", \"w\") as f:\n",
    "    json.dump(data, f, indent=2)\n\n",
    "print(\"Data saved to 'data_out.json'\")\n\n",
    "# Show the JSON structure\n",
    "print(f\"\\nJSON file contains {len(data)} examples\")\n",
    "print(\"Sample JSON structure:\")\n",
    "print(json.dumps(data[:2], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Output Format\n\n",
    "Here's an example of what the processed data looks like. This demonstrates the expected structure and format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of processed data structure (for reference)\n",
    "sample_data = [\n",
    "    {\n",
    "        \"id\": \"example_000\",\n",
    "        \"question\": \"What is 2+2?\",\n",
    "        \"answer\": \"4\",\n",
    "        \"difficulty\": 0.15\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"example_001\",\n",
    "        \"question\": \"If x=5, what is 2x?\",\n",
    "        \"answer\": \"10\",\n",
    "        \"difficulty\": 0.22\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"example_002\",\n",
    "        \"question\": \"Solve: 3y + 6 = 15\",\n",
    "        \"answer\": \"y=3\",\n",
    "        \"difficulty\": 0.28\n",
    "    }\n",
    "]\n\n",
    "print(\"Sample data structure:\")\n",
    "print(json.dumps(sample_data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage and Customization\n\n",
    "This notebook is completely self-contained and ready to run! Here are some ways you can customize it:\n\n",
    "### Modify Dataset Parameters:\n",
    "- Change the number of examples: `split=\"test[:200]\"` â†’ `split=\"test[:500]\"`\n",
    "- Use different splits: `split=\"test\"` or `split=\"train\"`\n",
    "- Use validation set: `split=\"validation\"`\n\n",
    "### Adjust Difficulty Calculation:\n",
    "The current difficulty is based on question length. You could modify it to use:\n",
    "- Number of mathematical operations\n",
    "- Presence of certain keywords\n",
    "- Complexity scoring algorithms\n\n",
    "### Data Processing:\n",
    "- Add additional fields (e.g., topic classification, solution steps)\n",
    "- Filter examples by certain criteria\n",
    "- Apply text preprocessing\n\n",
    "### Running the Notebook:\n",
    "1. Make sure you have the required packages: `pip install datasets`\n",
    "2. Run all cells in order\n",
    "3. The data will be collected from HuggingFace automatically\n",
    "4. No external files needed!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}