{
 "cells": [
  {
   "cell_type": "code",
   "source": "# Optional: Save results to JSON file (as the original script did)\n# Uncomment the lines below if you want to save the results\n\n# with open(\"method_out.json\", \"w\") as f:\n#     json.dump(results, f, indent=2)\n# print(\"Results saved to method_out.json\")\n\n# Display sample results in JSON format\nprint(\"Sample results structure (first 3 entries each):\")\nprint(json.dumps({\n    \"baseline\": results[\"baseline\"][:3],\n    \"proposed\": results[\"proposed\"][:3]\n}, indent=2))\n\nprint(\"\\nðŸŽ‰ Notebook execution completed successfully!\")\nprint(\"\\nðŸ’¡ Key takeaways:\")\nprint(\"- The DKW controller adaptively switches between fusion and fission modes\")\nprint(\"- It provides statistical guarantees on error rates using the DKW inequality\")\nprint(\"- The controller starts conservatively and becomes more aggressive as it gains confidence\")\nprint(\"- Hysteresis prevents rapid oscillation between modes\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Saving Results (Optional)\n\nThe original script saved results to `method_out.json`. Here's how you can save the results if needed:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create visualization of controller behavior over time\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n\n# Extract decision timeline for proposed method\ndecisions_timeline = [r['decision'] for r in results['proposed']]\ndecision_numeric = [1 if d == 'fusion' else 0 for d in decisions_timeline]\nerrors_timeline = [r['error'] for r in results['proposed']]\n\n# Plot 1: Decision timeline\nx = range(len(decision_numeric))\nax1.plot(x, decision_numeric, 'b-', linewidth=2, label='Controller Decision')\nax1.fill_between(x, 0, decision_numeric, alpha=0.3, color='blue')\nax1.set_ylabel('Decision\\n(0=Fission, 1=Fusion)')\nax1.set_title('DKW Controller Decision Timeline')\nax1.grid(True, alpha=0.3)\nax1.legend()\n\n# Add error markers\nerror_indices = [i for i, error in enumerate(errors_timeline) if error]\nif error_indices:\n    ax1.scatter(error_indices, [decision_numeric[i] for i in error_indices], \n                color='red', s=50, marker='x', label='Error Occurred', zorder=5)\n    ax1.legend()\n\n# Plot 2: Cumulative error rate comparison\nbaseline_cumulative_errors = np.cumsum([r['error'] for r in results['baseline']])\nproposed_cumulative_errors = np.cumsum([r['error'] for r in results['proposed']])\nbaseline_error_rate = baseline_cumulative_errors / (np.arange(len(baseline_cumulative_errors)) + 1)\nproposed_error_rate = proposed_cumulative_errors / (np.arange(len(proposed_cumulative_errors)) + 1)\n\nax2.plot(x, baseline_error_rate, 'r--', linewidth=2, label='Baseline (Always Fission)')\nax2.plot(x, proposed_error_rate, 'g-', linewidth=2, label='Proposed (DKW Controller)')\nax2.axhline(y=0.10, color='black', linestyle=':', alpha=0.7, label='Target Error Rate (0.10)')\nax2.set_xlabel('Example Number')\nax2.set_ylabel('Cumulative Error Rate')\nax2.set_title('Error Rate Comparison Over Time')\nax2.grid(True, alpha=0.3)\nax2.legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Final error rates:\")\nprint(f\"  Baseline: {baseline_error_rate[-1]:.4f}\")\nprint(f\"  Proposed: {proposed_error_rate[-1]:.4f}\")\nprint(f\"  Target: 0.1000\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Results Visualization\n\nLet's visualize the controller's behavior over time to see how it adapts:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Run the experiment\nresults = run_experiment(sample_data)\n\nprint(\"Experiment completed!\")\nprint(f\"Total examples processed: {len(results['baseline'])}\")\n\n# Show first few results for comparison\nprint(\"\\n--- First 10 Results Comparison ---\")\nprint(\"ID\\t\\tBaseline\\tProposed\\tError\")\nprint(\"-\" * 50)\nfor i in range(10):\n    baseline_result = results['baseline'][i]\n    proposed_result = results['proposed'][i]\n    print(f\"{baseline_result['id']}\\t{baseline_result['decision']}\\t\\t{proposed_result['decision']}\\t\\t{baseline_result['error']}\")\n\n# Show summary statistics\nbaseline_fission_count = sum(1 for r in results['baseline'] if r['decision'] == 'fission')\nproposed_fission_count = sum(1 for r in results['proposed'] if r['decision'] == 'fission')\nproposed_fusion_count = sum(1 for r in results['proposed'] if r['decision'] == 'fusion')\n\nprint(f\"\\n--- Summary Statistics ---\")\nprint(f\"Baseline (always fission): {baseline_fission_count}/{len(results['baseline'])} fission decisions\")\nprint(f\"Proposed method: {proposed_fission_count}/{len(results['proposed'])} fission, {proposed_fusion_count}/{len(results['proposed'])} fusion decisions\")\n\n# Calculate error rates\nbaseline_errors = sum(1 for r in results['baseline'] if r['error'])\nproposed_errors = sum(1 for r in results['proposed'] if r['error'])\n\nprint(f\"\\nTotal errors encountered: baseline={baseline_errors}, proposed={proposed_errors}\")\nprint(f\"Error rate: baseline={baseline_errors/len(results['baseline']):.3f}, proposed={proposed_errors/len(results['proposed']):.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Running the Experiment\n\nNow let's run the experiment on our sample data and examine the results:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def run_experiment(data):\n    \"\"\"Run DKW controller experiment.\n    \n    Args:\n        data: List of examples, each with 'id' and 'difficulty' fields\n        \n    Returns:\n        Dictionary with 'baseline' and 'proposed' results\n    \"\"\"\n    controller = DKWController()\n    results = {\"baseline\": [], \"proposed\": []}\n\n    for example in data:\n        # Simulate error occurrence based on difficulty\n        error = np.random.random() < example[\"difficulty\"]\n        controller.add_observation(float(error))\n        decision = controller.decide()\n\n        results[\"proposed\"].append({\n            \"id\": example[\"id\"],\n            \"decision\": decision,\n            \"error\": error,\n        })\n        results[\"baseline\"].append({\n            \"id\": example[\"id\"],\n            \"decision\": \"fission\",  # Always conservative\n            \"error\": error,\n        })\n\n    return results\n\nprint(\"Experiment function defined successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Experiment Function\n\nThe `run_experiment` function simulates running both the proposed DKW controller and a baseline approach on the same data:\n\n- **Proposed method**: Uses the DKW controller to adaptively switch between fusion and fission\n- **Baseline method**: Always uses conservative fission mode\n\nFor each example, we simulate error occurrence based on the difficulty level (higher difficulty = higher probability of error).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Sample dataset - inlined for self-contained execution\n# Each example has an 'id' and a 'difficulty' value (probability of error occurrence)\nsample_data = [\n    {\"id\": \"example_000\", \"difficulty\": 0.05},  # Easy example (5% error probability)\n    {\"id\": \"example_001\", \"difficulty\": 0.08},  # Easy-medium example\n    {\"id\": \"example_002\", \"difficulty\": 0.15},  # Medium example (15% error probability)\n    {\"id\": \"example_003\", \"difficulty\": 0.12},  # Medium example\n    {\"id\": \"example_004\", \"difficulty\": 0.20},  # Hard example (20% error probability)\n    {\"id\": \"example_005\", \"difficulty\": 0.03},  # Very easy example\n    {\"id\": \"example_006\", \"difficulty\": 0.25},  # Very hard example\n    {\"id\": \"example_007\", \"difficulty\": 0.10},  # Medium example\n    {\"id\": \"example_008\", \"difficulty\": 0.18},  # Hard example\n    {\"id\": \"example_009\", \"difficulty\": 0.07},  # Easy example\n] * 12  # Repeat to have 120 examples total (more than min_samples = 100)\n\nprint(f\"Created sample dataset with {len(sample_data)} examples\")\nprint(\"Sample examples:\")\nfor i, example in enumerate(sample_data[:5]):\n    print(f\"  {i}: {example}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Sample Data\n\nSince this is a self-contained notebook, we'll inline the sample data directly instead of reading from external files. The original script would read from `../dataset_001/data_out.json`.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "@dataclass\nclass DKWController:\n    \"\"\"DKW-guided fusion/fission controller.\"\"\"\n    epsilon_target: float = 0.10\n    delta: float = 0.05\n    min_samples: int = 100\n    hysteresis: float = 0.05\n\n    samples: list = field(default_factory=list)\n    current_state: str = \"fission\"\n\n    def dkw_epsilon(self, n: int) -> float:\n        \"\"\"Compute DKW epsilon for n samples.\"\"\"\n        if n < 2:\n            return 1.0\n        return np.sqrt(np.log(2 / self.delta) / (2 * n))\n\n    def add_observation(self, error: float) -> None:\n        \"\"\"Add error observation for calibration.\"\"\"\n        self.samples.append(error)\n\n    def decide(self) -> str:\n        \"\"\"Make fusion/fission decision with DKW guarantee.\"\"\"\n        n = len(self.samples)\n        if n < self.min_samples:\n            return self.current_state\n\n        epsilon = self.dkw_epsilon(n)\n        empirical_error = np.mean(self.samples[-self.min_samples:])\n        error_upper_bound = empirical_error + epsilon\n\n        if self.current_state == \"fusion\":\n            if error_upper_bound > self.epsilon_target + self.hysteresis:\n                self.current_state = \"fission\"\n        else:\n            if error_upper_bound < self.epsilon_target - self.hysteresis:\n                self.current_state = \"fusion\"\n\n        return self.current_state\n\n# Test the controller\ncontroller = DKWController()\nprint(f\"Controller initialized with state: {controller.current_state}\")\nprint(f\"Target error: {controller.epsilon_target}\")\nprint(f\"Min samples needed: {controller.min_samples}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## DKW Controller Class\n\nThe `DKWController` class implements a fusion/fission controller that uses the **Dvoretzky-Kiefer-Wolfowitz (DKW) inequality** to provide statistical guarantees.\n\n### Key Parameters:\n- `epsilon_target`: Target error threshold (default: 0.10)\n- `delta`: Confidence parameter for DKW bound (default: 0.05)\n- `min_samples`: Minimum samples before making decisions (default: 100)\n- `hysteresis`: Prevents oscillation between states (default: 0.05)\n\n### How it works:\n1. Collects error observations over time\n2. Uses DKW inequality to compute error upper bound with confidence 1-Î´\n3. Switches between fusion (aggressive) and fission (conservative) modes based on whether the error bound exceeds the target",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import json\nimport numpy as np\nfrom dataclasses import dataclass, field\nimport matplotlib.pyplot as plt\n\n# Set random seed for reproducible results\nnp.random.seed(42)\nprint(\"Libraries imported successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Required Imports\n\nFirst, let's import the necessary libraries:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# DKW Controller Implementation - method.py\n\nThis notebook demonstrates a **DKW-guided fusion/fission controller** implementation. The controller uses the Dvoretzky-Kiefer-Wolfowitz (DKW) inequality to make statistically guaranteed decisions about when to use fusion vs fission modes based on error observations.\n\n## Overview\n- **DKW Controller**: A class that maintains error statistics and makes fusion/fission decisions\n- **Experiment**: Simulates running the controller on sample data\n- **Analysis**: Compares the proposed method against a baseline approach",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}