{
 "cells": [
  {
   "cell_type": "code",
   "source": "# Interactive parameter exploration function\ndef run_experiment_with_params(epsilon_target=0.10, delta=0.05, min_samples=100, hysteresis=0.05):\n    \"\"\"Run experiment with custom parameters.\"\"\"\n    \n    # Create controller with custom parameters\n    controller = DKWController(\n        epsilon_target=epsilon_target,\n        delta=delta,\n        min_samples=min_samples,\n        hysteresis=hysteresis\n    )\n    \n    results = {\"baseline\": [], \"proposed\": []}\n    \n    for example in sample_data:\n        error = np.random.random() < example[\"difficulty\"]\n        controller.add_observation(float(error))\n        decision = controller.decide()\n\n        results[\"proposed\"].append({\n            \"id\": example[\"id\"],\n            \"decision\": decision,\n            \"error\": error,\n            \"empirical_error\": np.mean(controller.samples) if controller.samples else 0.0\n        })\n        results[\"baseline\"].append({\n            \"id\": example[\"id\"],\n            \"decision\": \"fission\",\n            \"error\": error,\n        })\n    \n    # Quick analysis\n    proposed_df = pd.DataFrame(results[\"proposed\"])\n    baseline_df = pd.DataFrame(results[\"baseline\"])\n    \n    fusion_rate = (proposed_df['decision'] == 'fusion').mean()\n    error_rate = proposed_df['error'].mean()\n    \n    print(f\"Parameters: target={epsilon_target}, δ={delta}, min_samples={min_samples}, hysteresis={hysteresis}\")\n    print(f\"Results: {fusion_rate:.1%} fusion decisions, {error_rate:.3f} error rate\")\n    \n    return results, proposed_df\n\n# Example: Try different target error rates\nprint(\"=== Comparing Different Target Error Rates ===\")\nfor target in [0.05, 0.10, 0.15]:\n    run_experiment_with_params(epsilon_target=target)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Interactive Parameter Exploration\n\nTry modifying the controller parameters below and re-run the experiment to see how they affect performance:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create visualizations\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Plot 1: Decision timeline for proposed method\naxes[0,0].plot(range(len(proposed_df)), proposed_df['decision'].map({'fission': 0, 'fusion': 1}), 'b-', linewidth=2)\naxes[0,0].set_title('DKW Controller Decision Timeline')\naxes[0,0].set_xlabel('Example Index')\naxes[0,0].set_ylabel('Decision (0=Fission, 1=Fusion)')\naxes[0,0].grid(True, alpha=0.3)\n\n# Plot 2: Empirical error rate over time\naxes[0,1].plot(range(len(proposed_df)), proposed_df['empirical_error'], 'r-', linewidth=2, label='Empirical Error')\naxes[0,1].axhline(y=0.10, color='g', linestyle='--', label='Target (10%)')\naxes[0,1].set_title('Empirical Error Rate Over Time')\naxes[0,1].set_xlabel('Example Index')\naxes[0,1].set_ylabel('Error Rate')\naxes[0,1].legend()\naxes[0,1].grid(True, alpha=0.3)\n\n# Plot 3: Error comparison histogram\nerror_comparison = pd.DataFrame({\n    'Baseline': baseline_df['error'].astype(int),\n    'Proposed': proposed_df['error'].astype(int)\n})\n\n# Count errors vs correct for each method\nbaseline_counts = baseline_df['error'].value_counts().sort_index()\nproposed_counts = proposed_df['error'].value_counts().sort_index()\n\nx = ['Correct', 'Error']\nbaseline_vals = [baseline_counts.get(False, 0), baseline_counts.get(True, 0)]\nproposed_vals = [proposed_counts.get(False, 0), proposed_counts.get(True, 0)]\n\nx_pos = np.arange(len(x))\nwidth = 0.35\n\naxes[1,0].bar(x_pos - width/2, baseline_vals, width, label='Baseline', alpha=0.7)\naxes[1,0].bar(x_pos + width/2, proposed_vals, width, label='Proposed', alpha=0.7)\naxes[1,0].set_title('Error Comparison')\naxes[1,0].set_xlabel('Outcome')\naxes[1,0].set_ylabel('Count')\naxes[1,0].set_xticks(x_pos)\naxes[1,0].set_xticklabels(x)\naxes[1,0].legend()\naxes[1,0].grid(True, alpha=0.3)\n\n# Plot 4: Decision distribution pie chart\ndecision_counts = proposed_df['decision'].value_counts()\naxes[1,1].pie(decision_counts.values, labels=decision_counts.index, autopct='%1.1f%%', startangle=90)\naxes[1,1].set_title('Decision Distribution (DKW Controller)')\n\nplt.tight_layout()\nplt.show()\n\n# Calculate and display key metrics\nprint(\"\\\\n=== Performance Metrics ===\")\nprint(f\"Total examples: {len(proposed_df)}\")\nprint(f\"Final empirical error rate: {proposed_df['empirical_error'].iloc[-1]:.4f}\")\n\nfusion_decisions = (proposed_df['decision'] == 'fusion').sum()\nfission_decisions = (proposed_df['decision'] == 'fission').sum()\nprint(f\"Fusion decisions: {fusion_decisions} ({fusion_decisions/len(proposed_df)*100:.1f}%)\")\nprint(f\"Fission decisions: {fission_decisions} ({fission_decisions/len(proposed_df)*100:.1f}%)\")\n\n# Calculate efficiency gains (assuming fusion is more efficient when safe)\nbaseline_always_conservative = len(baseline_df)  # Always uses fission (conservative)\nproposed_fusion_usage = fusion_decisions\nefficiency_gain = proposed_fusion_usage / baseline_always_conservative * 100\nprint(f\"Efficiency gain over baseline: {efficiency_gain:.1f}% more fusion usage\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Results Analysis and Visualization\n\nLet's visualize the results to better understand how the DKW controller behaves:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Run the experiment\nresults = run_experiment(sample_data)\n\n# Convert to DataFrames for easier analysis\nbaseline_df = pd.DataFrame(results[\"baseline\"])\nproposed_df = pd.DataFrame(results[\"proposed\"])\n\nprint(\"Experiment completed!\")\nprint(f\"Total examples processed: {len(baseline_df)}\")\nprint(f\"Baseline (always fission) error rate: {baseline_df['error'].mean():.3f}\")\nprint(f\"Proposed method error rate: {proposed_df['error'].mean():.3f}\")\n\n# Show decision distribution\nprint(f\"\\nBaseline decisions: {baseline_df['decision'].value_counts().to_dict()}\")\nprint(f\"Proposed decisions: {proposed_df['decision'].value_counts().to_dict()}\")\n\n# Show first few results\nprint(\"\\nFirst 10 results (Proposed method):\")\nprint(proposed_df[['id', 'decision', 'error', 'difficulty', 'samples_so_far', 'empirical_error']].head(10))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Run the Experiment\n\nLet's run our experiment and compare the DKW controller against the conservative baseline:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def run_experiment(data):\n    \"\"\"Run DKW controller experiment with inlined data.\"\"\"\n    controller = DKWController()\n    results = {\"baseline\": [], \"proposed\": []}\n\n    for example in data:\n        # Simulate error occurrence based on difficulty\n        error = np.random.random() < example[\"difficulty\"]\n        controller.add_observation(float(error))\n        decision = controller.decide()\n\n        results[\"proposed\"].append({\n            \"id\": example[\"id\"],\n            \"decision\": decision,\n            \"error\": error,\n            \"difficulty\": example[\"difficulty\"],\n            \"samples_so_far\": len(controller.samples),\n            \"empirical_error\": np.mean(controller.samples) if controller.samples else 0.0\n        })\n        results[\"baseline\"].append({\n            \"id\": example[\"id\"],\n            \"decision\": \"fission\",  # Always conservative\n            \"error\": error,\n            \"difficulty\": example[\"difficulty\"]\n        })\n\n    return results\n\nprint(\"Experiment function defined successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 4. Experiment Function\n\nNow let's define the experiment function that will test both our DKW controller and a conservative baseline that always chooses \"fission\".",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Sample dataset (inlined from ../dataset_001/data_out.json)\n# Each example has an ID and a difficulty level (probability of error)\nsample_data = [\n    {\"id\": \"example_000\", \"difficulty\": 0.02},  # Easy example\n    {\"id\": \"example_001\", \"difficulty\": 0.05},  # Medium-easy example  \n    {\"id\": \"example_002\", \"difficulty\": 0.15},  # Harder example\n    {\"id\": \"example_003\", \"difficulty\": 0.08},  # Medium example\n    {\"id\": \"example_004\", \"difficulty\": 0.03},  # Easy example\n    {\"id\": \"example_005\", \"difficulty\": 0.12},  # Hard example\n    {\"id\": \"example_006\", \"difficulty\": 0.06},  # Medium example\n    {\"id\": \"example_007\", \"difficulty\": 0.18},  # Very hard example\n    {\"id\": \"example_008\", \"difficulty\": 0.04},  # Easy example\n    {\"id\": \"example_009\", \"difficulty\": 0.09},  # Medium example\n]\n\n# Extend the dataset with more examples for a more realistic experiment\nfor i in range(10, 150):\n    # Create a mix of difficulties following a realistic distribution\n    difficulty = np.random.beta(2, 8)  # Skewed toward lower difficulties\n    sample_data.append({\n        \"id\": f\"example_{i:03d}\",\n        \"difficulty\": min(difficulty, 0.25)  # Cap at 25% max difficulty\n    })\n\nprint(f\"Generated {len(sample_data)} examples\")\nprint(f\"Difficulty range: {min(ex['difficulty'] for ex in sample_data):.3f} - {max(ex['difficulty'] for ex in sample_data):.3f}\")\nprint(f\"Average difficulty: {np.mean([ex['difficulty'] for ex in sample_data]):.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Sample Data (Inlined)\n\nInstead of reading from external files, we'll define our sample data directly in the notebook. This data represents examples with varying difficulty levels that will be used to test our controller.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "@dataclass\nclass DKWController:\n    \"\"\"DKW-guided fusion/fission controller.\"\"\"\n    epsilon_target: float = 0.10\n    delta: float = 0.05\n    min_samples: int = 100\n    hysteresis: float = 0.05\n\n    samples: list = field(default_factory=list)\n    current_state: str = \"fission\"\n\n    def dkw_epsilon(self, n: int) -> float:\n        \"\"\"Compute DKW epsilon for n samples.\"\"\"\n        if n < 2:\n            return 1.0\n        return np.sqrt(np.log(2 / self.delta) / (2 * n))\n\n    def add_observation(self, error: float) -> None:\n        \"\"\"Add error observation for calibration.\"\"\"\n        self.samples.append(error)\n\n    def decide(self) -> str:\n        \"\"\"Make fusion/fission decision with DKW guarantee.\"\"\"\n        n = len(self.samples)\n        if n < self.min_samples:\n            return self.current_state\n\n        epsilon = self.dkw_epsilon(n)\n        empirical_error = np.mean(self.samples[-self.min_samples:])\n        error_upper_bound = empirical_error + epsilon\n\n        if self.current_state == \"fusion\":\n            if error_upper_bound > self.epsilon_target + self.hysteresis:\n                self.current_state = \"fission\"\n        else:\n            if error_upper_bound < self.epsilon_target - self.hysteresis:\n                self.current_state = \"fusion\"\n\n        return self.current_state\n\n# Test the controller with basic functionality\ncontroller = DKWController()\nprint(f\"Initial state: {controller.current_state}\")\nprint(f\"DKW epsilon for 100 samples: {controller.dkw_epsilon(100):.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 2. DKW Controller Class\n\nThe core of our implementation is the `DKWController` class. This controller uses the **Dvoretzky-Kiefer-Wolfowitz inequality** to bound the error rate with high confidence.\n\n### Key Parameters:\n- `epsilon_target`: Target error rate threshold (10% by default)\n- `delta`: Confidence parameter for DKW bound (95% confidence with δ=0.05)\n- `min_samples`: Minimum observations before making decisions\n- `hysteresis`: Prevents oscillation between states",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import json\nimport numpy as np\nfrom dataclasses import dataclass, field\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\nprint(\"All imports successful!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Imports and Setup\n\nFirst, let's import the required libraries:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# DKW Controller Implementation - Interactive Demo\n\nThis notebook demonstrates a **DKW-guided fusion/fission controller** implementation from `method.py`. \n\nThe controller uses the **Dvoretzky-Kiefer-Wolfowitz (DKW) inequality** to provide statistical guarantees when deciding between \"fusion\" and \"fission\" modes based on observed error rates.\n\n## Key Features:\n- Statistical error bounds using DKW inequality\n- Adaptive decision making with hysteresis\n- Comparison between proposed method and conservative baseline",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}