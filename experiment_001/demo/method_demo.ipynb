{
 "cells": [
  {
   "cell_type": "code",
   "source": "# ðŸ’¾ Optional: Save results to JSON file (matches original script output)\nsave_results = True  # Set to False if you don't want to save\n\nif save_results:\n    # Format results to match original script output structure\n    output_results = {\n        \"baseline\": [\n            {\n                \"id\": r[\"id\"],\n                \"decision\": r[\"decision\"], \n                \"error\": r[\"error\"]\n            }\n            for r in results[\"baseline\"]\n        ],\n        \"proposed\": [\n            {\n                \"id\": r[\"id\"],\n                \"decision\": r[\"decision\"],\n                \"error\": r[\"error\"]\n            }\n            for r in results[\"proposed\"]\n        ]\n    }\n    \n    # Save to file\n    with open(\"method_out.json\", \"w\") as f:\n        json.dump(output_results, f, indent=2)\n    \n    print(\"âœ… Results saved to 'method_out.json'\")\n    print(f\"   Baseline results: {len(output_results['baseline'])} entries\")\n    print(f\"   Proposed results: {len(output_results['proposed'])} entries\")\nelse:\n    print(\"ðŸ’¡ Set save_results=True above to save results to JSON file\")\n\nprint()\nprint(\"ðŸŽ‰ Notebook complete! The DKW Controller has been successfully demonstrated.\")\nprint(\"   Feel free to modify parameters and re-run cells to explore different behaviors.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Conclusions & Next Steps\n\n### Key Findings:\n1. **Adaptive Control**: The DKW controller automatically adjusts between fusion and fission based on observed error rates\n2. **Statistical Guarantees**: The DKW bound provides high-confidence upper bounds on true error rates\n3. **Hysteresis Effect**: Prevents rapid switching between modes, providing stability\n4. **Trade-offs**: More fusion usage can improve efficiency but may increase error rates\n\n### Parameters to Experiment With:\n- **`epsilon_target`**: Lower values = more conservative, higher values = more aggressive\n- **`delta`**: Lower values = higher confidence but tighter bounds  \n- **`min_samples`**: Higher values = more data before decisions but slower adaptation\n- **`hysteresis`**: Higher values = less switching but potentially suboptimal decisions\n\n### Potential Applications:\n- Adaptive system configuration\n- Quality vs. speed trade-offs in ML pipelines\n- Resource allocation under uncertainty\n- A/B testing with statistical guarantees",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ðŸ§ª INTERACTIVE EXPERIMENT - Try different parameters!\n\n# Modify these parameters and re-run to see the impact\nCUSTOM_EPSILON_TARGET = 0.08   # Try values like 0.05, 0.10, 0.15\nCUSTOM_DELTA = 0.05           # Try values like 0.01, 0.05, 0.10  \nCUSTOM_MIN_SAMPLES = 75       # Try values like 50, 100, 150\nCUSTOM_HYSTERESIS = 0.03      # Try values like 0.01, 0.05, 0.10\n\n# Create custom controller\ncustom_controller = DKWController(\n    epsilon_target=CUSTOM_EPSILON_TARGET,\n    delta=CUSTOM_DELTA, \n    min_samples=CUSTOM_MIN_SAMPLES,\n    hysteresis=CUSTOM_HYSTERESIS\n)\n\nprint(f\"ðŸ”¬ Testing custom parameters:\")\nprint(f\"  Target error rate: {CUSTOM_EPSILON_TARGET:.1%}\")\nprint(f\"  Confidence level: {100*(1-CUSTOM_DELTA):.0f}%\") \nprint(f\"  Min samples: {CUSTOM_MIN_SAMPLES}\")\nprint(f\"  Hysteresis: {CUSTOM_HYSTERESIS:.3f}\")\nprint()\n\n# Quick simulation with custom parameters\ncustom_results = {\"decisions\": [], \"errors\": [], \"state_changes\": 0}\nprevious_decision = \"fission\"\n\nfor i, example in enumerate(sample_data[:200]):  # Test on first 200 examples\n    error = np.random.random() < example[\"difficulty\"]\n    custom_controller.add_observation(float(error))\n    decision = custom_controller.decide()\n    \n    custom_results[\"decisions\"].append(decision)\n    custom_results[\"errors\"].append(error)\n    \n    if decision != previous_decision:\n        custom_results[\"state_changes\"] += 1\n        print(f\"  State change at example {i}: {previous_decision} â†’ {decision}\")\n        previous_decision = decision\n\ncustom_fusion_rate = sum(1 for d in custom_results[\"decisions\"] if d == \"fusion\") / len(custom_results[\"decisions\"])\ncustom_error_rate = sum(custom_results[\"errors\"]) / len(custom_results[\"errors\"])\n\nprint()\nprint(f\"ðŸ“Š Custom controller results (first 200 examples):\")\nprint(f\"  Fusion rate: {custom_fusion_rate:.1%}\")\nprint(f\"  Error rate: {custom_error_rate:.3f}\")\nprint(f\"  State changes: {custom_results['state_changes']}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Interactive Exploration\n\nTry modifying the parameters below to see how they affect the controller's behavior!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create visualizations\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n\n# Extract data for plotting\nexamples = range(len(results[\"proposed\"]))\ndifficulties = [r[\"difficulty\"] for r in results[\"proposed\"]]\ndecisions = [1 if r[\"decision\"] == \"fusion\" else 0 for r in results[\"proposed\"]]\nerrors = [1 if r[\"error\"] else 0 for r in results[\"proposed\"]]\nempirical_errors = [r[\"empirical_error\"] for r in results[\"proposed\"]]\ndkw_bounds = [r[\"dkw_bound\"] for r in results[\"proposed\"]]\n\n# Plot 1: Controller decisions over time\nax1.fill_between(examples, decisions, alpha=0.7, label=\"DKW Controller\")\nax1.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\nax1.set_xlabel(\"Example Index\")\nax1.set_ylabel(\"Decision (0=Fission, 1=Fusion)\")\nax1.set_title(\"Controller Decisions Over Time\")\nax1.set_ylim(-0.1, 1.1)\nax1.legend()\n\n# Plot 2: Difficulty and errors\nax2.scatter(examples, difficulties, alpha=0.6, s=10, label=\"Difficulty\", color='orange')\nax2.scatter([i for i, e in enumerate(errors) if e], [difficulties[i] for i, e in enumerate(errors) if e], \n           s=20, color='red', alpha=0.8, label=\"Errors\", marker='x')\nax2.axhline(y=0.1, color='blue', linestyle='--', alpha=0.7, label=\"Target Error Rate\")\nax2.set_xlabel(\"Example Index\")\nax2.set_ylabel(\"Difficulty / Error Indicator\")\nax2.set_title(\"Difficulty and Error Distribution\")\nax2.legend()\n\n# Plot 3: Empirical error rate with DKW bounds\nwindow_size = 50\nrolling_error = []\nfor i in range(len(empirical_errors)):\n    start_idx = max(0, i - window_size + 1)\n    rolling_error.append(np.mean(errors[start_idx:i+1]))\n\nax3.plot(examples, rolling_error, label=\"Rolling Error Rate\", color='red', linewidth=2)\nax3.plot(examples, [r + b for r, b in zip(empirical_errors, dkw_bounds)], \n         label=\"Empirical + DKW Bound\", color='orange', alpha=0.8)\nax3.axhline(y=0.1, color='blue', linestyle='--', label=\"Target (10%)\")\nax3.axhline(y=0.15, color='blue', linestyle=':', alpha=0.7, label=\"Target + Hysteresis\")\nax3.axhline(y=0.05, color='blue', linestyle=':', alpha=0.7, label=\"Target - Hysteresis\")\nax3.set_xlabel(\"Example Index\")\nax3.set_ylabel(\"Error Rate\")\nax3.set_title(\"Error Rate Evolution with DKW Bounds\")\nax3.legend()\n\n# Plot 4: Performance comparison\nmethods = ['Baseline\\n(Always Fission)', 'Proposed\\n(DKW Controller)']\nerror_rates = [baseline_error_rate, proposed_error_rate]\nfusion_rates = [baseline_fusion_count/len(results['baseline']), proposed_fusion_count/len(results['proposed'])]\n\nx = np.arange(len(methods))\nwidth = 0.35\n\nbars1 = ax4.bar(x - width/2, error_rates, width, label='Error Rate', color='lightcoral')\nbars2 = ax4.bar(x + width/2, fusion_rates, width, label='Fusion Rate', color='lightblue')\n\nax4.set_xlabel(\"Method\")\nax4.set_ylabel(\"Rate\")\nax4.set_title(\"Performance Comparison\")\nax4.set_xticks(x)\nax4.set_xticklabels(methods)\nax4.legend()\n\n# Add value labels on bars\nfor bars in [bars1, bars2]:\n    for bar in bars:\n        height = bar.get_height()\n        ax4.annotate(f'{height:.3f}',\n                    xy=(bar.get_x() + bar.get_width() / 2, height),\n                    xytext=(0, 3),  # 3 points vertical offset\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nðŸ“ˆ Visualization shows:\")\nprint(\"â€¢ Top-left: When the controller chose fusion vs fission\")\nprint(\"â€¢ Top-right: Example difficulty and where errors occurred\") \nprint(\"â€¢ Bottom-left: How error rates evolved with statistical bounds\")\nprint(\"â€¢ Bottom-right: Overall performance comparison\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Visualization\n\nLet's create visualizations to better understand the controller's behavior over time.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Basic statistics\nbaseline_errors = sum(1 for r in results[\"baseline\"] if r[\"error\"])\nproposed_errors = sum(1 for r in results[\"proposed\"] if r[\"error\"])\n\nbaseline_error_rate = baseline_errors / len(results[\"baseline\"])\nproposed_error_rate = proposed_errors / len(results[\"proposed\"])\n\n# Count decisions\nbaseline_fusion_count = sum(1 for r in results[\"baseline\"] if r[\"decision\"] == \"fusion\")\nproposed_fusion_count = sum(1 for r in results[\"proposed\"] if r[\"decision\"] == \"fusion\")\n\nprint(\"ðŸ“Š EXPERIMENT RESULTS\")\nprint(\"=\" * 50)\nprint(f\"Total examples processed: {len(results['baseline'])}\")\nprint()\n\nprint(\"ðŸŽ¯ ERROR RATES:\")\nprint(f\"  Baseline (always fission): {baseline_error_rate:.3f} ({baseline_errors}/{len(results['baseline'])})\")\nprint(f\"  Proposed (DKW controller): {proposed_error_rate:.3f} ({proposed_errors}/{len(results['proposed'])})\")\nprint()\n\nprint(\"âš¡ FUSION USAGE:\")\nprint(f\"  Baseline: {baseline_fusion_count} fusion decisions ({baseline_fusion_count/len(results['baseline']):.1%})\")\nprint(f\"  Proposed: {proposed_fusion_count} fusion decisions ({proposed_fusion_count/len(results['proposed']):.1%})\")\nprint()\n\nprint(\"ðŸ”„ STATE CHANGES:\")\nif state_changes:\n    print(f\"  Total changes: {len(state_changes)}\")\n    for change in state_changes:\n        print(f\"    Example {change['example_id']:3d}: {change['from_state']} â†’ {change['to_state']} \" +\n              f\"(samples: {change['samples_so_far']}, error rate: {change['empirical_error']:.3f})\")\nelse:\n    print(\"  No state changes occurred\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Results Analysis\n\nLet's analyze the performance of both approaches and visualize the controller's behavior.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def run_experiment(data):\n    \"\"\"Run DKW controller experiment.\n    \n    Args:\n        data: List of examples with 'id' and 'difficulty' fields\n        \n    Returns:\n        Dictionary with 'baseline' and 'proposed' results\n    \"\"\"\n    controller = DKWController()\n    results = {\"baseline\": [], \"proposed\": []}\n    \n    # Track controller state changes for analysis\n    state_changes = []\n    \n    for i, example in enumerate(data):\n        # Simulate error occurrence based on difficulty\n        # Higher difficulty = higher probability of error\n        error = np.random.random() < example[\"difficulty\"]\n        \n        # Add observation to controller and get decision\n        controller.add_observation(float(error))\n        decision = controller.decide()\n        \n        # Track state changes\n        if i > 0 and decision != results[\"proposed\"][-1][\"decision\"]:\n            state_changes.append({\n                \"example_id\": i,\n                \"from_state\": results[\"proposed\"][-1][\"decision\"],\n                \"to_state\": decision,\n                \"samples_so_far\": len(controller.samples),\n                \"empirical_error\": np.mean(controller.samples) if controller.samples else 0\n            })\n\n        # Store results for proposed method (DKW controller)\n        results[\"proposed\"].append({\n            \"id\": example[\"id\"],\n            \"decision\": decision,\n            \"error\": error,\n            \"difficulty\": example[\"difficulty\"],\n            \"empirical_error\": np.mean(controller.samples) if controller.samples else 0,\n            \"dkw_bound\": controller.dkw_epsilon(len(controller.samples)) if controller.samples else 1.0\n        })\n        \n        # Store results for baseline (always fission)\n        results[\"baseline\"].append({\n            \"id\": example[\"id\"],\n            \"decision\": \"fission\",  # Always conservative\n            \"error\": error,\n            \"difficulty\": example[\"difficulty\"]\n        })\n\n    return results, state_changes\n\n# Run the experiment\nprint(\"ðŸš€ Running experiment...\")\nresults, state_changes = run_experiment(sample_data)\n\nprint(f\"âœ… Experiment completed!\")\nprint(f\"Processed {len(results['proposed'])} examples\")\nprint(f\"Controller made {len(state_changes)} state changes\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Experiment Function\n\nThe experiment compares two approaches:\n1. **Baseline**: Always uses fission (conservative) mode\n2. **Proposed**: Uses the DKW controller to adaptively switch between fusion and fission\n\nBoth approaches process the same sequence of examples and observe the same errors (based on difficulty).",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Usage Notes\n\n- **Self-contained**: This notebook requires no external files - all sample data is inlined\n- **Customizable**: You can modify the data collection parameters (e.g., change `test[:200]` to get more/fewer examples)\n- **Interactive**: Run cells individually to explore the data step by step\n- **Original functionality**: The core logic matches the original `data.py` script\n\nTo get started, simply run all cells from top to bottom!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create sample dataset inline (replaces external JSON file)\n# This simulates examples with different difficulty levels\n\n# Generate a realistic dataset with varying difficulty\nnp.random.seed(42)  # For reproducibility\nn_examples = 500\n\nsample_data = []\nfor i in range(n_examples):\n    # Create varying difficulty patterns:\n    # - Early examples: low difficulty (0.05-0.15)\n    # - Middle examples: medium difficulty (0.08-0.25) \n    # - Later examples: high difficulty (0.15-0.35)\n    \n    if i < n_examples // 3:\n        difficulty = np.random.uniform(0.05, 0.15)  # Easy examples\n    elif i < 2 * n_examples // 3:\n        difficulty = np.random.uniform(0.08, 0.25)  # Medium examples\n    else:\n        difficulty = np.random.uniform(0.15, 0.35)  # Hard examples\n    \n    sample_data.append({\n        \"id\": f\"example_{i:03d}\",\n        \"difficulty\": difficulty\n    })\n\nprint(f\"Created {len(sample_data)} examples\")\nprint(f\"Difficulty range: {min(ex['difficulty'] for ex in sample_data):.3f} - {max(ex['difficulty'] for ex in sample_data):.3f}\")\n\n# Show first few examples\nprint(\"\\nFirst 5 examples:\")\nfor ex in sample_data[:5]:\n    print(f\"  {ex['id']}: difficulty = {ex['difficulty']:.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Optional: Save the collected data to a JSON file\n# Uncomment the lines below if you want to save the data\n\n# with open(\"data_out.json\", \"w\") as f:\n#     json.dump(data, f, indent=2)\n# print(f\"Saved {len(data)} examples to data_out.json\")\n\nprint(\"To save data, uncomment the lines above and run this cell\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Optional: Save Data to File\n\nIf you want to save the collected data to a JSON file (as the original script did), you can run the following cell:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Sample Dataset\n\nInstead of reading from external files, we'll create sample data inline. The dataset simulates examples with varying difficulty levels that influence error probability.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Sample data that would have been written to data_out.json\n# This is inlined to make the notebook self-contained\nsample_output_data = [\n    {\n        \"id\": \"example_000\",\n        \"question\": \"What is 2+2?\",\n        \"answer\": \"4\",\n        \"difficulty\": 0.15\n    },\n    {\n        \"id\": \"example_001\",\n        \"question\": \"If x=5, what is 2x?\",\n        \"answer\": \"10\",\n        \"difficulty\": 0.22\n    },\n    {\n        \"id\": \"example_002\",\n        \"question\": \"Solve: 3y + 6 = 15\",\n        \"answer\": \"y=3\",\n        \"difficulty\": 0.28\n    }\n]\n\nprint(\"Sample output data structure:\")\nprint(json.dumps(sample_output_data, indent=2))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "@dataclass\nclass DKWController:\n    \"\"\"DKW-guided fusion/fission controller.\"\"\"\n    epsilon_target: float = 0.10    # Target error rate (10%)\n    delta: float = 0.05             # Confidence parameter (95% confidence)\n    min_samples: int = 100           # Minimum samples before decision making\n    hysteresis: float = 0.05         # Hysteresis to prevent oscillation\n\n    samples: list = field(default_factory=list)\n    current_state: str = \"fission\"   # Start conservatively\n\n    def dkw_epsilon(self, n: int) -> float:\n        \"\"\"Compute DKW epsilon for n samples.\n        \n        This uses the Dvoretzky-Kiefer-Wolfowitz inequality to bound\n        the difference between empirical and true error rates.\n        \"\"\"\n        if n < 2:\n            return 1.0  # Very conservative bound for small samples\n        return np.sqrt(np.log(2 / self.delta) / (2 * n))\n\n    def add_observation(self, error: float) -> None:\n        \"\"\"Add error observation for calibration.\"\"\"\n        self.samples.append(error)\n\n    def decide(self) -> str:\n        \"\"\"Make fusion/fission decision with DKW guarantee.\n        \n        Returns:\n            'fusion' for aggressive mode, 'fission' for conservative mode\n        \"\"\"\n        n = len(self.samples)\n        if n < self.min_samples:\n            return self.current_state\n\n        # Compute statistical bound\n        epsilon = self.dkw_epsilon(n)\n        empirical_error = np.mean(self.samples[-self.min_samples:])\n        error_upper_bound = empirical_error + epsilon\n\n        # State transition logic with hysteresis\n        if self.current_state == \"fusion\":\n            # Switch to fission if error bound exceeds target + hysteresis\n            if error_upper_bound > self.epsilon_target + self.hysteresis:\n                self.current_state = \"fission\"\n        else:  # current_state == \"fission\"\n            # Switch to fusion if error bound is below target - hysteresis\n            if error_upper_bound < self.epsilon_target - self.hysteresis:\n                self.current_state = \"fusion\"\n\n        return self.current_state\n\n# Create a controller instance to demonstrate\ncontroller = DKWController()\nprint(f\"Initial state: {controller.current_state}\")\nprint(f\"Target error rate: {controller.epsilon_target:.1%}\")\nprint(f\"Confidence level: {100*(1-controller.delta):.0f}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Sample Output Data\n\nFor reference, here's what the original script would have written to `data_out.json`. This sample data is now inlined to make the notebook self-contained:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Collect the data\ndata = collect_data()\nprint(f\"Collected {len(data)} examples\")\n\n# Display the first few examples\nprint(\"\\nFirst 3 examples:\")\nfor i in range(min(3, len(data))):\n    print(f\"\\nExample {i+1}:\")\n    print(f\"  ID: {data[i]['id']}\")\n    print(f\"  Question: {data[i]['question'][:100]}...\")  # Truncate for display\n    print(f\"  Answer: {data[i]['answer']}\")\n    print(f\"  Difficulty: {data[i]['difficulty']:.2f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Collect and Process Data\n\nNow let's run the data collection function and see how many examples we get:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## DKW Controller Implementation\n\nThe `DKWController` class implements an adaptive controller that switches between fusion (aggressive) and fission (conservative) modes based on empirical error observations and statistical confidence bounds.\n\n### Key Parameters:\n- **`epsilon_target`**: Target error rate (10% by default)\n- **`delta`**: Confidence level parameter (5% means 95% confidence)\n- **`min_samples`**: Minimum samples needed before making decisions\n- **`hysteresis`**: Buffer to prevent rapid oscillations between states",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def collect_data():\n    \"\"\"Collect benchmark data for DKW controller evaluation.\"\"\"\n    # Load HuggingFace dataset\n    ds = load_dataset(\"gsm8k\", \"main\", split=\"test[:200]\")\n\n    data = []\n    for i, example in enumerate(ds):\n        data.append({\n            \"id\": f\"example_{i:03d}\",\n            \"question\": example[\"question\"],\n            \"answer\": example[\"answer\"],\n            \"difficulty\": len(example[\"question\"]) / 100,  # Simple proxy\n        })\n\n    return data",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Import required libraries\nimport json\nimport numpy as np\nfrom dataclasses import dataclass, field\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\nprint(\"âœ… All libraries imported successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Data Collection Function\n\nThe `collect_data()` function loads the GSM8K dataset from HuggingFace and processes it into a standardized format. Each example includes:\n- **id**: Unique identifier for the example\n- **question**: The math problem text\n- **answer**: The correct answer\n- **difficulty**: A simple proxy based on question length",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# DKW Controller Implementation (Experiment 001)\n\nThis notebook demonstrates a **DKW-guided fusion/fission controller** that makes adaptive decisions based on statistical confidence bounds. The controller uses the Dvoretzky-Kiefer-Wolfowitz (DKW) inequality to provide probabilistic guarantees on error rates.\n\n## Key Concepts:\n- **Fusion**: Aggressive strategy (faster but potentially more errors)\n- **Fission**: Conservative strategy (slower but safer) \n- **DKW Bound**: Statistical method to bound empirical error with high confidence\n- **Hysteresis**: Prevents oscillation between states\n\n## Notebook Structure:\n1. **Imports & Setup**: Required libraries\n2. **DKW Controller Class**: Core implementation with adaptive decision making\n3. **Sample Data**: Inline dataset for experiments\n4. **Experiment Function**: Comparison between baseline and proposed methods  \n5. **Results & Analysis**: Visualization and interpretation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\"\"\"Dataset collection script for DKW benchmark.\"\"\"\nimport json\nfrom datasets import load_dataset",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Setup and Imports\n\nFirst, let's import the required libraries. This notebook uses the HuggingFace `datasets` library to load the GSM8K dataset.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# Dataset Collection for DKW Benchmark\n\nThis notebook demonstrates the `data.py` script converted into an interactive format. It collects benchmark data for DKW controller evaluation using the GSM8K dataset.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Experiment with different parameters\ndef experiment_with_parameters(epsilon_target=0.10, delta=0.05, min_samples=100, hysteresis=0.05):\n    \"\"\"Run experiment with custom controller parameters.\"\"\"\n    np.random.seed(42)  # For reproducibility\n    \n    # Create custom controller\n    custom_controller = DKWController(\n        epsilon_target=epsilon_target,\n        delta=delta,\n        min_samples=min_samples,\n        hysteresis=hysteresis\n    )\n    \n    results = {\"baseline\": [], \"proposed\": []}\n    \n    for example in sample_data:\n        error = np.random.random() < example[\"difficulty\"]\n        custom_controller.add_observation(float(error))\n        decision = custom_controller.decide()\n        \n        results[\"proposed\"].append({\n            \"id\": example[\"id\"],\n            \"decision\": decision,\n            \"error\": error,\n        })\n        results[\"baseline\"].append({\n            \"id\": example[\"id\"],\n            \"decision\": \"fission\",\n            \"error\": error,\n        })\n    \n    # Show decision counts\n    proposed_decisions = [r[\"decision\"] for r in results[\"proposed\"]]\n    fusion_count = proposed_decisions.count(\"fusion\")\n    fission_count = proposed_decisions.count(\"fission\")\n    \n    print(f\"Controller Parameters:\")\n    print(f\"  epsilon_target: {epsilon_target}\")\n    print(f\"  delta: {delta}\")\n    print(f\"  min_samples: {min_samples}\")\n    print(f\"  hysteresis: {hysteresis}\")\n    print(f\"\\nDecisions: {fusion_count} fusion, {fission_count} fission\")\n    print(f\"Decision sequence: {' -> '.join(proposed_decisions)}\")\n    \n    return results\n\n# Try default parameters\nprint(\"=== DEFAULT PARAMETERS ===\")\ndefault_results = experiment_with_parameters()\n\nprint(\"\\n=== MORE AGGRESSIVE (lower target error) ===\")\naggressive_results = experiment_with_parameters(epsilon_target=0.05)\n\nprint(\"\\n=== MORE CONSERVATIVE (higher target error) ===\")\nconservative_results = experiment_with_parameters(epsilon_target=0.15)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Interactive Parameter Tuning\n\nTry experimenting with different controller parameters to see how they affect behavior:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create a more detailed analysis with visualization\ndef analyze_controller_behavior(data, results):\n    \"\"\"Analyze and visualize controller behavior.\"\"\"\n    \n    # Extract decision sequences\n    proposed_decisions = [r[\"decision\"] for r in results[\"proposed\"]]\n    errors = [r[\"error\"] for r in results[\"proposed\"]]\n    difficulties = [r[\"difficulty\"] for r in results[\"proposed\"]]\n    \n    # Plot 1: Decision timeline\n    plt.figure(figsize=(12, 8))\n    \n    plt.subplot(2, 2, 1)\n    decision_numeric = [1 if d == \"fusion\" else 0 for d in proposed_decisions]\n    plt.plot(decision_numeric, 'bo-', label='DKW Controller')\n    plt.axhline(y=0, color='r', linestyle='--', alpha=0.5, label='Baseline (Always Fission)')\n    plt.ylabel('Decision (0=Fission, 1=Fusion)')\n    plt.xlabel('Example Index')\n    plt.title('Controller Decision Timeline')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    # Plot 2: Error rate vs difficulty\n    plt.subplot(2, 2, 2)\n    plt.scatter(difficulties, errors, c=['red' if e else 'green' for e in errors], alpha=0.7)\n    plt.xlabel('Difficulty Level')\n    plt.ylabel('Error Occurred')\n    plt.title('Errors vs Difficulty')\n    plt.grid(True, alpha=0.3)\n    \n    # Plot 3: Decisions vs difficulty  \n    plt.subplot(2, 2, 3)\n    colors = ['blue' if d == 'fusion' else 'orange' for d in proposed_decisions]\n    plt.scatter(difficulties, decision_numeric, c=colors, alpha=0.7)\n    plt.xlabel('Difficulty Level')\n    plt.ylabel('Decision (0=Fission, 1=Fusion)')\n    plt.title('Decisions vs Difficulty')\n    plt.grid(True, alpha=0.3)\n    \n    # Plot 4: Running error rate\n    plt.subplot(2, 2, 4)\n    running_errors = []\n    error_sum = 0\n    for i, error in enumerate(errors):\n        error_sum += error\n        running_errors.append(error_sum / (i + 1))\n    \n    plt.plot(running_errors, 'g-', linewidth=2, label='Running Error Rate')\n    plt.axhline(y=0.10, color='r', linestyle='--', label='Target Error Rate (0.10)')\n    plt.ylabel('Error Rate')\n    plt.xlabel('Example Index')\n    plt.title('Running Error Rate')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Print controller state information\n    print(f\"Final running error rate: {running_errors[-1]:.3f}\")\n    print(f\"Target error rate: 0.10\")\n    print(f\"Controller final state: {proposed_decisions[-1]}\")\n\n# Run the analysis\nanalyze_controller_behavior(sample_data, results)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Analysis and Visualization\n\nLet's analyze the controller's behavior and visualize how it adapts based on the error observations.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Run the experiment\nresults = run_experiment(sample_data)\n\n# Display results in a readable format\nprint(\"BASELINE RESULTS (Always Fission):\")\nprint(\"-\" * 40)\nfor result in results[\"baseline\"]:\n    print(f\"ID: {result['id']:12} | Decision: {result['decision']:8} | Error: {str(result['error']):5} | Difficulty: {result['difficulty']:.2f}\")\n\nprint(\"\\nPROPOSED DKW CONTROLLER RESULTS:\")\nprint(\"-\" * 40)  \nfor result in results[\"proposed\"]:\n    print(f\"ID: {result['id']:12} | Decision: {result['decision']:8} | Error: {str(result['error']):5} | Difficulty: {result['difficulty']:.2f}\")\n\n# Calculate some basic statistics\nbaseline_fissions = sum(1 for r in results[\"baseline\"] if r[\"decision\"] == \"fission\")\nproposed_fissions = sum(1 for r in results[\"proposed\"] if r[\"decision\"] == \"fission\")\nproposed_fusions = sum(1 for r in results[\"proposed\"] if r[\"decision\"] == \"fusion\")\n\nprint(f\"\\nSUMMARY:\")\nprint(f\"Baseline: {baseline_fissions} fission decisions, 0 fusion decisions\")\nprint(f\"Proposed: {proposed_fissions} fission decisions, {proposed_fusions} fusion decisions\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Running the Experiment",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def run_experiment(data, seed=42):\n    \"\"\"Run DKW controller experiment with inline data.\"\"\"\n    # Set seed for reproducible results\n    np.random.seed(seed)\n    \n    controller = DKWController()\n    results = {\"baseline\": [], \"proposed\": []}\n\n    for example in data:\n        # Simulate error occurrence based on difficulty\n        error = np.random.random() < example[\"difficulty\"]\n        controller.add_observation(float(error))\n        decision = controller.decide()\n\n        results[\"proposed\"].append({\n            \"id\": example[\"id\"],\n            \"decision\": decision,\n            \"error\": error,\n            \"difficulty\": example[\"difficulty\"]\n        })\n        results[\"baseline\"].append({\n            \"id\": example[\"id\"],\n            \"decision\": \"fission\",  # Always conservative\n            \"error\": error,\n            \"difficulty\": example[\"difficulty\"]\n        })\n\n    return results",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Inline sample data (replaces external file dependency)\nsample_data = [\n    {\"id\": \"example_000\", \"difficulty\": 0.1},  # Low difficulty\n    {\"id\": \"example_001\", \"difficulty\": 0.05}, # Very low difficulty  \n    {\"id\": \"example_002\", \"difficulty\": 0.8},  # High difficulty\n    {\"id\": \"example_003\", \"difficulty\": 0.2},  # Medium-low difficulty\n    {\"id\": \"example_004\", \"difficulty\": 0.15}, # Low-medium difficulty\n    {\"id\": \"example_005\", \"difficulty\": 0.9},  # Very high difficulty\n    {\"id\": \"example_006\", \"difficulty\": 0.3},  # Medium difficulty\n    {\"id\": \"example_007\", \"difficulty\": 0.05}, # Very low difficulty\n    {\"id\": \"example_008\", \"difficulty\": 0.6},  # Medium-high difficulty\n    {\"id\": \"example_009\", \"difficulty\": 0.1},  # Low difficulty\n]\n\nprint(f\"Created sample dataset with {len(sample_data)} examples\")\nprint(\"Difficulty levels:\", [x[\"difficulty\"] for x in sample_data])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Experiment Setup\n\nWe'll simulate an experiment comparing the DKW controller's decisions against a baseline (always conservative \"fission\") approach. The simulation uses sample data with varying difficulty levels to generate error probabilities.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "@dataclass\nclass DKWController:\n    \"\"\"DKW-guided fusion/fission controller.\"\"\"\n    epsilon_target: float = 0.10\n    delta: float = 0.05\n    min_samples: int = 100\n    hysteresis: float = 0.05\n\n    samples: list = field(default_factory=list)\n    current_state: str = \"fission\"\n\n    def dkw_epsilon(self, n: int) -> float:\n        \"\"\"Compute DKW epsilon for n samples.\"\"\"\n        if n < 2:\n            return 1.0\n        return np.sqrt(np.log(2 / self.delta) / (2 * n))\n\n    def add_observation(self, error: float) -> None:\n        \"\"\"Add error observation for calibration.\"\"\"\n        self.samples.append(error)\n\n    def decide(self) -> str:\n        \"\"\"Make fusion/fission decision with DKW guarantee.\"\"\"\n        n = len(self.samples)\n        if n < self.min_samples:\n            return self.current_state\n\n        epsilon = self.dkw_epsilon(n)\n        empirical_error = np.mean(self.samples[-self.min_samples:])\n        error_upper_bound = empirical_error + epsilon\n\n        if self.current_state == \"fusion\":\n            if error_upper_bound > self.epsilon_target + self.hysteresis:\n                self.current_state = \"fission\"\n        else:\n            if error_upper_bound < self.epsilon_target - self.hysteresis:\n                self.current_state = \"fusion\"\n\n        return self.current_state",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## DKW Controller Class\n\nThe DKWController uses the Dvoretzky-Kiefer-Wolfowitz inequality to provide statistical guarantees when making fusion/fission decisions. Key parameters:\n\n- `epsilon_target`: Target error rate (default: 0.10)\n- `delta`: Confidence parameter for DKW bound (default: 0.05)  \n- `min_samples`: Minimum samples before making decisions (default: 100)\n- `hysteresis`: Prevents oscillation between states (default: 0.05)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import json\nimport numpy as np\nfrom dataclasses import dataclass, field\nimport matplotlib.pyplot as plt",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# DKW Controller Implementation\n## experiment_001: method.py Demo\n\nThis notebook demonstrates a DKW-guided fusion/fission controller implementation. The DKW (Dvoretzky-Kiefer-Wolfowitz) inequality provides statistical guarantees for decision making under uncertainty.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Conclusion and Next Steps\n\nThis notebook demonstrates how the DKW controller adapts its decisions based on observed error rates while providing statistical guarantees. Key observations:\n\n1. **Adaptive Behavior**: The controller switches between fusion and fission based on error observations\n2. **Statistical Guarantees**: Uses DKW inequality to bound estimation error\n3. **Hysteresis**: Prevents oscillation between states\n\n### Experiment with Parameters\n\nYou can modify the controller parameters to see different behaviors:\n\n```python\n# Create a new controller with different parameters\ncustom_controller = DKWController(\n    epsilon_target=0.05,    # Tighter error tolerance\n    delta=0.01,            # Higher confidence\n    min_samples=50,        # Faster adaptation\n    hysteresis=0.02        # Less hysteresis\n)\n```\n\n### Modify the Data\n\nChange the `sample_data` generation to test different scenarios:\n- Different difficulty patterns\n- More or fewer examples\n- Varying error rates over time\n\n### Save Results\n\nUncomment the save block in the previous cell to save results to a JSON file.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Display sample results (first 5 examples)\nsample_results = {\n    \"baseline\": results[\"baseline\"][:5],\n    \"proposed\": results[\"proposed\"][:5]\n}\n\nprint(\"Sample Results (first 5 examples):\")\nprint(json.dumps(sample_results, indent=2))\n\n# Optionally save full results to file\n# with open(\"method_out.json\", \"w\") as f:\n#     json.dump(results, f, indent=2)\n# print(f\"\\nFull results saved to method_out.json\")\n\nprint(f\"\\nTotal examples in full results: {len(results['baseline'])}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Sample Output\n\nHere's a sample of the results in the same format as the original `method_out.json` file:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create visualization of decision patterns over time\nfig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 10))\n\n# Extract decision sequences\nproposed_decisions = [1 if r['decision'] == 'fusion' else 0 for r in results['proposed']]\nerrors = [1 if r['error'] else 0 for r in results['proposed']]\nexample_ids = list(range(len(results['proposed'])))\n\n# Plot 1: Decision patterns over time\nax1.plot(example_ids, proposed_decisions, 'b-', alpha=0.7, linewidth=2, label='DKW Controller (1=Fusion, 0=Fission)')\nax1.fill_between(example_ids, proposed_decisions, alpha=0.3, color='blue')\nax1.set_ylabel('Decision')\nax1.set_title('DKW Controller Decisions Over Time')\nax1.set_ylim(-0.1, 1.1)\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Plot 2: Error occurrences\nax2.scatter(example_ids, errors, alpha=0.6, c='red', s=20)\nax2.set_ylabel('Error Occurred')\nax2.set_title('Error Occurrences Over Time')\nax2.set_ylim(-0.1, 1.1)\nax2.grid(True, alpha=0.3)\n\n# Plot 3: Running error rate\nwindow_size = 20\nrunning_error_rate = []\nfor i in range(len(errors)):\n    start_idx = max(0, i - window_size + 1)\n    window_errors = errors[start_idx:i+1]\n    running_error_rate.append(np.mean(window_errors))\n\nax3.plot(example_ids, running_error_rate, 'g-', linewidth=2, label='Running Error Rate (20-sample window)')\nax3.axhline(y=0.10, color='red', linestyle='--', alpha=0.7, label='Target Error Rate (0.10)')\nax3.set_xlabel('Example Index')\nax3.set_ylabel('Error Rate')\nax3.set_title('Running Error Rate vs Target')\nax3.legend()\nax3.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Experiment: What if we had a different proposed method?\n# Modify these parameters to see different scenarios\n\ndef create_experimental_results(n_examples=200, fusion_rate=0.8, error_rate=0.05):\n    \"\"\"Create experimental results with custom parameters.\"\"\"\n    results = []\n    n_fusion = int(n_examples * fusion_rate)\n    n_errors = int(n_examples * error_rate)\n    \n    for i in range(n_examples):\n        decision = \"fusion\" if i < n_fusion else \"fission\"\n        error = i < n_errors\n        results.append({\"decision\": decision, \"error\": error})\n    \n    return results\n\n# Try different scenarios\nexperimental_scenarios = {\n    \"baseline\": baseline_results,  # Keep baseline the same\n    \"high_fusion_low_error\": create_experimental_results(fusion_rate=0.9, error_rate=0.03),\n    \"medium_fusion\": create_experimental_results(fusion_rate=0.5, error_rate=0.06),\n    \"original_proposed\": proposed_results\n}\n\nprint(\"Comparing different scenarios:\\n\")\nfor scenario_name, scenario_results in experimental_scenarios.items():\n    if scenario_name == \"baseline\":\n        continue\n    \n    scenario_data = {\"baseline\": baseline_results, \"proposed\": scenario_results}\n    scenario_metrics = compute_metrics(scenario_data)\n    \n    print(f\"{scenario_name.upper()}:\")\n    print(f\"  API reduction: {scenario_metrics['improvement']['api_reduction_pct']:.1f}%\")\n    print(f\"  Error rate change: {scenario_metrics['improvement']['error_rate_diff']:+.1%}\")\n    print(f\"  Fusion rate: {scenario_metrics['proposed']['fusion_rate']:.1%}\")\n    print()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Results Analysis\n\nLet's analyze the behavior of both methods and visualize how the DKW controller adapts over time.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Experiment with Different Scenarios\n\nYou can modify the parameters below to see how different controller behaviors would affect the metrics:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Run the experiment\nresults = run_experiment(sample_data)\n\n# Display basic statistics\nprint(\"Experiment Results:\")\nprint(f\"Total examples processed: {len(results['baseline'])}\")\n\n# Count decisions for each method\nbaseline_fission = sum(1 for r in results['baseline'] if r['decision'] == 'fission')\nbaseline_fusion = sum(1 for r in results['baseline'] if r['decision'] == 'fusion')\n\nproposed_fission = sum(1 for r in results['proposed'] if r['decision'] == 'fission')\nproposed_fusion = sum(1 for r in results['proposed'] if r['decision'] == 'fusion')\n\nprint(f\"\\nBaseline method:\")\nprint(f\"  Fission decisions: {baseline_fission}\")\nprint(f\"  Fusion decisions: {baseline_fusion}\")\n\nprint(f\"\\nProposed DKW method:\")\nprint(f\"  Fission decisions: {proposed_fission}\")\nprint(f\"  Fusion decisions: {proposed_fusion}\")\n\n# Count errors for each method\nbaseline_errors = sum(1 for r in results['baseline'] if r['error'])\nproposed_errors = sum(1 for r in results['proposed'] if r['error'])\n\nprint(f\"\\nError counts:\")\nprint(f\"  Baseline: {baseline_errors} errors\")\nprint(f\"  Proposed: {proposed_errors} errors\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Compute metrics\nmetrics = compute_metrics(results)\n\n# Display results in a formatted way\nprint(\"=\"*50)\nprint(\"DKW CONTROLLER EVALUATION RESULTS\")\nprint(\"=\"*50)\n\nfor method in [\"baseline\", \"proposed\"]:\n    print(f\"\\n{method.upper()} METHOD:\")\n    m = metrics[method]\n    print(f\"  Fusion rate:     {m['fusion_rate']:.1%}\")\n    print(f\"  Fission rate:    {m['fission_rate']:.1%}\") \n    print(f\"  Error rate:      {m['error_rate']:.1%}\")\n    print(f\"  Total API calls: {m['api_calls']}\")\n    print(f\"  Avg calls/example: {m['avg_calls_per_example']:.2f}\")\n\nprint(f\"\\nIMPROVEMENT:\")\nprint(f\"  API reduction: {metrics['improvement']['api_reduction_pct']:.1f}%\")\nprint(f\"  Error rate change: {metrics['improvement']['error_rate_diff']:+.1%}\")\n\n# Also save as JSON (inline output)\nprint(f\"\\nFull metrics as JSON:\")\nprint(json.dumps(metrics, indent=2))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Run Experiment\n\nLet's execute the experiment and collect results from both the proposed DKW controller and the baseline always-fission approach.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def run_experiment(data):\n    \"\"\"Run DKW controller experiment.\"\"\"\n    controller = DKWController()\n    results = {\"baseline\": [], \"proposed\": []}\n\n    for example in data:\n        # Simulate error occurrence based on difficulty\n        error = np.random.random() < example[\"difficulty\"]\n        controller.add_observation(float(error))\n        decision = controller.decide()\n\n        results[\"proposed\"].append({\n            \"id\": example[\"id\"],\n            \"decision\": decision,\n            \"error\": error,\n        })\n        results[\"baseline\"].append({\n            \"id\": example[\"id\"],\n            \"decision\": \"fission\",  # Always conservative\n            \"error\": error,\n        })\n\n    return results",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Run Evaluation\n\nNow let's compute the metrics and display the results:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def compute_metrics(results: dict) -> dict:\n    \"\"\"Compute evaluation metrics.\"\"\"\n    metrics = {}\n\n    for method in [\"baseline\", \"proposed\"]:\n        preds = results[method]\n\n        # Count decisions\n        fusion_count = sum(1 for p in preds if p[\"decision\"] == \"fusion\")\n        fission_count = sum(1 for p in preds if p[\"decision\"] == \"fission\")\n\n        # Compute error rate\n        errors = sum(1 for p in preds if p[\"error\"])\n        error_rate = errors / len(preds)\n\n        # API calls (fusion=1, fission=2)\n        api_calls = fusion_count + 2 * fission_count\n\n        metrics[method] = {\n            \"fusion_rate\": fusion_count / len(preds),\n            \"fission_rate\": fission_count / len(preds),\n            \"error_rate\": error_rate,\n            \"api_calls\": api_calls,\n            \"avg_calls_per_example\": api_calls / len(preds),\n        }\n\n    # Compute improvement\n    baseline_calls = metrics[\"baseline\"][\"avg_calls_per_example\"]\n    proposed_calls = metrics[\"proposed\"][\"avg_calls_per_example\"]\n    metrics[\"improvement\"] = {\n        \"api_reduction_pct\": (baseline_calls - proposed_calls) / baseline_calls * 100,\n        \"error_rate_diff\": metrics[\"proposed\"][\"error_rate\"] - metrics[\"baseline\"][\"error_rate\"],\n    }\n\n    return metrics",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Experiment Function\n\nThe `run_experiment` function simulates the controller's behavior over a sequence of examples, comparing:\n- **Proposed method**: Uses DKW controller for adaptive decisions\n- **Baseline method**: Always uses conservative \"fission\" mode\n\nFor each example, we simulate whether an error occurs based on the difficulty level.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Sample data - normally read from \"../dataset_001/data_out.json\"\n# Create synthetic data with varying difficulty levels\nsample_data = []\n\n# Generate 300 examples with varying difficulty\nfor i in range(300):\n    # Create examples with different difficulty patterns\n    if i < 100:\n        difficulty = 0.05  # Easy examples (low error rate)\n    elif i < 200:\n        difficulty = 0.15  # Medium examples (moderate error rate)\n    else:\n        difficulty = 0.08  # Harder examples (but still manageable)\n    \n    sample_data.append({\n        \"id\": f\"example_{i:03d}\",\n        \"difficulty\": difficulty\n    })\n\nprint(f\"Created {len(sample_data)} sample examples\")\nprint(f\"Sample data preview: {sample_data[:3]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Evaluation Metrics Function\n\nThe `compute_metrics` function calculates several key performance indicators:\n\n- **Fusion/Fission rates**: Proportion of decisions for each strategy\n- **Error rate**: Percentage of incorrect decisions\n- **API calls**: Total API usage (fusion = 1 call, fission = 2 calls)\n- **Efficiency improvements**: Comparison between methods",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Sample Data\n\nWe'll create sample data to simulate the input that would normally be read from a JSON file. Each example has:\n- `id`: Unique identifier\n- `difficulty`: Probability of error occurring (0.0 to 1.0)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create sample experimental results\n# This data represents the decisions made by baseline vs proposed methods\n\n# Baseline method: always chooses fission, 8% error rate\nbaseline_results = []\nfor i in range(200):\n    baseline_results.append({\n        \"decision\": \"fission\",\n        \"error\": i < 16  # First 16 examples have errors (8% of 200)\n    })\n\n# Proposed method: 65% fusion, 35% fission, 9% error rate  \nproposed_results = []\nfor i in range(200):\n    if i < 130:  # First 130 examples use fusion (65% of 200)\n        decision = \"fusion\"\n    else:  # Remaining 70 examples use fission (35% of 200)\n        decision = \"fission\"\n    \n    proposed_results.append({\n        \"decision\": decision,\n        \"error\": i < 18  # First 18 examples have errors (9% of 200)\n    })\n\n# Combine into results structure\nresults = {\n    \"baseline\": baseline_results,\n    \"proposed\": proposed_results\n}\n\nprint(f\"Generated {len(baseline_results)} baseline results and {len(proposed_results)} proposed results\")\nprint(f\"Baseline decisions: {sum(1 for r in baseline_results if r['decision'] == 'fusion')} fusion, {sum(1 for r in baseline_results if r['decision'] == 'fission')} fission\")\nprint(f\"Proposed decisions: {sum(1 for r in proposed_results if r['decision'] == 'fusion')} fusion, {sum(1 for r in proposed_results if r['decision'] == 'fission')} fission\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "@dataclass\nclass DKWController:\n    \"\"\"DKW-guided fusion/fission controller.\"\"\"\n    epsilon_target: float = 0.10\n    delta: float = 0.05\n    min_samples: int = 100\n    hysteresis: float = 0.05\n\n    samples: list = field(default_factory=list)\n    current_state: str = \"fission\"\n\n    def dkw_epsilon(self, n: int) -> float:\n        \"\"\"Compute DKW epsilon for n samples.\"\"\"\n        if n < 2:\n            return 1.0\n        return np.sqrt(np.log(2 / self.delta) / (2 * n))\n\n    def add_observation(self, error: float) -> None:\n        \"\"\"Add error observation for calibration.\"\"\"\n        self.samples.append(error)\n\n    def decide(self) -> str:\n        \"\"\"Make fusion/fission decision with DKW guarantee.\"\"\"\n        n = len(self.samples)\n        if n < self.min_samples:\n            return self.current_state\n\n        epsilon = self.dkw_epsilon(n)\n        empirical_error = np.mean(self.samples[-self.min_samples:])\n        error_upper_bound = empirical_error + epsilon\n\n        if self.current_state == \"fusion\":\n            if error_upper_bound > self.epsilon_target + self.hysteresis:\n                self.current_state = \"fission\"\n        else:\n            if error_upper_bound < self.epsilon_target - self.hysteresis:\n                self.current_state = \"fusion\"\n\n        return self.current_state",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Sample Data\n\nInstead of reading from external JSON files, we'll create sample data inline that represents the experimental results from both baseline and proposed methods.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## DKW Controller Class\n\nThe core controller that implements the DKW-guided decision making algorithm.\n\n**Parameters:**\n- `epsilon_target`: Target error threshold (0.10)\n- `delta`: Confidence parameter for DKW bound (0.05)\n- `min_samples`: Minimum samples before making decisions (100)\n- `hysteresis`: Buffer to prevent oscillation (0.05)\n\n**Key Methods:**\n- `dkw_epsilon()`: Computes the DKW confidence interval width\n- `add_observation()`: Records error observations\n- `decide()`: Makes fusion/fission decision with statistical guarantees",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import json\nimport numpy as np",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# DKW Controller Evaluation\n\nThis notebook evaluates the performance of a DKW Controller, comparing baseline and proposed methods. The evaluation focuses on:\n- **Fusion vs Fission decisions**: The controller can choose to fuse or split operations\n- **API efficiency**: Fusion requires 1 API call, fission requires 2 API calls\n- **Error rates**: How often the controller makes incorrect decisions\n\nThe goal is to measure the improvement in API efficiency while maintaining acceptable error rates.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\"\"\"DKW Controller Implementation.\"\"\"\nimport json\nimport numpy as np\nfrom dataclasses import dataclass, field\nimport matplotlib.pyplot as plt\n\n# Set random seed for reproducibility\nnp.random.seed(42)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# DKW Controller Implementation Demo\n\nThis notebook demonstrates a **DKW-guided fusion/fission controller** that makes adaptive decisions based on error observations with statistical guarantees.\n\n## Overview\nThe DKW (Dvoretzky-Kiefer-Wolfowitz) inequality provides a way to bound the difference between empirical and true error rates, enabling principled decision-making between fusion and fission states.\n\n**Key Features:**\n- Statistical guarantees via DKW inequality\n- Adaptive switching between fusion/fission modes\n- Hysteresis to prevent oscillation\n- Real-time error calibration",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Collection for DKW Benchmark\n\n",
    "This notebook demonstrates dataset collection for DKW controller evaluation using the GSM8K dataset from HuggingFace. The script processes mathematical word problems and creates structured benchmark data with difficulty estimates.\n\n",
    "**Artifact ID:** dataset_001  \n",
    "**Original file:** data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n\n",
    "We'll need the `datasets` library to load data from HuggingFace and `json` for data serialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Dataset collection script for DKW benchmark.\"\"\"\n",
    "import json\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection Function\n\n",
    "The `collect_data()` function loads the GSM8K dataset from HuggingFace and processes it into a structured format suitable for benchmarking. Each example includes:\n",
    "- **id**: Unique identifier for the example\n",
    "- **question**: The mathematical word problem\n",
    "- **answer**: The correct answer\n",
    "- **difficulty**: A simple difficulty estimate based on question length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data():\n",
    "    \"\"\"Collect benchmark data for DKW controller evaluation.\"\"\"\n",
    "    # Load HuggingFace dataset\n",
    "    ds = load_dataset(\"gsm8k\", \"main\", split=\"test[:200]\")\n\n",
    "    data = []\n",
    "    for i, example in enumerate(ds):\n",
    "        data.append({\n",
    "            \"id\": f\"example_{i:03d}\",\n",
    "            \"question\": example[\"question\"],\n",
    "            \"answer\": example[\"answer\"],\n",
    "            \"difficulty\": len(example[\"question\"]) / 100,  # Simple proxy\n",
    "        })\n\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Data Collection\n\n",
    "Run the data collection function and display the results. This will download 200 examples from the GSM8K test set and process them into our benchmark format.\n\n",
    "**Note:** This is completely self-contained - no external files are needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the data\n",
    "data = collect_data()\n\n",
    "# Display results\n",
    "print(f\"Collected {len(data)} examples\")\n",
    "print(f\"\\nFirst 3 examples:\")\n",
    "for i in range(min(3, len(data))):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  ID: {data[i]['id']}\")\n",
    "    print(f\"  Question: {data[i]['question'][:100]}...\")\n",
    "    print(f\"  Answer: {data[i]['answer']}\")\n",
    "    print(f\"  Difficulty: {data[i]['difficulty']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data to JSON File\n\n",
    "Optionally save the collected data to a JSON file for later use. This mimics the original script's behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to JSON file (optional)\n",
    "with open(\"data_out.json\", \"w\") as f:\n",
    "    json.dump(data, f, indent=2)\n\n",
    "print(\"Data saved to 'data_out.json'\")\n\n",
    "# Show the JSON structure\n",
    "print(f\"\\nJSON file contains {len(data)} examples\")\n",
    "print(\"Sample JSON structure:\")\n",
    "print(json.dumps(data[:2], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Output Format\n\n",
    "Here's an example of what the processed data looks like. This demonstrates the expected structure and format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of processed data structure (for reference)\n",
    "sample_data = [\n",
    "    {\n",
    "        \"id\": \"example_000\",\n",
    "        \"question\": \"What is 2+2?\",\n",
    "        \"answer\": \"4\",\n",
    "        \"difficulty\": 0.15\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"example_001\",\n",
    "        \"question\": \"If x=5, what is 2x?\",\n",
    "        \"answer\": \"10\",\n",
    "        \"difficulty\": 0.22\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"example_002\",\n",
    "        \"question\": \"Solve: 3y + 6 = 15\",\n",
    "        \"answer\": \"y=3\",\n",
    "        \"difficulty\": 0.28\n",
    "    }\n",
    "]\n\n",
    "print(\"Sample data structure:\")\n",
    "print(json.dumps(sample_data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage and Customization\n\n",
    "This notebook is completely self-contained and ready to run! Here are some ways you can customize it:\n\n",
    "### Modify Dataset Parameters:\n",
    "- Change the number of examples: `split=\"test[:200]\"` â†’ `split=\"test[:500]\"`\n",
    "- Use different splits: `split=\"test\"` or `split=\"train\"`\n",
    "- Use validation set: `split=\"validation\"`\n\n",
    "### Adjust Difficulty Calculation:\n",
    "The current difficulty is based on question length. You could modify it to use:\n",
    "- Number of mathematical operations\n",
    "- Presence of certain keywords\n",
    "- Complexity scoring algorithms\n\n",
    "### Data Processing:\n",
    "- Add additional fields (e.g., topic classification, solution steps)\n",
    "- Filter examples by certain criteria\n",
    "- Apply text preprocessing\n\n",
    "### Running the Notebook:\n",
    "1. Make sure you have the required packages: `pip install datasets`\n",
    "2. Run all cells in order\n",
    "3. The data will be collected from HuggingFace automatically\n",
    "4. No external files needed!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}