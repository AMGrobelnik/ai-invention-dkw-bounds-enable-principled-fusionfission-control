{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## 8. Summary\n\nThis notebook demonstrates a **DKW-guided fusion/fission controller** that:\n\n### Key Features:\n- **Statistical Guarantees**: Uses DKW inequality for confidence bounds on error rates\n- **Adaptive Behavior**: Switches between fusion/fission modes based on observed performance\n- **Hysteresis**: Prevents oscillation between modes\n- **Parameter Tuning**: Adjustable thresholds and confidence levels\n\n### Main Components:\n1. **DKWController Class**: Core implementation with statistical guarantees\n2. **Sample Data Generation**: Creates realistic test scenarios with varying difficulties\n3. **Experiment Framework**: Compares proposed method against baseline\n4. **Analysis & Visualization**: Interactive exploration of results\n5. **Parameter Testing**: Ability to modify controller behavior\n\n### Usage:\n- Run all cells to see the complete analysis\n- Modify parameters in Section 6 to explore different behaviors\n- The notebook is completely self-contained - no external files needed\n- All data and results are generated and analyzed within the notebook\n\n### Next Steps:\n- Try different parameter combinations in the interactive section\n- Modify the sample data generation to test edge cases\n- Implement additional baseline methods for comparison\n- Add more sophisticated visualization and analysis",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Optional: Save results to JSON file (uncomment to use)\n# with open(\"method_out.json\", \"w\") as f:\n#     json.dump(results, f, indent=2)\n# print(\"Results saved to method_out.json\")\n\n# Display sample results in the format that would be saved\nprint(\"Sample results (first 3 examples):\")\nprint(\"\\nBaseline:\")\nfor i in range(3):\n    print(f\"  {results['baseline'][i]}\")\n\nprint(\"\\nProposed:\")\nfor i in range(3):\n    print(f\"  {results['proposed'][i]}\")\n    \n# Show the example output that was provided in the original question\nprint(\"\\n\" + \"=\"*50)\nprint(\"Expected output format (from original method_out.json):\")\nexample_output = {\n    \"baseline\": [\n        {\"id\": \"example_000\", \"decision\": \"fission\", \"error\": False},\n        {\"id\": \"example_001\", \"decision\": \"fission\", \"error\": False},\n        {\"id\": \"example_002\", \"decision\": \"fission\", \"error\": True}\n    ],\n    \"proposed\": [\n        {\"id\": \"example_000\", \"decision\": \"fission\", \"error\": False},\n        {\"id\": \"example_001\", \"decision\": \"fusion\", \"error\": False},\n        {\"id\": \"example_002\", \"decision\": \"fusion\", \"error\": True}\n    ]\n}\nprint(json.dumps(example_output, indent=2))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Save Results (Optional)\n\nIf you want to save the results to JSON files (like the original script), run the cell below:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Interactive parameter exploration\ndef test_controller_params(epsilon_target=0.10, delta=0.05, min_samples=100, hysteresis=0.05):\n    \"\"\"Test controller with different parameters.\"\"\"\n    # Create a controller with custom parameters\n    custom_controller = DKWController(\n        epsilon_target=epsilon_target,\n        delta=delta,\n        min_samples=min_samples,\n        hysteresis=hysteresis\n    )\n    \n    # Run a quick experiment with subset of data\n    test_data = sample_data[:150]  # Use first 150 examples for faster testing\n    custom_results = {\"baseline\": [], \"proposed\": []}\n    \n    for example in test_data:\n        error = np.random.random() < example[\"difficulty\"]\n        custom_controller.add_observation(float(error))\n        decision = custom_controller.decide()\n        \n        custom_results[\"proposed\"].append({\n            \"id\": example[\"id\"],\n            \"decision\": decision,\n            \"error\": error,\n        })\n    \n    # Calculate metrics\n    proposed_df = pd.DataFrame(custom_results[\"proposed\"])\n    metrics = calculate_metrics(proposed_df)\n    \n    print(f\"Parameters: Îµ_target={epsilon_target}, Î´={delta}, min_samples={min_samples}, hysteresis={hysteresis}\")\n    print(f\"Results: Error rate={metrics['error_rate']:.3f}, Fusion rate={metrics['fusion_rate']:.3f}\")\n    print(f\"Fusion decisions: {metrics['fusion_decisions']} out of {metrics['total_examples']}\")\n    \n    return custom_results\n\n# Test default parameters\nprint(\"=== DEFAULT PARAMETERS ===\")\ndefault_results = test_controller_params()\n\nprint(\"\\n=== MODIFIED PARAMETERS ===\")\n# Try more aggressive parameters (lower target, less hysteresis)\naggressive_results = test_controller_params(epsilon_target=0.05, hysteresis=0.02)\n\n# Try more conservative parameters (higher target, more hysteresis)\nconservative_results = test_controller_params(epsilon_target=0.15, hysteresis=0.08)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Interactive Parameter Exploration\n\nTry modifying the controller parameters below to see how they affect the behavior:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Visualization of controller behavior\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n\n# 1. Decision patterns over time\ndecision_numeric = [1 if d == \"fusion\" else 0 for d in decisions_over_time]\nax1.plot(decision_numeric, alpha=0.7, linewidth=0.8)\nax1.set_title(\"Decision Pattern Over Time\\n(1=Fusion, 0=Fission)\")\nax1.set_xlabel(\"Example Number\")\nax1.set_ylabel(\"Decision\")\nax1.grid(True, alpha=0.3)\n\n# 2. Cumulative fusion rate\nfusion_rate_over_time = [fusion_count[i] / (i+1) for i in range(len(fusion_count))]\nax2.plot(fusion_rate_over_time, color='orange')\nax2.axhline(y=0.1, color='red', linestyle='--', alpha=0.7, label='Target Îµ=0.1')\nax2.set_title(\"Cumulative Fusion Rate\")\nax2.set_xlabel(\"Example Number\")\nax2.set_ylabel(\"Fusion Rate\")\nax2.legend()\nax2.grid(True, alpha=0.3)\n\n# 3. Error distribution by decision type\nbaseline_errors = baseline_df.groupby('decision')['error'].sum()\nproposed_errors = proposed_df.groupby('decision')['error'].sum()\n\ndecisions = ['fission', 'fusion']\nbaseline_vals = [baseline_errors.get(d, 0) for d in decisions]\nproposed_vals = [proposed_errors.get(d, 0) for d in decisions]\n\nx = np.arange(len(decisions))\nwidth = 0.35\nax3.bar(x - width/2, baseline_vals, width, label='Baseline', alpha=0.8)\nax3.bar(x + width/2, proposed_vals, width, label='Proposed', alpha=0.8)\nax3.set_title(\"Total Errors by Decision Type\")\nax3.set_xlabel(\"Decision\")\nax3.set_ylabel(\"Number of Errors\")\nax3.set_xticks(x)\nax3.set_xticklabels(decisions)\nax3.legend()\nax3.grid(True, alpha=0.3)\n\n# 4. Difficulty distribution\ndifficulties = [ex['difficulty'] for ex in sample_data]\nax4.hist(difficulties, bins=30, alpha=0.7, color='green')\nax4.set_title(\"Distribution of Example Difficulties\")\nax4.set_xlabel(\"Difficulty Level\")\nax4.set_ylabel(\"Frequency\")\nax4.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Expected output format (from original data_out.json)\nexpected_format = [\n    {\n        \"id\": \"example_000\",\n        \"question\": \"What is 2+2?\",\n        \"answer\": \"4\",\n        \"difficulty\": 0.15\n    },\n    {\n        \"id\": \"example_001\",\n        \"question\": \"If x=5, what is 2x?\",\n        \"answer\": \"10\",\n        \"difficulty\": 0.22\n    },\n    {\n        \"id\": \"example_002\",\n        \"question\": \"Solve: 3y + 6 = 15\",\n        \"answer\": \"y=3\",\n        \"difficulty\": 0.28\n    }\n]\n\nprint(\"Expected format structure:\")\nprint(json.dumps(expected_format, indent=2))\n\nprint(f\"\\nðŸ“‹ Note: Our processed data follows the same structure with ID, question, answer, and difficulty fields.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Convert results to DataFrames for easier analysis\nbaseline_df = pd.DataFrame(results[\"baseline\"])\nproposed_df = pd.DataFrame(results[\"proposed\"])\n\n# Calculate metrics\ndef calculate_metrics(df):\n    total_examples = len(df)\n    errors = df[\"error\"].sum()\n    fission_decisions = (df[\"decision\"] == \"fission\").sum()\n    fusion_decisions = (df[\"decision\"] == \"fusion\").sum()\n    \n    error_rate = errors / total_examples\n    fission_rate = fission_decisions / total_examples\n    fusion_rate = fusion_decisions / total_examples\n    \n    return {\n        \"total_examples\": total_examples,\n        \"errors\": errors,\n        \"error_rate\": error_rate,\n        \"fission_decisions\": fission_decisions,\n        \"fusion_decisions\": fusion_decisions,\n        \"fission_rate\": fission_rate,\n        \"fusion_rate\": fusion_rate\n    }\n\nbaseline_metrics = calculate_metrics(baseline_df)\nproposed_metrics = calculate_metrics(proposed_df)\n\nprint(\"=== METRICS COMPARISON ===\")\nprint(f\"Baseline  - Error rate: {baseline_metrics['error_rate']:.3f}, Fission rate: {baseline_metrics['fission_rate']:.3f}\")\nprint(f\"Proposed  - Error rate: {proposed_metrics['error_rate']:.3f}, Fission rate: {proposed_metrics['fission_rate']:.3f}\")\nprint(f\"Difference - Error rate: {proposed_metrics['error_rate'] - baseline_metrics['error_rate']:+.3f}, Fission rate: {proposed_metrics['fission_rate'] - baseline_metrics['fission_rate']:+.3f}\")\n\n# Show how decisions evolved over time for proposed method\ndecisions_over_time = [r[\"decision\"] for r in results[\"proposed\"]]\nfusion_count = [decisions_over_time[:i+1].count(\"fusion\") for i in range(len(decisions_over_time))]\nprint(f\"\\nFusion decisions over time (final: {fusion_count[-1]} out of {len(decisions_over_time)})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Expected Output Format Reference\n\nFor reference, here's the expected output format that matches the original `data_out.json`:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Results Analysis\n\nLet's analyze the results and visualize how the DKW controller performs compared to the baseline approach.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Demonstrate JSON serialization (in original script, this would save to file)\njson_output = json.dumps(data, indent=2)\nprint(\"JSON format output:\")\nprint(json_output)\n\n# In the original script, this would be:\n# with open(\"data_out.json\", \"w\") as f:\n#     json.dump(data, f, indent=2)\nprint(f\"\\nâœ“ Data collection complete! Processed {len(data)} examples.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def run_experiment(data):\n    \"\"\"Run DKW controller experiment with inline data.\"\"\"\n    controller = DKWController()\n    results = {\"baseline\": [], \"proposed\": []}\n\n    for example in data:\n        # Simulate error occurrence based on difficulty\n        error = np.random.random() < example[\"difficulty\"]\n        controller.add_observation(float(error))\n        decision = controller.decide()\n\n        results[\"proposed\"].append({\n            \"id\": example[\"id\"],\n            \"decision\": decision,\n            \"error\": error,\n        })\n        results[\"baseline\"].append({\n            \"id\": example[\"id\"],\n            \"decision\": \"fission\",  # Always conservative\n            \"error\": error,\n        })\n\n    return results\n\n# Run the experiment\nprint(\"Running experiment...\")\nresults = run_experiment(sample_data)\n\nprint(f\"Processed {len(results['baseline'])} examples\")\nprint(f\"Baseline decisions: {len([r for r in results['baseline'] if r['decision'] == 'fission'])} fission, {len([r for r in results['baseline'] if r['decision'] == 'fusion'])} fusion\")\nprint(f\"Proposed decisions: {len([r for r in results['proposed'] if r['decision'] == 'fission'])} fission, {len([r for r in results['proposed'] if r['decision'] == 'fusion'])} fusion\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Save Data (Optional)\n\nIn the original script, data would be saved to a JSON file. Here we'll demonstrate this functionality:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Display all processed data\nprint(\"Complete processed dataset:\")\nprint(\"=\" * 50)\nfor i, example in enumerate(data):\n    print(f\"\\nExample {i+1}:\")\n    print(f\"ID: {example['id']}\")\n    print(f\"Question: {example['question']}\")\n    print(f\"Answer: {example['answer']}\")\n    print(f\"Difficulty: {example['difficulty']:.2f}\")\n    print(\"-\" * 30)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 4. Experiment Function\n\nThe experiment compares two approaches:\n1. **Baseline**: Always uses \"fission\" mode (conservative approach)\n2. **Proposed**: Uses DKW controller to adaptively switch between fusion/fission\n\nFor each example, we simulate whether an error occurs based on the difficulty level, then record the controller's decision.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Display All Processed Data\n\nLet's view the complete processed dataset:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Execute the data collection\ndata = collect_data()\n\nprint(f\"Collected {len(data)} examples\")\nprint(\"\\nFirst example:\")\nprint(json.dumps(data[0], indent=2))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create sample data inline (replaces reading from JSON file)\n# Each example has an ID and a difficulty level (0.0 to 1.0)\n# Higher difficulty means higher probability of error\nsample_data = [\n    {\"id\": f\"example_{i:03d}\", \"difficulty\": np.random.beta(2, 8)}  # Most examples easy\n    for i in range(200)\n] + [\n    {\"id\": f\"example_{i+200:03d}\", \"difficulty\": np.random.beta(5, 3)}  # Some harder examples\n    for i in range(100)\n] + [\n    {\"id\": f\"example_{i+300:03d}\", \"difficulty\": np.random.beta(8, 2)}  # Few very hard examples\n    for i in range(50)\n]\n\nprint(f\"Created {len(sample_data)} examples\")\nprint(f\"Difficulty range: {min(ex['difficulty'] for ex in sample_data):.3f} - {max(ex['difficulty'] for ex in sample_data):.3f}\")\nprint(f\"Mean difficulty: {np.mean([ex['difficulty'] for ex in sample_data]):.3f}\")\n\n# Show first few examples\nprint(\"\\nFirst 5 examples:\")\nfor i, example in enumerate(sample_data[:5]):\n    print(f\"  {example['id']}: difficulty = {example['difficulty']:.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Execute Data Collection\n\nNow let's run the data collection function and see the processed results.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Sample Data\n\nInstead of reading from external JSON files, we'll create sample data inline. This data represents examples with varying difficulty levels that determine the probability of errors.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def collect_data(dataset=None):\n    \"\"\"Collect benchmark data for DKW controller evaluation.\"\"\"\n    # Use sample data if no dataset provided (for self-contained version)\n    if dataset is None:\n        dataset = sample_dataset\n    \n    data = []\n    for i, example in enumerate(dataset):\n        data.append({\n            \"id\": f\"example_{i:03d}\",\n            \"question\": example[\"question\"],\n            \"answer\": example[\"answer\"],\n            \"difficulty\": len(example[\"question\"]) / 100,  # Simple proxy based on question length\n        })\n\n    return data\n\nprint(\"Data processing function defined!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Data Processing Function\n\nThe `collect_data()` function processes the raw dataset and converts it to a standardized format for the DKW benchmark. It adds unique IDs and calculates a difficulty score based on question length.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "@dataclass\nclass DKWController:\n    \"\"\"DKW-guided fusion/fission controller.\"\"\"\n    epsilon_target: float = 0.10\n    delta: float = 0.05\n    min_samples: int = 100\n    hysteresis: float = 0.05\n\n    samples: list = field(default_factory=list)\n    current_state: str = \"fission\"\n\n    def dkw_epsilon(self, n: int) -> float:\n        \"\"\"Compute DKW epsilon for n samples.\"\"\"\n        if n < 2:\n            return 1.0\n        return np.sqrt(np.log(2 / self.delta) / (2 * n))\n\n    def add_observation(self, error: float) -> None:\n        \"\"\"Add error observation for calibration.\"\"\"\n        self.samples.append(error)\n\n    def decide(self) -> str:\n        \"\"\"Make fusion/fission decision with DKW guarantee.\"\"\"\n        n = len(self.samples)\n        if n < self.min_samples:\n            return self.current_state\n\n        epsilon = self.dkw_epsilon(n)\n        empirical_error = np.mean(self.samples[-self.min_samples:])\n        error_upper_bound = empirical_error + epsilon\n\n        if self.current_state == \"fusion\":\n            if error_upper_bound > self.epsilon_target + self.hysteresis:\n                self.current_state = \"fission\"\n        else:\n            if error_upper_bound < self.epsilon_target - self.hysteresis:\n                self.current_state = \"fusion\"\n\n        return self.current_state\n\n# Test the controller with a simple example\ncontroller = DKWController()\nprint(f\"Initial state: {controller.current_state}\")\nprint(f\"DKW epsilon for 100 samples: {controller.dkw_epsilon(100):.4f}\")\nprint(f\"DKW epsilon for 1000 samples: {controller.dkw_epsilon(1000):.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Sample data mimicking GSM8K dataset structure\n# In the real implementation, this would be: ds = load_dataset(\"gsm8k\", \"main\", split=\"test[:200]\")\n\nsample_dataset = [\n    {\n        \"question\": \"What is 2+2?\",\n        \"answer\": \"2 + 2 = 4\"\n    },\n    {\n        \"question\": \"If x=5, what is 2x?\",\n        \"answer\": \"If x = 5, then 2x = 2 * 5 = 10\"\n    },\n    {\n        \"question\": \"Solve: 3y + 6 = 15\",\n        \"answer\": \"3y + 6 = 15\\n3y = 15 - 6\\n3y = 9\\ny = 3\"\n    },\n    {\n        \"question\": \"A store has 48 apples. If they sell 3/4 of them, how many apples are left?\",\n        \"answer\": \"3/4 of 48 = (3/4) * 48 = 36\\nApples left = 48 - 36 = 12\"\n    },\n    {\n        \"question\": \"Calculate the area of a rectangle with length 8 cm and width 5 cm.\",\n        \"answer\": \"Area = length * width = 8 * 5 = 40 square cm\"\n    }\n]\n\nprint(f\"Sample dataset loaded with {len(sample_dataset)} examples\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 2. DKW Controller Class\n\nThe `DKWController` uses the **Dvoretzky-Kiefer-Wolfowitz inequality** to provide statistical guarantees on error rate estimates. Here's how it works:\n\n### Key Parameters:\n- `epsilon_target`: Target error rate threshold (10%)\n- `delta`: Confidence parameter for DKW bound (5%)\n- `min_samples`: Minimum samples before making decisions (100)\n- `hysteresis`: Buffer to prevent mode oscillation (5%)\n\n### Algorithm:\n1. Collect error observations over time\n2. Compute empirical error rate from recent samples\n3. Use DKW inequality to get upper confidence bound on true error rate\n4. Switch modes based on whether bound exceeds target (with hysteresis)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Sample Data (Self-Contained)\n\nSince this is a self-contained notebook, we'll use sample data instead of loading from HuggingFace. This mimics the structure of the GSM8K dataset.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\"\"\"DKW Controller Implementation.\"\"\"\nimport json\nimport numpy as np\nfrom dataclasses import dataclass, field\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Set random seed for reproducible results\nnp.random.seed(42)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "\"\"\"Dataset collection script for DKW benchmark.\"\"\"\nimport json\n# from datasets import load_dataset  # Would be used in full implementation\n\nprint(\"Libraries imported successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Import Dependencies\n\nFirst, let's import the necessary libraries for our implementation:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Imports and Setup\n\nFirst, let's import the necessary libraries. In a full implementation, this would use the `datasets` library to load from HuggingFace, but here we'll work with sample data.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# DKW Controller Implementation Demo\n\nThis notebook demonstrates a **DKW-guided fusion/fission controller** - a statistical method for making decisions between fusion and fission modes based on error observations with confidence guarantees.\n\n## Overview\n- **DKW (Dvoretzky-Kiefer-Wolfowitz)** inequality provides statistical bounds on empirical distributions\n- The controller switches between \"fusion\" and \"fission\" modes based on error rate observations\n- Includes hysteresis to prevent oscillation between modes",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# Dataset Collection Script for DKW Benchmark\n\nThis notebook demonstrates the dataset collection process for DKW controller evaluation. The original script collected data from the GSM8K dataset, but this version is self-contained with sample data inlined.\n\n**Artifact Information:**\n- ID: dataset_001\n- Name: data.py\n- Purpose: Collect and process benchmark data for evaluation",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## How to Modify This Notebook\n\nThis notebook is completely self-contained and ready to use! Here's how you can customize it:\n\n### ðŸ”§ Modify the Data\n- Edit the **Sample Data** cell to change the number of examples or error rates\n- Adjust the fusion/fission decision ratios for either method\n- Add more methods by extending the `results` dictionary\n\n### ðŸ“Š Add New Metrics  \n- Extend the `compute_metrics` function to calculate additional performance measures\n- Add visualizations using matplotlib or seaborn\n- Compare with other baseline methods\n\n### ðŸ’¾ Export Results\n- The metrics are stored in the `eval_output` variable\n- Use `json.dumps()` to save results to a file if needed\n- Create CSV exports for spreadsheet analysis\n\n**Original Script**: This notebook replaces the `eval.py` script and eliminates the need for external JSON files by inlining all data directly in the notebook.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Display detailed metrics in a readable format\nprint(\"=\" * 60)\nprint(\"DETAILED EVALUATION RESULTS\")\nprint(\"=\" * 60)\n\nprint(\"\\nðŸ“Š BASELINE METHOD:\")\nprint(f\"  Fusion Rate:     {metrics['baseline']['fusion_rate']:.1%}\")\nprint(f\"  Fission Rate:    {metrics['baseline']['fission_rate']:.1%}\")  \nprint(f\"  Error Rate:      {metrics['baseline']['error_rate']:.1%}\")\nprint(f\"  Total API Calls: {metrics['baseline']['api_calls']}\")\nprint(f\"  Avg Calls/Example: {metrics['baseline']['avg_calls_per_example']:.2f}\")\n\nprint(\"\\nðŸš€ PROPOSED METHOD:\")\nprint(f\"  Fusion Rate:     {metrics['proposed']['fusion_rate']:.1%}\")\nprint(f\"  Fission Rate:    {metrics['proposed']['fission_rate']:.1%}\")\nprint(f\"  Error Rate:      {metrics['proposed']['error_rate']:.1%}\")  \nprint(f\"  Total API Calls: {metrics['proposed']['api_calls']}\")\nprint(f\"  Avg Calls/Example: {metrics['proposed']['avg_calls_per_example']:.2f}\")\n\nprint(\"\\nðŸ“ˆ IMPROVEMENTS:\")\nprint(f\"  API Reduction:   {metrics['improvement']['api_reduction_pct']:.1f}%\")\nprint(f\"  Error Rate Change: {metrics['improvement']['error_rate_diff']:+.1%}\")\n\nprint(\"\\n\" + \"=\" * 60)\n\n# Also display the raw metrics as JSON for reference\nprint(\"\\nRaw metrics (JSON format):\")\nprint(json.dumps(metrics, indent=2))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Detailed Results\n\nLet's examine all the computed metrics in detail:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Compute metrics from our sample data\nmetrics = compute_metrics(results)\n\n# Display key result (equivalent to the original script's print statement)\nprint(f\"API reduction: {metrics['improvement']['api_reduction_pct']:.1f}%\")\n\n# Store the computed metrics for further analysis\n# (This replaces the original file writing operation)\neval_output = metrics",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Run Evaluation\n\nNow let's compute the metrics using our sample data and display the results. This replaces the original file I/O operations with direct computation.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def compute_metrics(results: dict) -> dict:\n    \"\"\"Compute evaluation metrics for both baseline and proposed methods.\"\"\"\n    metrics = {}\n\n    for method in [\"baseline\", \"proposed\"]:\n        preds = results[method]\n\n        # Count decisions\n        fusion_count = sum(1 for p in preds if p[\"decision\"] == \"fusion\")\n        fission_count = sum(1 for p in preds if p[\"decision\"] == \"fission\")\n\n        # Compute error rate\n        errors = sum(1 for p in preds if p[\"error\"])\n        error_rate = errors / len(preds)\n\n        # API calls (fusion=1, fission=2)\n        api_calls = fusion_count + 2 * fission_count\n\n        metrics[method] = {\n            \"fusion_rate\": fusion_count / len(preds),\n            \"fission_rate\": fission_count / len(preds),\n            \"error_rate\": error_rate,\n            \"api_calls\": api_calls,\n            \"avg_calls_per_example\": api_calls / len(preds),\n        }\n\n    # Compute improvement\n    baseline_calls = metrics[\"baseline\"][\"avg_calls_per_example\"]\n    proposed_calls = metrics[\"proposed\"][\"avg_calls_per_example\"]\n    metrics[\"improvement\"] = {\n        \"api_reduction_pct\": (baseline_calls - proposed_calls) / baseline_calls * 100,\n        \"error_rate_diff\": metrics[\"proposed\"][\"error_rate\"] - metrics[\"baseline\"][\"error_rate\"],\n    }\n\n    return metrics\n\nprint(\"Metrics computation function defined successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Metrics Computation Function\n\nThe `compute_metrics` function analyzes the results from both methods and calculates:\n\n1. **Decision Rates**: Percentage of fusion vs fission decisions\n2. **Error Rates**: Percentage of examples that resulted in errors  \n3. **API Efficiency**: Total API calls (fusion=1 call, fission=2 calls)\n4. **Performance Comparison**: Improvement metrics between methods",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Sample evaluation results data (inlined instead of reading from JSON files)\n# This represents results from 200 test examples for both methods\n\n# Create baseline results: all fission decisions, 8% error rate\nbaseline_results = []\nfor i in range(200):\n    baseline_results.append({\n        \"decision\": \"fission\",\n        \"error\": i < 16  # First 16 examples have errors (8% error rate)\n    })\n\n# Create proposed method results: 65% fusion, 35% fission, 9% error rate  \nproposed_results = []\nfor i in range(200):\n    if i < 130:  # First 130 examples use fusion (65%)\n        decision = \"fusion\"\n    else:  # Remaining 70 examples use fission (35%)\n        decision = \"fission\"\n    \n    proposed_results.append({\n        \"decision\": decision,\n        \"error\": i < 18  # First 18 examples have errors (9% error rate)\n    })\n\n# Combine into the expected data structure\nresults = {\n    \"baseline\": baseline_results,\n    \"proposed\": proposed_results\n}\n\nprint(f\"Created evaluation data:\")\nprint(f\"  Baseline examples: {len(results['baseline'])}\")\nprint(f\"  Proposed examples: {len(results['proposed'])}\")\nprint(f\"  Baseline fusion decisions: {sum(1 for p in results['baseline'] if p['decision'] == 'fusion')}\")\nprint(f\"  Proposed fusion decisions: {sum(1 for p in results['proposed'] if p['decision'] == 'fusion')}\")\nprint(f\"  Baseline errors: {sum(1 for p in results['baseline'] if p['error'])}\")\nprint(f\"  Proposed errors: {sum(1 for p in results['proposed'] if p['error'])}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Sample Data\n\nInstead of reading from external JSON files, we'll define the evaluation data directly in the notebook. This represents the results of running both baseline and proposed methods on a test dataset.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\"\"\"Evaluation script for DKW Controller.\"\"\"\nimport json\nimport numpy as np\nfrom typing import Dict, Any\n\nprint(\"Libraries imported successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Overview\n\nThis evaluation script compares two methods:\n- **Baseline**: Traditional approach that always uses fission decisions\n- **Proposed**: Improved approach that intelligently chooses between fusion and fission\n\nThe metrics we compute include:\n- Fusion/fission decision rates\n- Error rates\n- API call efficiency\n- Performance improvements",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# DKW Controller Evaluation\n\nThis notebook evaluates the performance of DKW Controller methods by comparing baseline and proposed approaches. It computes various metrics including fusion/fission rates, error rates, and API call efficiency.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## ðŸŽ¯ Usage Instructions\n\nThis notebook is now completely self-contained and ready to use! Here's how to customize it:\n\n### ðŸ”§ Configuration Options:\n\n1. **Data Source** (Cell 6):\n   - Set `USE_LIVE_DATA = True` to fetch real data from HuggingFace GSM8K dataset\n   - Set `USE_LIVE_DATA = False` to use the inlined sample data\n\n2. **Export Format** (Cell 8):\n   - `\"display\"` - Show JSON output in notebook (default)\n   - `\"json\"` - Save to `data_out.json` file\n   - `\"csv\"` - Save to `data_out.csv` file\n\n### ðŸš€ Next Steps:\n\n- Modify the difficulty calculation in the `collect_data()` function\n- Add more data processing steps\n- Integrate with your DKW benchmark evaluation pipeline\n- Extend the sample data with more examples\n\n### ðŸ“ Notes:\n\n- No external file dependencies required\n- All data is either fetched from HuggingFace or inlined as Python objects\n- Fully interactive with pandas DataFrame display\n- Error handling included for network issues\n\n**Enjoy your self-contained dataset collection notebook!** ðŸŽ‰",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Export options\ndef export_data(data, format_type=\"json\"):\n    \"\"\"Export data in various formats.\"\"\"\n    if format_type == \"json\":\n        # Original functionality - save as JSON\n        with open(\"data_out.json\", \"w\") as f:\n            json.dump(data, f, indent=2)\n        print(f\"âœ… Exported {len(data)} examples to data_out.json\")\n        \n    elif format_type == \"csv\":\n        # Export as CSV using pandas\n        df.to_csv(\"data_out.csv\", index=False)\n        print(f\"âœ… Exported {len(data)} examples to data_out.csv\")\n        \n    elif format_type == \"display\":\n        # Just display the JSON in notebook\n        print(\"ðŸ“„ JSON Output:\")\n        print(json.dumps(data, indent=2))\n\n# Choose export format\nEXPORT_FORMAT = \"display\"  # Options: \"json\", \"csv\", \"display\"\n\nif EXPORT_FORMAT in [\"json\", \"csv\"]:\n    export_data(data, EXPORT_FORMAT)\nelse:\n    export_data(data, \"display\")\n\nprint(f\"\\\\nðŸŽ‰ Data collection complete! Processed {len(data)} examples.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## ðŸ’¾ Data Export\n\nExport the processed data to various formats. This replaces the original file writing functionality with more flexible export options.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Convert to pandas DataFrame for better display\ndf = pd.DataFrame(data)\n\n# Display basic statistics\nprint(\"ðŸ“ˆ Data Statistics:\")\nprint(f\"   Questions range from {df['difficulty'].min():.3f} to {df['difficulty'].max():.3f} difficulty\")\nprint(f\"   Question lengths: {df['question'].str.len().min()} - {df['question'].str.len().max()} characters\")\n\n# Display the data table\nprint(f\"\\nðŸ—‚ï¸ Complete Dataset ({len(df)} rows):\")\ndisplay(df)\n\n# Show difficulty distribution\nprint(f\"\\nðŸ“Š Difficulty Distribution:\")\ndifficulty_bins = pd.cut(df['difficulty'], bins=3, labels=['Easy', 'Medium', 'Hard'])\nprint(difficulty_bins.value_counts())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## ðŸ“‹ Interactive Data Display\n\nView the collected data in an interactive table format with sorting and filtering capabilities.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Choose your data source\nUSE_LIVE_DATA = False  # Set to True to fetch from HuggingFace, False to use sample data\n\nif USE_LIVE_DATA:\n    print(\"ðŸŒ Collecting live data from HuggingFace GSM8K dataset...\")\n    try:\n        data = collect_data()\n        print(f\"âœ… Successfully collected {len(data)} examples from HuggingFace\")\n    except Exception as e:\n        print(f\"âŒ Error loading live data: {e}\")\n        print(\"ðŸ“‹ Falling back to sample data...\")\n        data = sample_data\nelse:\n    print(\"ðŸ“‹ Using inlined sample data...\")\n    data = sample_data\n\nprint(f\"\\nDataset Summary:\")\nprint(f\"ðŸ“Š Total examples: {len(data)}\")\nprint(f\"ðŸ’¡ Average difficulty: {sum(item['difficulty'] for item in data) / len(data):.3f}\")\nprint(f\"ðŸ“ Average question length: {sum(len(item['question']) for item in data) / len(data):.1f} characters\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## ðŸš€ Run Data Collection\n\nExecute the data collection function and display the results. You can choose to either:\n1. Collect live data from HuggingFace (requires internet connection)\n2. Use the inlined sample data for demonstration",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Sample data inlined from data_out.json\nsample_data = [\n    {\n        \"id\": \"example_000\",\n        \"question\": \"What is 2+2?\",\n        \"answer\": \"4\",\n        \"difficulty\": 0.15\n    },\n    {\n        \"id\": \"example_001\",\n        \"question\": \"If x=5, what is 2x?\",\n        \"answer\": \"10\",\n        \"difficulty\": 0.22\n    },\n    {\n        \"id\": \"example_002\",\n        \"question\": \"Solve: 3y + 6 = 15\",\n        \"answer\": \"y=3\",\n        \"difficulty\": 0.28\n    }\n]\n\nprint(f\"Sample dataset contains {len(sample_data)} examples\")\nprint(\"Sample data structure:\")\nfor item in sample_data[:1]:  # Show structure of first item\n    for key, value in item.items():\n        print(f\"  {key}: {value} ({type(value).__name__})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## ðŸ“Š Sample Data (Inlined)\n\nInstead of reading from external JSON files, here's the sample data inlined for demonstration purposes. This makes the notebook completely self-contained.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def collect_data():\n    \"\"\"Collect benchmark data for DKW controller evaluation.\"\"\"\n    # Load HuggingFace dataset\n    print(\"Loading GSM8K dataset from HuggingFace...\")\n    ds = load_dataset(\"gsm8k\", \"main\", split=\"test[:200]\")\n    print(f\"Loaded {len(ds)} examples\")\n\n    data = []\n    for i, example in enumerate(ds):\n        data.append({\n            \"id\": f\"example_{i:03d}\",\n            \"question\": example[\"question\"],\n            \"answer\": example[\"answer\"],\n            \"difficulty\": len(example[\"question\"]) / 100,  # Simple proxy based on question length\n        })\n\n    return data",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## ðŸ”§ Data Collection Function\n\nThe main function that processes the GSM8K dataset and creates structured benchmark data with difficulty scoring.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\"\"\"Dataset collection script for DKW benchmark.\"\"\"\nimport json\nfrom datasets import load_dataset\nimport pandas as pd\nfrom IPython.display import display, HTML",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## ðŸ“¦ Imports and Setup",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# Dataset Collection Script for DKW Benchmark\n\n**Artifact:** dataset_001 - data.py\n\nThis notebook contains a self-contained version of the dataset collection script for DKW benchmark evaluation. It loads and processes the GSM8K dataset to create benchmark data for controller evaluation.\n\n## Features:\n- Loads HuggingFace GSM8K dataset\n- Processes examples with metadata (ID, difficulty score)\n- Displays collected data in an interactive format\n- Completely self-contained with inlined sample data",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Usage Notes\n\n### Running this notebook:\n1. **Self-contained mode**: Run all cells as-is to see sample data processing\n2. **Live mode**: Uncomment the live data collection lines to fetch real GSM8K data\n\n### Modifications you can make:\n- Change the dataset split in `collect_data()` (e.g., `\"test[:500]\"` for more examples)\n- Modify the difficulty calculation logic\n- Add additional data processing steps\n- Export results to different formats\n\n### Original vs. Notebook differences:\n- âœ… **Inlined data**: No dependency on `data_out.json` file\n- âœ… **Interactive**: Can run sections independently\n- âœ… **Documented**: Clear explanations for each step\n- âœ… **Flexible**: Easy to modify and experiment with\n\n**This notebook is completely self-contained and ready to run!**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Display the collected data in a readable format\nfor i, example in enumerate(data):\n    print(f\"\\n--- Example {i+1} ---\")\n    print(f\"ID: {example['id']}\")\n    print(f\"Question: {example['question']}\")\n    print(f\"Answer: {example['answer']}\")\n    print(f\"Difficulty: {example['difficulty']:.2f}\")\n\nprint(f\"\\nâœ… Successfully processed {len(data)} examples\")\nprint(\"ðŸ“ Data structure matches the original script output\")\nprint(\"ðŸ”„ Notebook is completely self-contained - no external files needed!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Option 1: Live data collection (uncomment to run with internet access)\n# print(\"Collecting live data from HuggingFace...\")\n# live_data = collect_data()\n# print(f\"Collected {len(live_data)} examples from GSM8K dataset\")\n\n# Option 2: Use sample data for demonstration (self-contained)\nprint(\"Using sample data for demonstration:\")\ndata = sample_output_data\n\n# Original script would save to file - here we'll just display\n# with open(\"data_out.json\", \"w\") as f:\n#     json.dump(data, f, indent=2)\n\nprint(f\"Collected {len(data)} examples\")\nprint(\"\\nFirst few examples:\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Execute Data Collection\n\nNow let's run the data collection function. In the original script, this would load from HuggingFace and save to a file. Here we'll demonstrate both approaches:\n\n1. **Live data collection** (requires internet and HuggingFace datasets)\n2. **Display of sample data** (self-contained)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Inlined sample data (replaces data_out.json for self-contained execution)\nsample_output_data = [\n    {\n        \"id\": \"example_000\",\n        \"question\": \"What is 2+2?\",\n        \"answer\": \"4\",\n        \"difficulty\": 0.15\n    },\n    {\n        \"id\": \"example_001\",\n        \"question\": \"If x=5, what is 2x?\",\n        \"answer\": \"10\",\n        \"difficulty\": 0.22\n    },\n    {\n        \"id\": \"example_002\",\n        \"question\": \"Solve: 3y + 6 = 15\",\n        \"answer\": \"y=3\",\n        \"difficulty\": 0.28\n    }\n]\n\nprint(f\"Sample data contains {len(sample_output_data)} examples\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Self-Contained Data (No External Dependencies)\n\nInstead of reading from `data_out.json`, we'll inline the sample data here to make this notebook completely self-contained. This demonstrates what the output would look like without requiring external files.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def collect_data():\n    \"\"\"Collect benchmark data for DKW controller evaluation.\"\"\"\n    # Load HuggingFace dataset\n    ds = load_dataset(\"gsm8k\", \"main\", split=\"test[:200]\")\n\n    data = []\n    for i, example in enumerate(ds):\n        data.append({\n            \"id\": f\"example_{i:03d}\",\n            \"question\": example[\"question\"],\n            \"answer\": example[\"answer\"],\n            \"difficulty\": len(example[\"question\"]) / 100,  # Simple proxy\n        })\n\n    return data",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "\"\"\"Dataset collection script for DKW benchmark.\"\"\"\nimport json\nfrom datasets import load_dataset",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Overview\n\nThis notebook demonstrates:\n1. **Data Collection**: Loading benchmark data from HuggingFace's GSM8K dataset\n2. **Data Processing**: Formatting examples with IDs, questions, answers, and difficulty scores\n3. **Self-contained execution**: All data is inlined to eliminate external dependencies\n\nThe original script would save data to `data_out.json` - here we'll display the results directly and provide the data inline for demonstration.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# Dataset Collection for DKW Benchmark\n\n**Artifact:** dataset_001 (data.py)\n\nThis notebook contains a self-contained version of the dataset collection script for DKW benchmark evaluation. The original script has been converted to run entirely within this notebook without any external file dependencies.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}