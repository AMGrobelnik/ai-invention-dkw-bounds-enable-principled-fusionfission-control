{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## Customization\n\nTo modify this notebook for your own data:\n\n1. **Update the sample data** in the \"Sample Data\" section:\n   - Replace the `baseline_data` and `proposed_data` lists with your actual experimental results\n   - Each item should be a dictionary with `\"decision\"` (\"fusion\" or \"fission\") and `\"error\"` (True/False) keys\n\n2. **Modify metrics calculation** if needed:\n   - The `compute_metrics` function assumes fusion=1 API call and fission=2 API calls\n   - Adjust these values in the function if your system uses different call patterns\n\n3. **Add new visualizations** or analysis by creating additional code cells below\n\nThe notebook is completely self-contained and doesn't require any external files to run!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\n\n# Create a comparison chart\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))\n\nmethods = ['Baseline', 'Proposed']\n\n# 1. Decision Types\nfusion_rates = [metrics['baseline']['fusion_rate'], metrics['proposed']['fusion_rate']]\nfission_rates = [metrics['baseline']['fission_rate'], metrics['proposed']['fission_rate']]\n\nax1.bar(methods, fusion_rates, label='Fusion Rate', alpha=0.7)\nax1.bar(methods, fission_rates, bottom=fusion_rates, label='Fission Rate', alpha=0.7)\nax1.set_ylabel('Rate')\nax1.set_title('Decision Type Distribution')\nax1.legend()\n\n# 2. API Calls per Example\napi_calls = [metrics['baseline']['avg_calls_per_example'], metrics['proposed']['avg_calls_per_example']]\nbars = ax2.bar(methods, api_calls, color=['red', 'green'], alpha=0.7)\nax2.set_ylabel('Average API Calls')\nax2.set_title('API Efficiency')\nfor i, bar in enumerate(bars):\n    height = bar.get_height()\n    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n             f'{height:.2f}', ha='center', va='bottom')\n\n# 3. Error Rates\nerror_rates = [metrics['baseline']['error_rate'], metrics['proposed']['error_rate']]\nax3.bar(methods, error_rates, color=['orange', 'blue'], alpha=0.7)\nax3.set_ylabel('Error Rate')\nax3.set_title('Error Rate Comparison')\nfor i, bar in enumerate(ax3.patches):\n    height = bar.get_height()\n    ax3.text(bar.get_x() + bar.get_width()/2., height + 0.002,\n             f'{height:.3f}', ha='center', va='bottom')\n\n# 4. Key Improvements\nimprovements = ['API Reduction %', 'Error Rate Diff']\nvalues = [metrics['improvement']['api_reduction_pct'], metrics['improvement']['error_rate_diff'] * 100]  # Convert to percentage\ncolors = ['green' if v > 0 else 'red' for v in values]\nbars = ax4.bar(improvements, values, color=colors, alpha=0.7)\nax4.set_ylabel('Improvement (%)')\nax4.set_title('Key Improvements')\nax4.axhline(y=0, color='black', linestyle='-', alpha=0.3)\nfor i, bar in enumerate(bars):\n    height = bar.get_height()\n    ax4.text(bar.get_x() + bar.get_width()/2., height + (0.5 if height > 0 else -1),\n             f'{height:.1f}%', ha='center', va='bottom' if height > 0 else 'top')\n\nplt.tight_layout()\nplt.show()\n\n# Summary\nprint(f\"\\nðŸŽ¯ KEY FINDINGS:\")\nprint(f\"   â€¢ API calls reduced by {metrics['improvement']['api_reduction_pct']:.1f}%\")\nprint(f\"   â€¢ Error rate changed by {metrics['improvement']['error_rate_diff']:.1f} percentage points\")\nprint(f\"   â€¢ Proposed method uses {metrics['proposed']['fusion_rate']:.0%} fusion vs {metrics['baseline']['fusion_rate']:.0%} baseline\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Visualization\n\nLet's create some simple visualizations to better understand the results:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Display the JSON output (equivalent to writing eval_out.json)\nprint(\"Contents of eval_out.json:\")\nprint(json.dumps(metrics, indent=2))\n\n# Optionally save to file (uncomment if you want to create the file)\n# with open(\"eval_out.json\", \"w\") as f:\n#     json.dump(metrics, f, indent=2)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Save Results\n\nThe original script saved results to `eval_out.json`. Here we'll display the JSON output:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Compute metrics\nmetrics = compute_metrics(results)\n\n# Display the main result (equivalent to the original script's print statement)\nprint(f\"API reduction: {metrics['improvement']['api_reduction_pct']:.1f}%\")\nprint()\n\n# Display all metrics in a nicely formatted way\nprint(\"=== DETAILED RESULTS ===\")\nprint()\nprint(\"Baseline Method:\")\nfor key, value in metrics[\"baseline\"].items():\n    if isinstance(value, float):\n        print(f\"  {key}: {value:.3f}\")\n    else:\n        print(f\"  {key}: {value}\")\n\nprint()\nprint(\"Proposed Method:\")\nfor key, value in metrics[\"proposed\"].items():\n    if isinstance(value, float):\n        print(f\"  {key}: {value:.3f}\")\n    else:\n        print(f\"  {key}: {value}\")\n\nprint()\nprint(\"Improvements:\")\nfor key, value in metrics[\"improvement\"].items():\n    if isinstance(value, float):\n        print(f\"  {key}: {value:.3f}\")\n    else:\n        print(f\"  {key}: {value}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Run Evaluation\n\nNow let's compute the metrics and display the results:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def compute_metrics(results: dict) -> dict:\n    \"\"\"Compute evaluation metrics.\"\"\"\n    metrics = {}\n\n    for method in [\"baseline\", \"proposed\"]:\n        preds = results[method]\n\n        # Count decisions\n        fusion_count = sum(1 for p in preds if p[\"decision\"] == \"fusion\")\n        fission_count = sum(1 for p in preds if p[\"decision\"] == \"fission\")\n\n        # Compute error rate\n        errors = sum(1 for p in preds if p[\"error\"])\n        error_rate = errors / len(preds)\n\n        # API calls (fusion=1, fission=2)\n        api_calls = fusion_count + 2 * fission_count\n\n        metrics[method] = {\n            \"fusion_rate\": fusion_count / len(preds),\n            \"fission_rate\": fission_count / len(preds),\n            \"error_rate\": error_rate,\n            \"api_calls\": api_calls,\n            \"avg_calls_per_example\": api_calls / len(preds),\n        }\n\n    # Compute improvement\n    baseline_calls = metrics[\"baseline\"][\"avg_calls_per_example\"]\n    proposed_calls = metrics[\"proposed\"][\"avg_calls_per_example\"]\n    metrics[\"improvement\"] = {\n        \"api_reduction_pct\": (baseline_calls - proposed_calls) / baseline_calls * 100,\n        \"error_rate_diff\": metrics[\"proposed\"][\"error_rate\"] - metrics[\"baseline\"][\"error_rate\"],\n    }\n\n    return metrics",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Evaluation Function\n\nThe `compute_metrics` function analyzes the experimental results and calculates:\n- **Fusion/Fission rates**: Percentage of each decision type\n- **Error rate**: Percentage of examples with errors\n- **API calls**: Total calls (fusion=1 call, fission=2 calls)\n- **Improvement metrics**: Comparison between baseline and proposed methods",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Sample experimental results data (normally loaded from method_out.json)\n# This data is constructed to match the expected output metrics\n\n# Create baseline data: 200 examples, all fission decisions, 8% error rate\nbaseline_data = []\nfor i in range(200):\n    baseline_data.append({\n        \"decision\": \"fission\",\n        \"error\": i < 16  # First 16 examples have errors (8% of 200)\n    })\n\n# Create proposed data: 200 examples, 65% fusion, 35% fission, 9% error rate\nproposed_data = []\nfor i in range(200):\n    if i < 130:  # First 130 examples are fusion (65% of 200)\n        decision = \"fusion\"\n    else:  # Remaining 70 examples are fission (35% of 200)\n        decision = \"fission\"\n    \n    proposed_data.append({\n        \"decision\": decision,\n        \"error\": i < 18  # First 18 examples have errors (9% of 200)\n    })\n\n# Combine into the expected format\nresults = {\n    \"baseline\": baseline_data,\n    \"proposed\": proposed_data\n}\n\nprint(f\"Loaded data:\")\nprint(f\"- Baseline: {len(results['baseline'])} examples\")\nprint(f\"- Proposed: {len(results['proposed'])} examples\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Sample Data\n\nThe following cell contains sample experimental results data that would normally be loaded from `method_out.json`. This data represents 200 examples for each method (baseline and proposed) with their decisions and error status.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\"\"\"Required imports for the evaluation.\"\"\"\nimport json\nimport numpy as np",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# DKW Controller Evaluation\n\nThis notebook evaluates the performance of a DKW (Decision Knowledge Worker) Controller, comparing baseline and proposed methods across several metrics including API call efficiency and error rates.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}