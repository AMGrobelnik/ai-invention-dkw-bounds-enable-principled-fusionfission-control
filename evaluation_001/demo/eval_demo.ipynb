{
 "cells": [
  {
   "cell_type": "code",
   "source": "# Customizable parameters - modify these to test different scenarios\nNUM_EXAMPLES = 200\nPROPOSED_FUSION_RATE = 0.65  # What fraction of proposed decisions are fusion\nBASELINE_ERROR_RATE = 0.08   # Baseline error rate\nPROPOSED_ERROR_RATE = 0.09   # Proposed method error rate\n\ndef generate_custom_data(n_examples, fusion_rate, baseline_error_rate, proposed_error_rate):\n    \"\"\"Generate custom evaluation data with specified parameters.\"\"\"\n    \n    # Baseline: all fission decisions\n    baseline_data = []\n    for i in range(n_examples):\n        error_flag = i < int(n_examples * baseline_error_rate)\n        baseline_data.append({\"decision\": \"fission\", \"error\": error_flag})\n    \n    # Proposed: mix of fusion/fission\n    proposed_data = []\n    n_fusion = int(n_examples * fusion_rate)\n    for i in range(n_examples):\n        decision = \"fusion\" if i < n_fusion else \"fission\"\n        error_flag = i < int(n_examples * proposed_error_rate)\n        proposed_data.append({\"decision\": decision, \"error\": error_flag})\n    \n    return {\"baseline\": baseline_data, \"proposed\": proposed_data}\n\n# Generate new data with custom parameters\ncustom_results = generate_custom_data(NUM_EXAMPLES, PROPOSED_FUSION_RATE, BASELINE_ERROR_RATE, PROPOSED_ERROR_RATE)\ncustom_metrics = compute_metrics(custom_results)\n\nprint(f\"ðŸ”§ Custom Analysis Results (Fusion Rate: {PROPOSED_FUSION_RATE:.0%}):\")\nprint(f\"   API Reduction: {custom_metrics['improvement']['api_reduction_pct']:.1f}%\")\nprint(f\"   Error Rate Change: {custom_metrics['improvement']['error_rate_diff']:.2%}\")\n\n# Quick comparison function\ndef compare_fusion_rates(rates):\n    \"\"\"Compare API reduction for different fusion rates.\"\"\"\n    print(\"\\nðŸ” Fusion Rate Sensitivity Analysis:\")\n    print(\"-\" * 40)\n    for rate in rates:\n        test_results = generate_custom_data(200, rate, 0.08, 0.09)\n        test_metrics = compute_metrics(test_results)\n        reduction = test_metrics['improvement']['api_reduction_pct']\n        print(f\"   {rate:.0%} Fusion Rate â†’ {reduction:.1f}% API Reduction\")\n\n# Example sensitivity analysis\ncompare_fusion_rates([0.3, 0.5, 0.65, 0.8, 1.0])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Customizable Parameters\n\nYou can modify the parameters below and re-run the analysis with different scenarios.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\n\n# Create comparison charts\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))\n\n# 1. Decision Distribution\nmethods = ['Baseline', 'Proposed']\nfusion_rates = [metrics['baseline']['fusion_rate'], metrics['proposed']['fusion_rate']]\nfission_rates = [metrics['baseline']['fission_rate'], metrics['proposed']['fission_rate']]\n\nx = range(len(methods))\nwidth = 0.35\n\nax1.bar([i - width/2 for i in x], fusion_rates, width, label='Fusion', color='lightgreen')\nax1.bar([i + width/2 for i in x], fission_rates, width, label='Fission', color='lightcoral')\nax1.set_xlabel('Method')\nax1.set_ylabel('Rate')\nax1.set_title('Decision Distribution')\nax1.set_xticks(x)\nax1.set_xticklabels(methods)\nax1.legend()\n\n# 2. Error Rates\nerror_rates = [metrics['baseline']['error_rate'], metrics['proposed']['error_rate']]\nax2.bar(methods, error_rates, color=['lightblue', 'orange'])\nax2.set_ylabel('Error Rate')\nax2.set_title('Error Rate Comparison')\n\n# 3. API Calls per Example\napi_calls = [metrics['baseline']['avg_calls_per_example'], metrics['proposed']['avg_calls_per_example']]\nax3.bar(methods, api_calls, color=['lightblue', 'orange'])\nax3.set_ylabel('Avg API Calls per Example')\nax3.set_title('API Efficiency')\n\n# 4. Total API Calls\ntotal_calls = [metrics['baseline']['api_calls'], metrics['proposed']['api_calls']]\nax4.bar(methods, total_calls, color=['lightblue', 'orange'])\nax4.set_ylabel('Total API Calls')\nax4.set_title('Total API Usage')\n\nplt.tight_layout()\nplt.show()\n\n# Summary table\nprint(\"\\nðŸ“‹ Summary Comparison:\")\nprint(\"-\" * 60)\nprint(f\"{'Metric':<25} {'Baseline':<12} {'Proposed':<12} {'Change':<12}\")\nprint(\"-\" * 60)\nprint(f\"{'Fusion Rate':<25} {metrics['baseline']['fusion_rate']:<12.2%} {metrics['proposed']['fusion_rate']:<12.2%} {'+' if metrics['proposed']['fusion_rate'] > metrics['baseline']['fusion_rate'] else ''}{metrics['proposed']['fusion_rate'] - metrics['baseline']['fusion_rate']:<11.2%}\")\nprint(f\"{'Error Rate':<25} {metrics['baseline']['error_rate']:<12.2%} {metrics['proposed']['error_rate']:<12.2%} {'+' if metrics['improvement']['error_rate_diff'] > 0 else ''}{metrics['improvement']['error_rate_diff']:<11.2%}\")\nprint(f\"{'Avg API Calls':<25} {metrics['baseline']['avg_calls_per_example']:<12.2f} {metrics['proposed']['avg_calls_per_example']:<12.2f} {metrics['improvement']['api_reduction_pct']:<11.1f}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Interactive Analysis\n\nLet's add some interactive analysis and visualization to better understand the results.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Compute evaluation metrics\nmetrics = compute_metrics(results)\n\n# Save results to JSON (optional, for compatibility with original script)\n# In the notebook context, we'll just display the results instead\neval_output = json.dumps(metrics, indent=2)\nprint(\"Evaluation Results:\")\nprint(\"=\" * 50)\nprint(eval_output)\n\n# Display key improvement metric (matching original script output)\nprint(\"\\n\" + \"=\" * 50)\nprint(f\"ðŸš€ API reduction: {metrics['improvement']['api_reduction_pct']:.1f}%\")\nprint(f\"ðŸ“Š Error rate change: {metrics['improvement']['error_rate_diff']:.2f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Run Evaluation\n\nNow let's compute the metrics and display the results. This cell replicates the main execution logic from the original script.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def compute_metrics(results: dict) -> dict:\n    \"\"\"Compute evaluation metrics for both baseline and proposed methods.\"\"\"\n    metrics = {}\n\n    for method in [\"baseline\", \"proposed\"]:\n        preds = results[method]\n\n        # Count decisions\n        fusion_count = sum(1 for p in preds if p[\"decision\"] == \"fusion\")\n        fission_count = sum(1 for p in preds if p[\"decision\"] == \"fission\")\n\n        # Compute error rate\n        errors = sum(1 for p in preds if p[\"error\"])\n        error_rate = errors / len(preds)\n\n        # API calls (fusion=1, fission=2)\n        api_calls = fusion_count + 2 * fission_count\n\n        metrics[method] = {\n            \"fusion_rate\": fusion_count / len(preds),\n            \"fission_rate\": fission_count / len(preds),\n            \"error_rate\": error_rate,\n            \"api_calls\": api_calls,\n            \"avg_calls_per_example\": api_calls / len(preds),\n        }\n\n    # Compute improvement metrics\n    baseline_calls = metrics[\"baseline\"][\"avg_calls_per_example\"]\n    proposed_calls = metrics[\"proposed\"][\"avg_calls_per_example\"]\n    metrics[\"improvement\"] = {\n        \"api_reduction_pct\": (baseline_calls - proposed_calls) / baseline_calls * 100,\n        \"error_rate_diff\": metrics[\"proposed\"][\"error_rate\"] - metrics[\"baseline\"][\"error_rate\"],\n    }\n\n    return metrics",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Evaluation Metrics Function\n\nThe `compute_metrics` function analyzes the decision patterns and computes key performance indicators:\n\n- **Fusion/Fission rates**: Percentage of each decision type\n- **Error rate**: Fraction of predictions with errors\n- **API efficiency**: Total API calls (fusion=1, fission=2) and average per example\n- **Improvement metrics**: Comparison between baseline and proposed methods",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import json\nimport numpy as np\n\n# Simulated evaluation results - inlined data to make notebook self-contained\n# This replaces reading from \"../experiment_001/method_out.json\"\n\n# Create baseline method data: all fission decisions, 8% error rate\nbaseline_data = []\nfor i in range(200):\n    error_flag = i < 16  # First 16 examples have errors (8% error rate)\n    baseline_data.append({\n        \"decision\": \"fission\", \n        \"error\": error_flag\n    })\n\n# Create proposed method data: 65% fusion, 35% fission, 9% error rate  \nproposed_data = []\nfor i in range(200):\n    if i < 130:  # First 130 are fusion decisions (65%)\n        decision = \"fusion\"\n    else:  # Remaining 70 are fission decisions (35%)\n        decision = \"fission\"\n    \n    error_flag = i < 18  # First 18 examples have errors (9% error rate)\n    proposed_data.append({\n        \"decision\": decision,\n        \"error\": error_flag\n    })\n\n# Combine into the expected format\nresults = {\n    \"baseline\": baseline_data,\n    \"proposed\": proposed_data\n}\n\nprint(f\"Data loaded:\")\nprint(f\"- Baseline: {len(results['baseline'])} examples\")\nprint(f\"- Proposed: {len(results['proposed'])} examples\")\nprint(f\"- Sample baseline: {results['baseline'][0]}\")\nprint(f\"- Sample proposed: {results['proposed'][0]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Data Setup\n\nFirst, let's define our evaluation data. This simulates the results from both the baseline and proposed methods, with each prediction containing a decision type and error flag.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# DKW Controller Evaluation\n\nThis notebook evaluates the performance of a DKW (Decision, Knowledge, Workflow) Controller, comparing baseline and proposed methods. The evaluation focuses on:\n\n- **Fusion vs Fission decisions**: How often each method chooses fusion (1 API call) vs fission (2 API calls)\n- **Error rates**: Accuracy of the decision-making process  \n- **API efficiency**: Total API calls and reduction percentage\n\nThis is a self-contained notebook with all data inlined for easy execution.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}