{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DKW Controller Evaluation\n",
    "\n",
    "This notebook contains an evaluation script for the DKW Controller, comparing baseline and proposed methods across various metrics including fusion/fission decision rates, error rates, and API call efficiency.\n",
    "\n",
    "**Artifact ID:** evaluation_001  \n",
    "**Original File:** eval.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup\n",
    "\n",
    "Import required libraries for the evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluation script for DKW Controller.\"\"\"\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data (Inlined for Self-Contained Demo)\n",
    "\n",
    "The original script reads from `../experiment_001/method_out.json`. For this self-contained notebook, we'll inline the data that produces the exact same results shown in `eval_out.json`.\n",
    "\n",
    "The evaluation compares two methods:\n",
    "- **Baseline**: Uses fission-only strategy (2 API calls per example)\n",
    "- **Proposed**: Uses adaptive fusion/fission strategy (1-2 API calls per example)\n",
    "\n",
    "Each prediction contains:\n",
    "- `decision`: Either \"fusion\" (1 API call) or \"fission\" (2 API calls)\n",
    "- `error`: Boolean indicating if the prediction was incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inline sample data (replaces reading from external JSON files)\n",
    "# This data produces the exact metrics shown in eval_out.json\n",
    "\n",
    "# Create sample data for 200 examples each\n",
    "results = {\n",
    "    \"baseline\": [\n",
    "        # All fission decisions, 8% error rate (16 errors out of 200)\n",
    "        *[{\"decision\": \"fission\", \"error\": True} for _ in range(16)],   # 16 errors\n",
    "        *[{\"decision\": \"fission\", \"error\": False} for _ in range(184)]  # 184 correct\n",
    "    ],\n",
    "    \"proposed\": [\n",
    "        # 65% fusion (130), 35% fission (70), 9% error rate (18 errors out of 200)\n",
    "        *[{\"decision\": \"fusion\", \"error\": True} for _ in range(12)],    # 12 fusion errors  \n",
    "        *[{\"decision\": \"fusion\", \"error\": False} for _ in range(118)],  # 118 fusion correct\n",
    "        *[{\"decision\": \"fission\", \"error\": True} for _ in range(6)],    # 6 fission errors\n",
    "        *[{\"decision\": \"fission\", \"error\": False} for _ in range(64)]   # 64 fission correct\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(f\"Baseline examples: {len(results['baseline'])}\")\n",
    "print(f\"Proposed examples: {len(results['proposed'])}\")\n",
    "print(f\"\\nBaseline fusion decisions: {sum(1 for p in results['baseline'] if p['decision'] == 'fusion')}\")\n",
    "print(f\"Proposed fusion decisions: {sum(1 for p in results['proposed'] if p['decision'] == 'fusion')}\")\n",
    "print(f\"Proposed fission decisions: {sum(1 for p in results['proposed'] if p['decision'] == 'fission')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics Computation Function\n",
    "\n",
    "The `compute_metrics` function calculates:\n",
    "- **Fusion/Fission rates**: Proportion of each decision type\n",
    "- **Error rate**: Proportion of incorrect predictions  \n",
    "- **API calls**: Total calls (fusion=1, fission=2 per example)\n",
    "- **API efficiency**: Average calls per example\n",
    "- **Improvement metrics**: Percentage reduction and error rate difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(results: dict) -> dict:\n",
    "    \"\"\"Compute evaluation metrics.\"\"\"\n",
    "    metrics = {}\n",
    "\n",
    "    for method in [\"baseline\", \"proposed\"]:\n",
    "        preds = results[method]\n",
    "\n",
    "        # Count decisions\n",
    "        fusion_count = sum(1 for p in preds if p[\"decision\"] == \"fusion\")\n",
    "        fission_count = sum(1 for p in preds if p[\"decision\"] == \"fission\")\n",
    "\n",
    "        # Compute error rate\n",
    "        errors = sum(1 for p in preds if p[\"error\"])\n",
    "        error_rate = errors / len(preds)\n",
    "\n",
    "        # API calls (fusion=1, fission=2)\n",
    "        api_calls = fusion_count + 2 * fission_count\n",
    "\n",
    "        metrics[method] = {\n",
    "            \"fusion_rate\": fusion_count / len(preds),\n",
    "            \"fission_rate\": fission_count / len(preds),\n",
    "            \"error_rate\": error_rate,\n",
    "            \"api_calls\": api_calls,\n",
    "            \"avg_calls_per_example\": api_calls / len(preds),\n",
    "        }\n",
    "\n",
    "    # Compute improvement\n",
    "    baseline_calls = metrics[\"baseline\"][\"avg_calls_per_example\"]\n",
    "    proposed_calls = metrics[\"proposed\"][\"avg_calls_per_example\"]\n",
    "    metrics[\"improvement\"] = {\n",
    "        \"api_reduction_pct\": (baseline_calls - proposed_calls) / baseline_calls * 100,\n",
    "        \"error_rate_diff\": metrics[\"proposed\"][\"error_rate\"] - metrics[\"baseline\"][\"error_rate\"],\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation\n",
    "\n",
    "Execute the metrics computation and display results (replicating the original script output):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics (equivalent to: metrics = compute_metrics(results))\n",
    "metrics = compute_metrics(results)\n",
    "\n",
    "# Display main result (matching original script output)\n",
    "print(f\"API reduction: {metrics['improvement']['api_reduction_pct']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results\n",
    "\n",
    "Display the complete evaluation results in JSON format (equivalent to what would be saved to `eval_out.json`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the metrics in formatted JSON (replicating file output)\n",
    "print(\"Contents of eval_out.json:\")\n",
    "print(json.dumps(metrics, indent=2))\n",
    "\n",
    "# Optionally save to file (uncomment to use)\n",
    "# with open(\"eval_out.json\", \"w\") as f:\n",
    "#     json.dump(metrics, f, indent=2)\n",
    "# print(\"\\nMetrics saved to eval_out.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Summary\n",
    "\n",
    "Interactive analysis of the evaluation results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract key findings for analysis\n",
    "baseline = metrics[\"baseline\"]\n",
    "proposed = metrics[\"proposed\"] \n",
    "improvement = metrics[\"improvement\"]\n",
    "\n",
    "print(\"üìä DKW CONTROLLER EVALUATION SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üéØ API Call Reduction: {improvement['api_reduction_pct']:.1f}%\")\n",
    "print(f\"üìà Baseline avg calls/example: {baseline['avg_calls_per_example']:.2f}\")\n",
    "print(f\"üìâ Proposed avg calls/example: {proposed['avg_calls_per_example']:.2f}\")\n",
    "print()\n",
    "print(\"üîÄ Decision Strategy Comparison:\")\n",
    "print(f\"   Baseline: {baseline['fusion_rate']:.0%} fusion, {baseline['fission_rate']:.0%} fission\")\n",
    "print(f\"   Proposed: {proposed['fusion_rate']:.0%} fusion, {proposed['fission_rate']:.0%} fission\") \n",
    "print()\n",
    "print(\"‚ö†Ô∏è Error Rate Analysis:\")\n",
    "print(f\"   Baseline: {baseline['error_rate']:.1%}\")\n",
    "print(f\"   Proposed: {proposed['error_rate']:.1%}\")\n",
    "print(f\"   Difference: {improvement['error_rate_diff']:+.1%}\")\n",
    "print()\n",
    "print(\"üí° Key Insight:\")\n",
    "print(f\"   The proposed method achieves a {improvement['api_reduction_pct']:.1f}% reduction in API calls\")\n",
    "print(f\"   by using fusion {proposed['fusion_rate']:.0%} of the time, with only a\")\n",
    "print(f\"   {improvement['error_rate_diff']:.1%} increase in error rate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Modify This Notebook\n",
    "\n",
    "This notebook is completely self-contained and runnable. To customize it:\n",
    "\n",
    "1. **Change the data**: Modify the `results` dictionary to test different scenarios\n",
    "2. **Add new metrics**: Extend the `compute_metrics()` function \n",
    "3. **Export results**: Uncomment the file saving code to write JSON output\n",
    "4. **Add visualization**: Use matplotlib/seaborn to create charts\n",
    "\n",
    "### Key Changes from Original Script:\n",
    "- **No external file dependencies**: JSON data is inlined as Python dictionaries\n",
    "- **Interactive exploration**: Added detailed analysis and formatted output\n",
    "- **Self-contained**: Can be run without any additional files\n",
    "\n",
    "### Expected Output Verification:\n",
    "This notebook produces the exact same results as shown in the provided `eval_out.json`:\n",
    "- API reduction: 32.5%\n",
    "- Baseline: 0% fusion, 100% fission, 8% error rate\n",
    "- Proposed: 65% fusion, 35% fission, 9% error rate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}