{
 "cells": [
  {
   "cell_type": "code",
   "source": "# Display comprehensive results\nprint(\"ðŸ“ˆ DETAILED EVALUATION METRICS\")\nprint(\"=\" * 50)\n\nprint(\"\\nðŸ”¹ BASELINE METHOD:\")\nfor key, value in metrics[\"baseline\"].items():\n    if \"rate\" in key:\n        print(f\"  {key}: {value:.1%}\")\n    elif \"calls\" in key:\n        print(f\"  {key}: {value:.2f}\")\n    else:\n        print(f\"  {key}: {value}\")\n\nprint(\"\\nðŸ”¸ PROPOSED METHOD:\")\nfor key, value in metrics[\"proposed\"].items():\n    if \"rate\" in key:\n        print(f\"  {key}: {value:.1%}\")\n    elif \"calls\" in key:\n        print(f\"  {key}: {value:.2f}\")\n    else:\n        print(f\"  {key}: {value}\")\n\nprint(\"\\nðŸš€ IMPROVEMENT:\")\nprint(f\"  API Reduction: {metrics['improvement']['api_reduction_pct']:.1f}%\")\nprint(f\"  Error Rate Change: {metrics['improvement']['error_rate_diff']:+.1%}\")\n\n# Pretty print the complete results (equivalent to the JSON output)\nprint(\"\\n\" + \"=\" * 50)\nprint(\"ðŸ“‹ COMPLETE RESULTS (JSON format):\")\nprint(json.dumps(metrics, indent=2))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Detailed Results\n\nLet's examine the detailed metrics to understand the performance comparison between baseline and proposed methods.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Run the evaluation (equivalent to the original script's main section)\nmetrics = compute_metrics(results)\n\n# Display the key result (equivalent to the original print statement)\nprint(f\"ðŸŽ¯ API reduction: {metrics['improvement']['api_reduction_pct']:.1f}%\")\n\n# Save results to variable (equivalent to writing eval_out.json)\neval_output = metrics\nprint(\"ðŸ“Š Evaluation completed successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 4. Run Evaluation\n\nLet's compute the metrics and save the results. This replaces the file I/O from the original script.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def compute_metrics(results: dict) -> dict:\n    \"\"\"Compute evaluation metrics.\"\"\"\n    metrics = {}\n\n    for method in [\"baseline\", \"proposed\"]:\n        preds = results[method]\n\n        # Count decisions\n        fusion_count = sum(1 for p in preds if p[\"decision\"] == \"fusion\")\n        fission_count = sum(1 for p in preds if p[\"decision\"] == \"fission\")\n\n        # Compute error rate\n        errors = sum(1 for p in preds if p[\"error\"])\n        error_rate = errors / len(preds)\n\n        # API calls (fusion=1, fission=2)\n        api_calls = fusion_count + 2 * fission_count\n\n        metrics[method] = {\n            \"fusion_rate\": fusion_count / len(preds),\n            \"fission_rate\": fission_count / len(preds),\n            \"error_rate\": error_rate,\n            \"api_calls\": api_calls,\n            \"avg_calls_per_example\": api_calls / len(preds),\n        }\n\n    # Compute improvement\n    baseline_calls = metrics[\"baseline\"][\"avg_calls_per_example\"]\n    proposed_calls = metrics[\"proposed\"][\"avg_calls_per_example\"]\n    metrics[\"improvement\"] = {\n        \"api_reduction_pct\": (baseline_calls - proposed_calls) / baseline_calls * 100,\n        \"error_rate_diff\": metrics[\"proposed\"][\"error_rate\"] - metrics[\"baseline\"][\"error_rate\"],\n    }\n\n    return metrics\n\nprint(\"âœ… Metrics computation function defined successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Evaluation Metrics Function\n\nThis function computes various metrics to compare the baseline and proposed methods:\n- **Fusion/Fission Rates**: Percentage of decisions for each operation type\n- **Error Rate**: Percentage of predictions that resulted in errors\n- **API Calls**: Total and average API calls (fusion=1 call, fission=2 calls)\n- **Improvement**: Percentage reduction in API calls and error rate difference",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Sample evaluation results (inlined data)\n# This replaces reading from \"../experiment_001/method_out.json\"\n\n# Generate sample baseline results: all fission, 8% error rate\nbaseline_results = []\nfor i in range(200):\n    baseline_results.append({\n        \"decision\": \"fission\",\n        \"error\": i < 16  # First 16 examples have errors (8% error rate)\n    })\n\n# Generate sample proposed results: 65% fusion, 35% fission, 9% error rate  \nproposed_results = []\nfor i in range(200):\n    if i < 130:  # First 130 examples (65%) use fusion\n        decision = \"fusion\"\n    else:  # Remaining 70 examples (35%) use fission\n        decision = \"fission\"\n    \n    proposed_results.append({\n        \"decision\": decision,\n        \"error\": i < 18  # First 18 examples have errors (9% error rate)\n    })\n\n# Combine into the expected data structure\nresults = {\n    \"baseline\": baseline_results,\n    \"proposed\": proposed_results\n}\n\nprint(f\"Generated sample data:\")\nprint(f\"- Baseline: {len(baseline_results)} predictions\")\nprint(f\"- Proposed: {len(proposed_results)} predictions\")\nprint(f\"- Baseline fusion rate: {sum(1 for r in baseline_results if r['decision'] == 'fusion') / len(baseline_results):.1%}\")\nprint(f\"- Proposed fusion rate: {sum(1 for r in proposed_results if r['decision'] == 'fusion') / len(proposed_results):.1%}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 2. Sample Evaluation Data\n\nInstead of reading from external JSON files, we'll inline the sample data directly into the notebook. \nThis data represents evaluation results from both the baseline and proposed methods on 200 test examples.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import json\nimport numpy as np\nfrom pprint import pprint",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Import Dependencies\n\nLet's start by importing the necessary libraries for our evaluation.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# DKW Controller Evaluation\n\nThis notebook contains an evaluation script for the DKW Controller that compares baseline and proposed methods for decision-making between fusion and fission operations.\n\n## Overview\n- **Baseline Method**: Always chooses fission operations\n- **Proposed Method**: Intelligently chooses between fusion and fission operations\n- **Metrics**: Fusion/fission rates, error rates, API call efficiency",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}