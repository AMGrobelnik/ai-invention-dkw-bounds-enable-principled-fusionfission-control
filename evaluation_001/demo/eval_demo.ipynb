{
 "cells": [
  {
   "cell_type": "code",
   "source": "# Display complete metrics as formatted JSON\nprint(json.dumps(metrics, indent=2))\n\n# Save to file if needed (uncomment below)\n# with open(\"eval_out.json\", \"w\") as f:\n#     json.dump(metrics, f, indent=2)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Complete Metrics Output\n\nThis shows the full metrics output that would normally be saved to `eval_out.json`:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Compute metrics\nmetrics = compute_metrics(results)\n\n# Display key results\nprint(\"=== DKW Controller Evaluation Results ===\")\nprint(f\"\\nAPI reduction: {metrics['improvement']['api_reduction_pct']:.1f}%\")\nprint(f\"Error rate difference: {metrics['improvement']['error_rate_diff']:.3f}\")\n\nprint(f\"\\nBaseline Method:\")\nprint(f\"  Fusion rate: {metrics['baseline']['fusion_rate']:.1%}\")\nprint(f\"  Fission rate: {metrics['baseline']['fission_rate']:.1%}\")\nprint(f\"  Error rate: {metrics['baseline']['error_rate']:.1%}\")\nprint(f\"  Avg API calls per example: {metrics['baseline']['avg_calls_per_example']:.2f}\")\n\nprint(f\"\\nProposed Method:\")\nprint(f\"  Fusion rate: {metrics['proposed']['fusion_rate']:.1%}\")\nprint(f\"  Fission rate: {metrics['proposed']['fission_rate']:.1%}\")\nprint(f\"  Error rate: {metrics['proposed']['error_rate']:.1%}\")\nprint(f\"  Avg API calls per example: {metrics['proposed']['avg_calls_per_example']:.2f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Run Evaluation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def compute_metrics(results: dict) -> dict:\n    \"\"\"Compute evaluation metrics.\"\"\"\n    metrics = {}\n\n    for method in [\"baseline\", \"proposed\"]:\n        preds = results[method]\n\n        # Count decisions\n        fusion_count = sum(1 for p in preds if p[\"decision\"] == \"fusion\")\n        fission_count = sum(1 for p in preds if p[\"decision\"] == \"fission\")\n\n        # Compute error rate\n        errors = sum(1 for p in preds if p[\"error\"])\n        error_rate = errors / len(preds)\n\n        # API calls (fusion=1, fission=2)\n        api_calls = fusion_count + 2 * fission_count\n\n        metrics[method] = {\n            \"fusion_rate\": fusion_count / len(preds),\n            \"fission_rate\": fission_count / len(preds),\n            \"error_rate\": error_rate,\n            \"api_calls\": api_calls,\n            \"avg_calls_per_example\": api_calls / len(preds),\n        }\n\n    # Compute improvement\n    baseline_calls = metrics[\"baseline\"][\"avg_calls_per_example\"]\n    proposed_calls = metrics[\"proposed\"][\"avg_calls_per_example\"]\n    metrics[\"improvement\"] = {\n        \"api_reduction_pct\": (baseline_calls - proposed_calls) / baseline_calls * 100,\n        \"error_rate_diff\": metrics[\"proposed\"][\"error_rate\"] - metrics[\"baseline\"][\"error_rate\"],\n    }\n\n    return metrics",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Metrics Computation\n\nThe `compute_metrics` function analyzes the results and calculates key performance indicators:\n\n1. **Decision rates**: Proportion of fusion vs fission decisions\n2. **Error rate**: Percentage of incorrect predictions \n3. **API efficiency**: Total and average API calls per example\n4. **Improvements**: Comparison between baseline and proposed methods",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Sample experimental results data (normally loaded from method_out.json)\nresults = {\n    \"baseline\": [\n        # 200 examples: all fission decisions, 16 with errors (8% error rate)\n        {\"decision\": \"fission\", \"error\": i < 16} \n        for i in range(200)\n    ],\n    \"proposed\": [\n        # 200 examples: 130 fusion (65%), 70 fission (35%), 18 with errors (9% error rate)  \n        {\"decision\": \"fusion\" if i < 130 else \"fission\", \"error\": i < 18}\n        for i in range(200)\n    ]\n}\n\nprint(f\"Baseline examples: {len(results['baseline'])}\")\nprint(f\"Proposed examples: {len(results['proposed'])}\")\nprint(f\"Baseline errors: {sum(1 for p in results['baseline'] if p['error'])}\")\nprint(f\"Proposed errors: {sum(1 for p in results['proposed'] if p['error'])}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Sample Data\n\nHere we define the experimental results data that would normally be read from `method_out.json`. \n\nThe data contains predictions from both baseline and proposed methods, where each prediction includes:\n- `decision`: Either \"fusion\" (1 API call) or \"fission\" (2 API calls)\n- `error`: Boolean indicating if the prediction was incorrect",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\"\"\"Evaluation script for DKW Controller.\"\"\"\nimport json\nimport numpy as np",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# DKW Controller Evaluation\n\nThis notebook evaluates the performance of a DKW (Decision-based Knowledge Worker) controller by comparing baseline and proposed methods. \n\nThe evaluation computes:\n- **Fusion/Fission rates**: How often each decision type is made\n- **Error rates**: Frequency of incorrect decisions  \n- **API efficiency**: Number of API calls required\n- **Performance improvements**: Reduction in API calls between methods",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}