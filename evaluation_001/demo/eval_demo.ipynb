{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## How to Use with Your Own Data\n\nTo use this notebook with your own evaluation data:\n\n1. **Replace the sample data generation**: Modify the second code cell to load your actual data instead of generating sample data.\n\n2. **Expected data format**: Your data should be a dictionary with this structure:\n   ```python\n   {\n       \"baseline\": [\n           {\"decision\": \"fusion\" or \"fission\", \"error\": True or False},\n           # ... more examples\n       ],\n       \"proposed\": [\n           {\"decision\": \"fusion\" or \"fission\", \"error\": True or False},\n           # ... more examples\n       ]\n   }\n   ```\n\n3. **Loading from files**: If you have JSON files, replace the sample data section with:\n   ```python\n   with open(\"your_results_file.json\") as f:\n       results = json.load(f)\n   ```\n\n4. **Customizing metrics**: Modify the `compute_metrics` function if you need different evaluation metrics or have different cost models for fusion/fission operations.\n\nThe rest of the notebook will automatically work with your data!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\n\n# Create visualizations\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 10))\nfig.suptitle('DKW Controller Evaluation Results', fontsize=16, fontweight='bold')\n\n# 1. Decision Types Comparison\nmethods = ['Baseline', 'Proposed']\nfusion_rates = [metrics['baseline']['fusion_rate'], metrics['proposed']['fusion_rate']]\nfission_rates = [metrics['baseline']['fission_rate'], metrics['proposed']['fission_rate']]\n\nx = range(len(methods))\nwidth = 0.35\n\nax1.bar([i - width/2 for i in x], fusion_rates, width, label='Fusion', color='skyblue')\nax1.bar([i + width/2 for i in x], fission_rates, width, label='Fission', color='lightcoral')\nax1.set_ylabel('Rate')\nax1.set_title('Decision Type Distribution')\nax1.set_xticks(x)\nax1.set_xticklabels(methods)\nax1.legend()\n\n# 2. Error Rates\nerror_rates = [metrics['baseline']['error_rate'], metrics['proposed']['error_rate']]\nax2.bar(methods, error_rates, color=['orange', 'red'], alpha=0.7)\nax2.set_ylabel('Error Rate')\nax2.set_title('Error Rate Comparison')\nax2.set_ylim(0, max(error_rates) * 1.2)\n\n# 3. API Calls per Example\napi_calls = [metrics['baseline']['avg_calls_per_example'], metrics['proposed']['avg_calls_per_example']]\nbars = ax3.bar(methods, api_calls, color=['lightblue', 'lightgreen'])\nax3.set_ylabel('Average API Calls per Example')\nax3.set_title('API Efficiency')\n\n# Add value labels on bars\nfor bar, value in zip(bars, api_calls):\n    height = bar.get_height()\n    ax3.text(bar.get_x() + bar.get_width()/2., height,\n             f'{value:.2f}', ha='center', va='bottom')\n\n# 4. Summary metrics\nsummary_labels = ['API Reduction\\n(%)', 'Error Rate\\nDifference (%)']\nsummary_values = [metrics['improvement']['api_reduction_pct'], \n                 metrics['improvement']['error_rate_diff'] * 100]\ncolors = ['green' if v > 0 else 'red' for v in summary_values]\n\nbars = ax4.bar(summary_labels, summary_values, color=colors, alpha=0.7)\nax4.set_title('Improvement Summary')\nax4.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n\n# Add value labels\nfor bar, value in zip(bars, summary_values):\n    height = bar.get_height()\n    ax4.text(bar.get_x() + bar.get_width()/2., height + (1 if height >= 0 else -1),\n             f'{value:.1f}%', ha='center', va='bottom' if height >= 0 else 'top')\n\nplt.tight_layout()\nplt.show()\n\n# Print key insights\nprint(\"KEY INSIGHTS:\")\nprint(f\"‚úì The proposed method achieves {metrics['improvement']['api_reduction_pct']:.1f}% reduction in API calls\")\nprint(f\"‚úì Fusion rate increased from {metrics['baseline']['fusion_rate']:.0%} to {metrics['proposed']['fusion_rate']:.0%}\")\nprint(f\"‚úì Error rate changed by {metrics['improvement']['error_rate_diff']:.1%} (slight increase)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Visualization\n\nLet's create some plots to visualize the controller's behavior over time.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Usage Notes & Customization\n\n### How to use this notebook:\n1. **Self-contained**: This notebook runs without any external files or dependencies\n2. **Customizable**: Modify the `simulated_dataset` to test with your own questions\n3. **Extensible**: Add new fields to the output format by modifying the `collect_data()` function\n\n### Original vs Notebook differences:\n- **Original**: Loads data from HuggingFace datasets library\n- **Notebook**: Uses inline sample data for demonstration\n- **Original**: Saves output to `data_out.json` file  \n- **Notebook**: Displays output directly in cells\n\n### To restore original functionality:\n1. Install dependencies: `pip install datasets`\n2. Uncomment the HuggingFace dataset loading code\n3. Add file writing functionality back if needed",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Analyze results\nbaseline_errors = sum(1 for r in results[\"baseline\"] if r[\"error\"])\nproposed_errors = sum(1 for r in results[\"proposed\"] if r[\"error\"])\n\nbaseline_error_rate = baseline_errors / len(results[\"baseline\"])\nproposed_error_rate = proposed_errors / len(results[\"proposed\"])\n\n# Count fusion vs fission decisions for proposed method\nfusion_count = sum(1 for r in results[\"proposed\"] if r[\"decision\"] == \"fusion\")\nfission_count = sum(1 for r in results[\"proposed\"] if r[\"decision\"] == \"fission\")\n\nprint(\"üìà EXPERIMENT RESULTS\")\nprint(\"=\" * 50)\nprint(f\"Total examples processed: {len(results['baseline'])}\")\nprint()\nprint(\"üî∏ BASELINE (always fission):\")\nprint(f\"   Error rate: {baseline_error_rate:.1%} ({baseline_errors}/{len(results['baseline'])})\")\nprint(f\"   Fusion decisions: 0/{len(results['baseline'])} (0%)\")\nprint()\nprint(\"üîπ PROPOSED (DKW controller):\")\nprint(f\"   Error rate: {proposed_error_rate:.1%} ({proposed_errors}/{len(results['proposed'])})\")\nprint(f\"   Fusion decisions: {fusion_count}/{len(results['proposed'])} ({100*fusion_count/len(results['proposed']):.1f}%)\")\nprint(f\"   Fission decisions: {fission_count}/{len(results['proposed'])} ({100*fission_count/len(results['proposed']):.1f}%)\")\nprint()\nprint(\"üîÑ DECISION CHANGES:\")\nprint(f\"   Mode switches: {len(decision_changes)}\")\nfor change in decision_changes:\n    print(f\"   Step {change['step']}: {change['from']} ‚Üí {change['to']} (upper bound: {change['stats']['upper_bound']:.3f})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Visualization\n\nCreate visualizations to better understand the performance comparison between baseline and proposed methods.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Expected output format (from original data_out.json)\nexpected_output = [\n    {\n        \"id\": \"example_000\",\n        \"question\": \"What is 2+2?\",\n        \"answer\": \"4\",\n        \"difficulty\": 0.15\n    },\n    {\n        \"id\": \"example_001\", \n        \"question\": \"If x=5, what is 2x?\",\n        \"answer\": \"10\",\n        \"difficulty\": 0.22\n    },\n    {\n        \"id\": \"example_002\",\n        \"question\": \"Solve: 3y + 6 = 15\",\n        \"answer\": \"y=3\",\n        \"difficulty\": 0.28\n    }\n]\n\nprint(\"Expected output format:\")\nprint(json.dumps(expected_output, indent=2))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Results Analysis\n\nLet's analyze the performance of our DKW controller compared to the baseline approach.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Save results to JSON file (equivalent to the original script)\nwith open(\"eval_out.json\", \"w\") as f:\n    json.dump(metrics, f, indent=2)\n\nprint(\"Results saved to eval_out.json\")\n\n# Display the JSON content for verification\nprint(\"\\nSaved JSON content:\")\nprint(json.dumps(metrics, indent=2))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def run_experiment(data, verbose=False):\n    \"\"\"Run DKW controller experiment with inline data.\"\"\"\n    controller = DKWController()\n    results = {\"baseline\": [], \"proposed\": []}\n    \n    if verbose:\n        print(\"üöÄ Starting experiment...\")\n        print(f\"Controller settings: target={controller.epsilon_target}, min_samples={controller.min_samples}\")\n    \n    decision_changes = []\n    \n    for i, example in enumerate(data):\n        # Simulate error occurrence based on difficulty\n        error = np.random.random() < example[\"difficulty\"]\n        controller.add_observation(float(error))\n        decision = controller.decide()\n        \n        # Track decision changes for analysis\n        if i > 0 and decision != results[\"proposed\"][-1][\"decision\"]:\n            stats = controller.get_stats()\n            decision_changes.append({\n                \"step\": i,\n                \"from\": results[\"proposed\"][-1][\"decision\"],\n                \"to\": decision,\n                \"stats\": stats\n            })\n\n        results[\"proposed\"].append({\n            \"id\": example[\"id\"],\n            \"decision\": decision,\n            \"error\": error,\n            \"difficulty\": example[\"difficulty\"]\n        })\n        results[\"baseline\"].append({\n            \"id\": example[\"id\"],\n            \"decision\": \"fission\",  # Always conservative\n            \"error\": error,\n            \"difficulty\": example[\"difficulty\"]\n        })\n        \n        if verbose and i % 50 == 0:\n            stats = controller.get_stats()\n            print(f\"  Step {i}: {stats['samples']} samples, error rate: {stats['empirical_error']:.3f}, mode: {stats['current_state']}\")\n\n    if verbose:\n        print(f\"‚úÖ Experiment complete! Decision changes: {len(decision_changes)}\")\n    \n    return results, decision_changes\n\n# Run the experiment\nprint(\"üî¨ Running experiment...\")\nresults, decision_changes = run_experiment(sample_data, verbose=True)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Expected Output Reference\n\nFor comparison, here's the expected output structure that was provided in the original specification:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Save Results\n\nSave the evaluation metrics to a JSON file (as in the original script).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Display the complete processed dataset\nprint(\"Complete processed dataset:\")\nprint(json.dumps(data, indent=2))\n\n# In the original script, this would be saved to a file:\n# with open(\"data_out.json\", \"w\") as f:\n#     json.dump(data, f, indent=2)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Compute the evaluation metrics\nmetrics = compute_metrics(results)\n\n# Display the key result (as in the original script)\nprint(f\"API reduction: {metrics['improvement']['api_reduction_pct']:.1f}%\")\nprint()\n\n# Display detailed metrics in a nice format\nprint(\"Detailed Evaluation Results:\")\nprint(\"=\" * 50)\n\nfor method in [\"baseline\", \"proposed\"]:\n    print(f\"\\n{method.upper()} METHOD:\")\n    m = metrics[method]\n    print(f\"  Fusion rate:           {m['fusion_rate']:.1%}\")\n    print(f\"  Fission rate:          {m['fission_rate']:.1%}\")\n    print(f\"  Error rate:            {m['error_rate']:.1%}\")\n    print(f\"  Total API calls:       {m['api_calls']}\")\n    print(f\"  Avg calls per example: {m['avg_calls_per_example']:.2f}\")\n\nprint(f\"\\nIMPROVEMENT:\")\nimp = metrics['improvement']\nprint(f\"  API reduction:         {imp['api_reduction_pct']:.1f}%\")\nprint(f\"  Error rate difference: {imp['error_rate_diff']:+.1%}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## View Complete Dataset\n\nLet's examine the complete processed dataset structure that would normally be saved to `data_out.json`.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Experiment Function\n\nThe experiment simulates running both the DKW controller (proposed method) and a baseline that always uses fission mode. Errors occur probabilistically based on each example's difficulty level.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Execute the data collection\ndata = collect_data()\n\nprint(f\"Collected {len(data)} examples\")\nprint(\"\\nFirst few examples:\")\nfor item in data[:3]:\n    print(f\"- ID: {item['id']}\")\n    print(f\"  Question: {item['question']}\")\n    print(f\"  Answer: {item['answer']}\")\n    print(f\"  Difficulty: {item['difficulty']:.2f}\")\n    print()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Compute Metrics\n\nRun the evaluation on our sample data and display the results.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Sample dataset - inlined instead of reading from file\n# This replaces the original: with open(\"../dataset_001/data_out.json\") as f: data = json.load(f)\n\nsample_data = [\n    {\"id\": f\"example_{i:03d}\", \"difficulty\": 0.05 + 0.15 * np.random.random()}\n    for i in range(200)\n]\n\n# Add some high-difficulty examples to test mode switching\nfor i in range(50):\n    sample_data.append({\n        \"id\": f\"hard_example_{i:03d}\", \n        \"difficulty\": 0.3 + 0.4 * np.random.random()\n    })\n\nprint(f\"üìä Created {len(sample_data)} sample examples\")\nprint(f\"üìà Difficulty range: {min(ex['difficulty'] for ex in sample_data):.3f} - {max(ex['difficulty'] for ex in sample_data):.3f}\")\n\n# Show first few examples\nprint(\"\\nüîç First 5 examples:\")\nfor i, ex in enumerate(sample_data[:5]):\n    print(f\"  {ex['id']}: difficulty = {ex['difficulty']:.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Execute Data Collection\n\nLet's run the data collection function and examine the results.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def compute_metrics(results: dict) -> dict:\n    \"\"\"Compute evaluation metrics.\"\"\"\n    metrics = {}\n\n    for method in [\"baseline\", \"proposed\"]:\n        preds = results[method]\n\n        # Count decisions\n        fusion_count = sum(1 for p in preds if p[\"decision\"] == \"fusion\")\n        fission_count = sum(1 for p in preds if p[\"decision\"] == \"fission\")\n\n        # Compute error rate\n        errors = sum(1 for p in preds if p[\"error\"])\n        error_rate = errors / len(preds)\n\n        # API calls (fusion=1, fission=2)\n        api_calls = fusion_count + 2 * fission_count\n\n        metrics[method] = {\n            \"fusion_rate\": fusion_count / len(preds),\n            \"fission_rate\": fission_count / len(preds),\n            \"error_rate\": error_rate,\n            \"api_calls\": api_calls,\n            \"avg_calls_per_example\": api_calls / len(preds),\n        }\n\n    # Compute improvement\n    baseline_calls = metrics[\"baseline\"][\"avg_calls_per_example\"]\n    proposed_calls = metrics[\"proposed\"][\"avg_calls_per_example\"]\n    metrics[\"improvement\"] = {\n        \"api_reduction_pct\": (baseline_calls - proposed_calls) / baseline_calls * 100,\n        \"error_rate_diff\": metrics[\"proposed\"][\"error_rate\"] - metrics[\"baseline\"][\"error_rate\"],\n    }\n\n    return metrics\n\nprint(\"Evaluation function defined successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def collect_data() -> List[Dict[str, Any]]:\n    \"\"\"Collect benchmark data for DKW controller evaluation.\"\"\"\n    \n    # In the original script, this would be:\n    # ds = load_dataset(\"gsm8k\", \"main\", split=\"test[:200]\")\n    # Here we use our simulated dataset instead\n    ds = simulated_dataset\n\n    data = []\n    for i, example in enumerate(ds):\n        processed_item = {\n            \"id\": f\"example_{i:03d}\",\n            \"question\": example[\"question\"],\n            \"answer\": example[\"answer\"],\n            \"difficulty\": len(example[\"question\"]) / 100,  # Simple proxy for difficulty\n        }\n        data.append(processed_item)\n\n    return data",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Sample Data\n\nInstead of reading from external files, we'll create sample data inline. The data represents examples with varying difficulty levels that influence error probability.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Evaluation Function\n\nThe `compute_metrics` function calculates performance metrics for both baseline and proposed methods, including fusion/fission rates, error rates, API call counts, and improvement percentages.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Data Processing Function\n\nThe `collect_data()` function processes the raw dataset and adds additional metadata like difficulty scoring based on question length.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "@dataclass\nclass DKWController:\n    \"\"\"DKW-guided fusion/fission controller.\"\"\"\n    epsilon_target: float = 0.10\n    delta: float = 0.05\n    min_samples: int = 100\n    hysteresis: float = 0.05\n\n    samples: list = field(default_factory=list)\n    current_state: str = \"fission\"\n\n    def dkw_epsilon(self, n: int) -> float:\n        \"\"\"Compute DKW epsilon for n samples.\"\"\"\n        if n < 2:\n            return 1.0\n        return np.sqrt(np.log(2 / self.delta) / (2 * n))\n\n    def add_observation(self, error: float) -> None:\n        \"\"\"Add error observation for calibration.\"\"\"\n        self.samples.append(error)\n\n    def decide(self) -> str:\n        \"\"\"Make fusion/fission decision with DKW guarantee.\"\"\"\n        n = len(self.samples)\n        if n < self.min_samples:\n            return self.current_state\n\n        epsilon = self.dkw_epsilon(n)\n        empirical_error = np.mean(self.samples[-self.min_samples:])\n        error_upper_bound = empirical_error + epsilon\n\n        if self.current_state == \"fusion\":\n            if error_upper_bound > self.epsilon_target + self.hysteresis:\n                self.current_state = \"fission\"\n        else:\n            if error_upper_bound < self.epsilon_target - self.hysteresis:\n                self.current_state = \"fusion\"\n\n        return self.current_state\n    \n    def get_stats(self):\n        \"\"\"Get current controller statistics.\"\"\"\n        n = len(self.samples)\n        if n == 0:\n            return {\"samples\": 0, \"empirical_error\": 0, \"epsilon\": 1.0, \"upper_bound\": 1.0}\n        \n        empirical_error = np.mean(self.samples[-self.min_samples:]) if n >= self.min_samples else np.mean(self.samples)\n        epsilon = self.dkw_epsilon(n)\n        \n        return {\n            \"samples\": n,\n            \"empirical_error\": empirical_error,\n            \"epsilon\": epsilon,\n            \"upper_bound\": empirical_error + epsilon,\n            \"current_state\": self.current_state\n        }\n\nprint(\"‚úÖ DKWController class defined!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Simulated GSM8K dataset samples (normally loaded from HuggingFace)\nsimulated_dataset = [\n    {\n        \"question\": \"What is 2+2?\",\n        \"answer\": \"4\"\n    },\n    {\n        \"question\": \"If x=5, what is 2x?\", \n        \"answer\": \"10\"\n    },\n    {\n        \"question\": \"Solve: 3y + 6 = 15\",\n        \"answer\": \"y=3\"\n    },\n    {\n        \"question\": \"A store sells apples for $3 per pound. How much do 4 pounds cost?\",\n        \"answer\": \"$12\"\n    },\n    {\n        \"question\": \"If a rectangle has length 8 and width 6, what is its area?\",\n        \"answer\": \"48\"\n    }\n]\n\nprint(f\"Loaded {len(simulated_dataset)} sample questions\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import json\nimport numpy as np\n\n# Sample data that produces the expected evaluation metrics\n# This replaces reading from \"../experiment_001/method_out.json\"\n\n# Generate sample baseline data: 100% fission, 8% error rate\nbaseline_data = []\nfor i in range(200):\n    error = i < 16  # First 16 examples have errors (8% error rate)\n    baseline_data.append({\n        \"decision\": \"fission\",  # 100% fission rate\n        \"error\": error\n    })\n\n# Generate sample proposed data: 65% fusion, 35% fission, 9% error rate\nproposed_data = []\nfor i in range(200):\n    decision = \"fusion\" if i < 130 else \"fission\"  # 65% fusion, 35% fission\n    error = i < 18  # First 18 examples have errors (9% error rate)\n    proposed_data.append({\n        \"decision\": decision,\n        \"error\": error\n    })\n\n# Combine into the expected format\nresults = {\n    \"baseline\": baseline_data,\n    \"proposed\": proposed_data\n}\n\nprint(f\"Generated sample data:\")\nprint(f\"- Baseline: {len(results['baseline'])} examples\")\nprint(f\"- Proposed: {len(results['proposed'])} examples\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## DKW Controller Class\n\nThe `DKWController` uses the Dvoretzky-Kiefer-Wolfowitz inequality to provide statistical guarantees on error rate estimates. \n\n### Key Parameters:\n- **`epsilon_target`**: Target error rate threshold (default: 0.10)\n- **`delta`**: Confidence level parameter for DKW bound (default: 0.05)  \n- **`min_samples`**: Minimum samples before making decisions (default: 100)\n- **`hysteresis`**: Prevents rapid mode switching (default: 0.05)\n\n### DKW Inequality:\nFor n samples, the true error rate is within `empirical_error ¬± epsilon` with probability ‚â• 1-Œ¥, where:\n```\nepsilon = sqrt(log(2/Œ¥) / (2*n))\n```",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Sample Dataset\n\nSince this is a self-contained notebook, we'll simulate the GSM8K dataset with sample data instead of loading from HuggingFace. In the original script, this would be loaded using `load_dataset(\"gsm8k\", \"main\", split=\"test[:200]\")`.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Sample Data\n\nThe original script reads from `../experiment_001/method_out.json`. For this self-contained notebook, we'll inline the sample data that would produce the expected evaluation results.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\"\"\"Dataset collection script for DKW benchmark.\"\"\"\nimport json\nfrom typing import List, Dict, Any\n\n# Note: In the original script, this would be: from datasets import load_dataset\n# For this self-contained notebook, we'll use inline data instead",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "\"\"\"DKW Controller Implementation - Imports and Setup\"\"\"\nimport json\nimport numpy as np\nfrom dataclasses import dataclass, field\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\nprint(\"üì¶ All packages imported successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# DKW Controller Evaluation\n\nThis notebook contains the evaluation script for the DKW Controller, converted from `eval.py` into an interactive format. The notebook analyzes the performance of two methods (baseline and proposed) by computing various metrics including API call reduction and error rates.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Import Dependencies\n\nFirst, let's import the required libraries for data processing.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# DKW Controller Implementation - Interactive Demo\n\nThis notebook demonstrates a **DKW-guided fusion/fission controller** implementation. The controller uses the Dvoretzky-Kiefer-Wolfowitz (DKW) inequality to make statistically guaranteed decisions between fusion and fission modes based on observed error rates.\n\n## Overview\n- **Fusion mode**: Aggressive strategy that may have higher error rates but better performance\n- **Fission mode**: Conservative strategy with lower error rates but potentially reduced performance\n- **DKW guarantee**: Statistical bound ensuring our error estimates are reliable\n\nThe controller switches between modes based on observed error rates with statistical confidence bounds.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# Dataset Collection for DKW Benchmark\n\nThis notebook demonstrates the dataset collection script for DKW controller evaluation. The script processes benchmark data from the GSM8K dataset and formats it for evaluation purposes.\n\n**Original Artifact:** data.py  \n**Purpose:** Collect and format benchmark data for mathematical reasoning tasks",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Interactive parameter exploration\n# Modify these parameters to see different scenarios\n\ndef create_custom_scenario(n_examples=200, \n                          proposed_fusion_rate=0.65, \n                          baseline_error_rate=0.08,\n                          proposed_error_rate=0.09):\n    \"\"\"Create a custom evaluation scenario.\"\"\"\n    \n    # Baseline: always fission\n    baseline_data = []\n    for i in range(n_examples):\n        error = i < int(n_examples * baseline_error_rate)\n        baseline_data.append({\n            \"decision\": \"fission\",\n            \"error\": error\n        })\n    \n    # Proposed: mix of fusion and fission  \n    proposed_data = []\n    fusion_count = int(n_examples * proposed_fusion_rate)\n    for i in range(n_examples):\n        decision = \"fusion\" if i < fusion_count else \"fission\"\n        error = i < int(n_examples * proposed_error_rate)\n        proposed_data.append({\n            \"decision\": decision,\n            \"error\": error\n        })\n    \n    custom_results = {\n        \"baseline\": baseline_data,\n        \"proposed\": proposed_data\n    }\n    \n    return compute_metrics(custom_results)\n\n# Try different scenarios\nprint(\"=== SCENARIO 1: Higher Fusion Rate ===\")\nscenario1 = create_custom_scenario(proposed_fusion_rate=0.80)\nprint(f\"API Reduction: {scenario1['improvement']['api_reduction_pct']:.1f}%\")\n\nprint(\"\\n=== SCENARIO 2: Lower Error Rate ===\")  \nscenario2 = create_custom_scenario(proposed_error_rate=0.05)\nprint(f\"API Reduction: {scenario2['improvement']['api_reduction_pct']:.1f}%\")\nprint(f\"Error Rate Diff: {scenario2['improvement']['error_rate_diff']:+.1%}\")\n\nprint(\"\\n=== SCENARIO 3: Conservative Approach ===\")\nscenario3 = create_custom_scenario(proposed_fusion_rate=0.40, proposed_error_rate=0.06)\nprint(f\"API Reduction: {scenario3['improvement']['api_reduction_pct']:.1f}%\")\nprint(f\"Error Rate Diff: {scenario3['improvement']['error_rate_diff']:+.1%}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Customization & Usage\n\n### Modifying the Sample Data\nYou can easily modify the `sample_dataset` variable above to include your own questions and answers. Just maintain the format:\n\n```python\nsample_dataset = [\n    {\n        \"question\": \"Your question here\",\n        \"answer\": \"Your answer here\"\n    }\n    # Add more examples...\n]\n```\n\n### Using Real HuggingFace Data\nTo use the actual GSM8k dataset from HuggingFace:\n\n1. Install the datasets library: `pip install datasets`\n2. Uncomment the import: `from datasets import load_dataset`\n3. Use the `collect_data_from_huggingface()` function\n\n### Difficulty Metric\nThe difficulty score is calculated as `question_length / 100`. You can modify this calculation in the `collect_data()` function to use more sophisticated metrics.\n\n### Next Steps\nThis processed data can now be used for:\n- DKW benchmark evaluation\n- Mathematical reasoning model testing\n- Performance analysis and comparison",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Interactive Exploration\n\nTry modifying the parameters below to see how different scenarios affect the results.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Display the formatted data (equivalent to what would be saved in data_out.json)\nformatted_output = json.dumps(collected_data, indent=2)\nprint(formatted_output)\n\nprint(f\"\\nüíæ In the original script, this data would be saved to 'data_out.json'\")\nprint(f\"üéØ The data is now ready for DKW benchmark evaluation!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Display detailed metrics in a formatted way\nimport json\n\nprint(\"=== BASELINE METHOD ===\")\nbaseline = metrics[\"baseline\"]\nprint(f\"Fusion Rate:     {baseline['fusion_rate']:.1%}\")\nprint(f\"Fission Rate:    {baseline['fission_rate']:.1%}\")\nprint(f\"Error Rate:      {baseline['error_rate']:.1%}\")\nprint(f\"Total API Calls: {baseline['api_calls']}\")\nprint(f\"Avg Calls/Example: {baseline['avg_calls_per_example']:.2f}\")\n\nprint(\"\\n=== PROPOSED METHOD ===\")\nproposed = metrics[\"proposed\"]\nprint(f\"Fusion Rate:     {proposed['fusion_rate']:.1%}\")\nprint(f\"Fission Rate:    {proposed['fission_rate']:.1%}\")\nprint(f\"Error Rate:      {proposed['error_rate']:.1%}\")\nprint(f\"Total API Calls: {proposed['api_calls']}\")\nprint(f\"Avg Calls/Example: {proposed['avg_calls_per_example']:.2f}\")\n\nprint(\"\\n=== IMPROVEMENT ===\")\nimprovement = metrics[\"improvement\"]\nprint(f\"API Reduction:   {improvement['api_reduction_pct']:.1f}%\")\nprint(f\"Error Rate Diff: {improvement['error_rate_diff']:+.1%}\")\n\nprint(\"\\n=== COMPLETE METRICS (JSON) ===\")\nprint(json.dumps(metrics, indent=2))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Execute the data collection\nprint(\"üöÄ Starting data collection process...\\n\")\n\n# Collect and process the data\ncollected_data = collect_data()\n\n# Display the results\nprint(f\"\\nüìà Collection Summary:\")\nprint(f\"   ‚Ä¢ Total examples: {len(collected_data)}\")\nprint(f\"   ‚Ä¢ Average difficulty: {sum(item['difficulty'] for item in collected_data) / len(collected_data):.3f}\")\n\n# Instead of writing to file, we'll display the data inline\nprint(f\"\\nüìã Collected Data Structure:\")\nprint(\"-\" * 50)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Detailed Results\n\nLet's examine the detailed metrics for both methods.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Execute Data Collection\n\nNow let's run the data collection process and see the results. The function will process our sample data and format it for benchmark evaluation.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Compute metrics using our sample data\nmetrics = compute_metrics(results)\n\n# Display the main result\nprint(f\"API reduction: {metrics['improvement']['api_reduction_pct']:.1f}%\")\n\n# Save results (optional - replaces writing to JSON file)\neval_output = metrics\nprint(\"\\nEvaluation completed successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def collect_data(dataset=None):\n    \"\"\"Collect benchmark data for DKW controller evaluation.\"\"\"\n    \n    # Use inline sample data if no dataset provided (self-contained mode)\n    if dataset is None:\n        dataset = sample_dataset\n        print(\"üîÑ Using inline sample data for self-contained execution\")\n    \n    # Process the dataset\n    data = []\n    for i, example in enumerate(dataset):\n        data.append({\n            \"id\": f\"example_{i:03d}\",\n            \"question\": example[\"question\"],\n            \"answer\": example[\"answer\"],\n            \"difficulty\": len(example[\"question\"]) / 100,  # Simple proxy based on question length\n        })\n    \n    print(f\"‚úÖ Processed {len(data)} examples successfully\")\n    return data\n\n# Alternative function that would work with HuggingFace datasets (commented for reference)\ndef collect_data_from_huggingface():\n    \"\"\"\n    Original function that loads from HuggingFace (requires 'datasets' package):\n    \n    from datasets import load_dataset\n    ds = load_dataset(\"gsm8k\", \"main\", split=\"test[:200]\")\n    return collect_data(ds)\n    \"\"\"\n    pass\n\nprint(\"üîß Data collection functions defined successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Run Evaluation\n\nNow let's compute the metrics and display the results.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def compute_metrics(results: dict) -> dict:\n    \"\"\"Compute evaluation metrics.\"\"\"\n    metrics = {}\n\n    for method in [\"baseline\", \"proposed\"]:\n        preds = results[method]\n\n        # Count decisions\n        fusion_count = sum(1 for p in preds if p[\"decision\"] == \"fusion\")\n        fission_count = sum(1 for p in preds if p[\"decision\"] == \"fission\")\n\n        # Compute error rate\n        errors = sum(1 for p in preds if p[\"error\"])\n        error_rate = errors / len(preds)\n\n        # API calls (fusion=1, fission=2)\n        api_calls = fusion_count + 2 * fission_count\n\n        metrics[method] = {\n            \"fusion_rate\": fusion_count / len(preds),\n            \"fission_rate\": fission_count / len(preds),\n            \"error_rate\": error_rate,\n            \"api_calls\": api_calls,\n            \"avg_calls_per_example\": api_calls / len(preds),\n        }\n\n    # Compute improvement\n    baseline_calls = metrics[\"baseline\"][\"avg_calls_per_example\"]\n    proposed_calls = metrics[\"proposed\"][\"avg_calls_per_example\"]\n    metrics[\"improvement\"] = {\n        \"api_reduction_pct\": (baseline_calls - proposed_calls) / baseline_calls * 100,\n        \"error_rate_diff\": metrics[\"proposed\"][\"error_rate\"] - metrics[\"baseline\"][\"error_rate\"],\n    }\n\n    return metrics",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Data Collection Function\n\nThe `collect_data()` function processes the raw dataset and formats it for DKW benchmark evaluation. It:\n\n1. Takes mathematical questions and answers\n2. Assigns unique IDs to each example\n3. Calculates a difficulty metric based on question length\n4. Returns structured data ready for benchmark testing",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Sample data that mimics HuggingFace GSM8k dataset format\n# This represents what would be loaded from: load_dataset(\"gsm8k\", \"main\", split=\"test[:200]\")\nsample_dataset = [\n    {\n        \"question\": \"What is 2+2?\",\n        \"answer\": \"4\"\n    },\n    {\n        \"question\": \"If x=5, what is 2x?\", \n        \"answer\": \"10\"\n    },\n    {\n        \"question\": \"Solve: 3y + 6 = 15\",\n        \"answer\": \"y=3\"\n    }\n]\n\nprint(f\"üìä Sample dataset loaded with {len(sample_dataset)} examples\")\nprint(\"\\nüîç Preview of first example:\")\nprint(f\"Question: {sample_dataset[0]['question']}\")\nprint(f\"Answer: {sample_dataset[0]['answer']}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Metrics Computation Function\n\nThis function analyzes the results and computes key performance metrics for both methods.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create sample data that matches the expected evaluation results\n# 200 examples total for each method\n\n# Baseline: 100% fission, 8% error rate  \nbaseline_data = []\nfor i in range(200):\n    error = i < 16  # First 16 examples have errors (8% error rate)\n    baseline_data.append({\n        \"decision\": \"fission\",\n        \"error\": error\n    })\n\n# Proposed: 65% fusion, 35% fission, 9% error rate\nproposed_data = []\nfor i in range(200):\n    if i < 130:  # First 130 examples use fusion (65%)\n        decision = \"fusion\"\n    else:  # Last 70 examples use fission (35%)\n        decision = \"fission\"\n    \n    error = i < 18  # First 18 examples have errors (9% error rate)\n    proposed_data.append({\n        \"decision\": decision,\n        \"error\": error\n    })\n\n# Combined results dictionary (replaces reading from JSON file)\nresults = {\n    \"baseline\": baseline_data,\n    \"proposed\": proposed_data\n}\n\nprint(f\"Created sample data:\")\nprint(f\"- Baseline: {len(results['baseline'])} examples\")\nprint(f\"- Proposed: {len(results['proposed'])} examples\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Sample Data (Inline)\n\nFor self-contained execution, we'll use sample data that represents what would normally be loaded from the HuggingFace GSM8k dataset. This data includes mathematical reasoning questions with their answers.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Sample Data\n\nInstead of reading from external JSON files, we'll define the sample data inline. This represents the results from 200 test examples for both baseline and proposed methods.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\"\"\"Dataset collection script for DKW benchmark.\"\"\"\nimport json\n# Note: In a real environment, you would need: pip install datasets\n# from datasets import load_dataset\n\n# For this self-contained demo, we'll use inline sample data\nprint(\"‚úÖ Imports loaded successfully!\")\nprint(\"üìù Note: This notebook uses inline sample data for self-contained execution\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# DKW Benchmark Dataset Collection\n\n**Artifact:** dataset_001 - data.py\n\nThis notebook demonstrates the dataset collection script for DKW benchmark evaluation. It processes mathematical reasoning questions from the GSM8k dataset and formats them for benchmark testing.\n\n## Features\n- Loads data from HuggingFace GSM8k dataset\n- Processes and formats questions with answers\n- Calculates difficulty metrics\n- Self-contained execution with sample data",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\"\"\"Evaluation script for DKW Controller.\"\"\"\nimport json\nimport numpy as np",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# DKW Controller Evaluation\n\nThis notebook evaluates the performance of a proposed method against a baseline for the DKW Controller system. \n\nThe evaluation compares two approaches:\n- **Baseline**: Always uses fission (2 API calls per example)\n- **Proposed**: Intelligently chooses between fusion (1 API call) and fission (2 API calls)\n\nKey metrics computed:\n- Fusion/Fission rates\n- Error rates \n- API call efficiency\n- Performance improvement",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}