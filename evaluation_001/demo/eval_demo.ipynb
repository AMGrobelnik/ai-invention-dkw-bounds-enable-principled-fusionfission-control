{
 "cells": [
  {
   "cell_type": "code",
   "source": "# Example: Create a custom dataset for experimentation\ndef create_custom_dataset(num_examples=100, fusion_rate=0.5, error_rate=0.05):\n    \"\"\"Create a custom dataset with specified characteristics.\n    \n    Args:\n        num_examples: Number of examples to generate\n        fusion_rate: Proportion of fusion decisions (rest will be fission)\n        error_rate: Proportion of examples with errors\n    \"\"\"\n    predictions = []\n    fusion_count = int(num_examples * fusion_rate)\n    error_count = int(num_examples * error_rate)\n    \n    for i in range(num_examples):\n        decision = \"fusion\" if i < fusion_count else \"fission\"\n        has_error = i < error_count\n        predictions.append({\n            \"decision\": decision,\n            \"error\": has_error,\n            \"example_id\": i\n        })\n    \n    return predictions\n\n# Uncomment and modify these lines to test your own scenarios:\n# custom_results = {\n#     \"baseline\": create_custom_dataset(num_examples=100, fusion_rate=0.2, error_rate=0.1),\n#     \"proposed\": create_custom_dataset(num_examples=100, fusion_rate=0.8, error_rate=0.05)\n# }\n# custom_metrics = compute_metrics(custom_results)\n# print(f\"Custom API reduction: {custom_metrics['improvement']['api_reduction_pct']:.1f}%\")\n\nprint(\"Customization example ready! Uncomment the lines above to test your own scenarios.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Customization and Experimentation\n\nYou can easily modify the data to test different scenarios. Here's an example of how to create your own dataset:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Print detailed comparison\nprint(\"=== DETAILED EVALUATION RESULTS ===\\n\")\n\nfor method in [\"baseline\", \"proposed\"]:\n    m = metrics[method]\n    print(f\"{method.upper()} METHOD:\")\n    print(f\"  Total Examples: {m['total_examples']}\")\n    print(f\"  Fusion Decisions: {m['fusion_count']} ({m['fusion_rate']:.1%})\")\n    print(f\"  Fission Decisions: {m['fission_count']} ({m['fission_rate']:.1%})\")\n    print(f\"  Error Count: {m['error_count']} ({m['error_rate']:.1%})\")\n    print(f\"  Total API Calls: {m['api_calls']}\")\n    print(f\"  Avg Calls/Example: {m['avg_calls_per_example']:.2f}\")\n    print()\n\nprint(\"=== IMPROVEMENTS ===\")\nimprovement = metrics[\"improvement\"]\nprint(f\"API Call Reduction: {improvement['api_reduction_pct']:.1f}%\")\nprint(f\"Error Rate Change: {improvement['error_rate_diff']:+.1%}\")\n\n# Calculate additional insights\ncall_savings = metrics[\"baseline\"][\"api_calls\"] - metrics[\"proposed\"][\"api_calls\"]\nefficiency_gain = (metrics[\"baseline\"][\"avg_calls_per_example\"] - metrics[\"proposed\"][\"avg_calls_per_example\"]) / metrics[\"baseline\"][\"avg_calls_per_example\"]\n\nprint(f\"Absolute Call Savings: {call_savings} API calls\")\nprint(f\"Efficiency Gain: {efficiency_gain:.1%}\")\n\n# Quick validation\nprint(f\"\\n=== VALIDATION ===\")\nprint(f\"Expected API reduction: 32.5% | Actual: {improvement['api_reduction_pct']:.1f}%\")\nprint(f\"Expected error diff: +0.01 | Actual: {improvement['error_rate_diff']:+.2f}\")\nprint(\"✓ Results match expected values!\" if abs(improvement['api_reduction_pct'] - 32.5) < 0.1 else \"⚠ Results don't match expected values\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Detailed Results Analysis\n\nLet's examine the detailed metrics and visualize the comparison between methods:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Compute metrics\nmetrics = compute_metrics(results)\n\n# Display the main result (matching original script output)\nprint(f\"API reduction: {metrics['improvement']['api_reduction_pct']:.1f}%\")\n\n# Also save to JSON file for compatibility (replacing original file write)\noutput_file = \"eval_out.json\"\nwith open(output_file, \"w\") as f:\n    # Create a simplified version matching the original output format\n    simplified_metrics = {\n        \"baseline\": {\n            \"fusion_rate\": metrics[\"baseline\"][\"fusion_rate\"],\n            \"fission_rate\": metrics[\"baseline\"][\"fission_rate\"],\n            \"error_rate\": metrics[\"baseline\"][\"error_rate\"],\n            \"api_calls\": metrics[\"baseline\"][\"api_calls\"],\n            \"avg_calls_per_example\": metrics[\"baseline\"][\"avg_calls_per_example\"]\n        },\n        \"proposed\": {\n            \"fusion_rate\": metrics[\"proposed\"][\"fusion_rate\"],\n            \"fission_rate\": metrics[\"proposed\"][\"fission_rate\"],\n            \"error_rate\": metrics[\"proposed\"][\"error_rate\"],\n            \"api_calls\": metrics[\"proposed\"][\"api_calls\"],\n            \"avg_calls_per_example\": metrics[\"proposed\"][\"avg_calls_per_example\"]\n        },\n        \"improvement\": metrics[\"improvement\"]\n    }\n    json.dump(simplified_metrics, f, indent=2)\n\nprint(f\"Results saved to {output_file}\")\nprint(\"\\nEvaluation completed successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Execute Evaluation\n\nNow let's run the evaluation and see the results!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def compute_metrics(results: dict) -> dict:\n    \"\"\"Compute evaluation metrics for both baseline and proposed methods.\n    \n    Args:\n        results: Dictionary containing 'baseline' and 'proposed' keys,\n                each with a list of prediction dictionaries containing \n                'decision' and 'error' fields.\n    \n    Returns:\n        Dictionary containing metrics for both methods plus improvement calculations.\n    \"\"\"\n    metrics = {}\n\n    for method in [\"baseline\", \"proposed\"]:\n        preds = results[method]\n\n        # Count decisions\n        fusion_count = sum(1 for p in preds if p[\"decision\"] == \"fusion\")\n        fission_count = sum(1 for p in preds if p[\"decision\"] == \"fission\")\n\n        # Compute error rate\n        errors = sum(1 for p in preds if p[\"error\"])\n        error_rate = errors / len(preds)\n\n        # API calls (fusion=1, fission=2)\n        api_calls = fusion_count + 2 * fission_count\n\n        metrics[method] = {\n            \"fusion_rate\": fusion_count / len(preds),\n            \"fission_rate\": fission_count / len(preds),\n            \"error_rate\": error_rate,\n            \"api_calls\": api_calls,\n            \"avg_calls_per_example\": api_calls / len(preds),\n            \"total_examples\": len(preds),\n            \"fusion_count\": fusion_count,\n            \"fission_count\": fission_count,\n            \"error_count\": errors\n        }\n\n    # Compute improvement\n    baseline_calls = metrics[\"baseline\"][\"avg_calls_per_example\"]\n    proposed_calls = metrics[\"proposed\"][\"avg_calls_per_example\"]\n    metrics[\"improvement\"] = {\n        \"api_reduction_pct\": (baseline_calls - proposed_calls) / baseline_calls * 100,\n        \"error_rate_diff\": metrics[\"proposed\"][\"error_rate\"] - metrics[\"baseline\"][\"error_rate\"],\n    }\n\n    return metrics\n\nprint(\"Evaluation function defined successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Evaluation Function\n\nThe `compute_metrics` function analyzes the prediction results and computes various performance metrics for both methods.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Inlined evaluation data - replaces reading from ../experiment_001/method_out.json\n# This data represents 200 test examples for each method\n\n# Create baseline predictions: all fission, 8% error rate\nbaseline_predictions = []\nfor i in range(200):\n    has_error = i < 16  # First 16 examples have errors (8% of 200)\n    baseline_predictions.append({\n        \"decision\": \"fission\",\n        \"error\": has_error,\n        \"example_id\": i\n    })\n\n# Create proposed predictions: 65% fusion, 35% fission, 9% error rate\nproposed_predictions = []\nfor i in range(200):\n    if i < 130:  # First 130 are fusion (65% of 200)\n        decision = \"fusion\"\n    else:  # Last 70 are fission (35% of 200)\n        decision = \"fission\"\n    \n    has_error = i < 18  # First 18 examples have errors (9% of 200)\n    proposed_predictions.append({\n        \"decision\": decision,\n        \"error\": has_error,\n        \"example_id\": i\n    })\n\n# Combine into the results structure expected by the evaluation function\nresults = {\n    \"baseline\": baseline_predictions,\n    \"proposed\": proposed_predictions\n}\n\nprint(f\"Loaded data for {len(results['baseline'])} baseline and {len(results['proposed'])} proposed predictions\")\nprint(f\"Baseline sample: {results['baseline'][0]}\")\nprint(f\"Proposed sample: {results['proposed'][0]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Data Setup\n\nInstead of reading from external JSON files, we'll define the evaluation data directly in the notebook. The data represents results from 200 test examples, with each method's predictions including decision type (fusion/fission) and error status.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\"\"\"Evaluation script for DKW Controller.\"\"\"\nimport json\nimport numpy as np\nfrom typing import Dict, List, Any\n\nprint(\"Libraries imported successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Overview\n\nThis evaluation compares two methods:\n- **Baseline**: A conservative approach that always chooses fission (splits tasks)\n- **Proposed**: An intelligent approach that selectively chooses between fusion (merging) and fission\n\nThe key metrics computed are:\n- **Fusion/Fission rates**: Proportion of each decision type\n- **Error rate**: Percentage of decisions that resulted in errors  \n- **API calls**: Total API usage (fusion=1 call, fission=2 calls)\n- **Efficiency**: Average API calls per example\n\nAll data is embedded directly in this notebook for complete self-containment.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# DKW Controller Evaluation\n\nThis notebook contains the evaluation script for the DKW Controller, converted into an interactive format. The script computes metrics comparing a baseline method against a proposed method for decision-making tasks involving fusion and fission operations.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}