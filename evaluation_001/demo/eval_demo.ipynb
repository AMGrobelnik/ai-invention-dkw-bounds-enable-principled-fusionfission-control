{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DKW Controller Evaluation\n",
    "\n",
    "This notebook evaluates the performance of a DKW Controller, comparing baseline and proposed methods. The evaluation focuses on:\n",
    "- **Fusion vs Fission decisions**: The controller can choose to fuse or split operations\n",
    "- **API efficiency**: Fusion requires 1 API call, fission requires 2 API calls\n",
    "- **Error rates**: How often the controller makes incorrect decisions\n",
    "\n",
    "The goal is to measure the improvement in API efficiency while maintaining acceptable error rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data\n",
    "\n",
    "Instead of reading from external JSON files, we'll create sample data inline that represents the experimental results from both baseline and proposed methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample experimental results\n",
    "# This data represents the decisions made by baseline vs proposed methods\n",
    "\n",
    "# Baseline method: always chooses fission, 8% error rate\n",
    "baseline_results = []\n",
    "for i in range(200):\n",
    "    baseline_results.append({\n",
    "        \"decision\": \"fission\",\n",
    "        \"error\": i < 16  # First 16 examples have errors (8% of 200)\n",
    "    })\n",
    "\n",
    "# Proposed method: 65% fusion, 35% fission, 9% error rate  \n",
    "proposed_results = []\n",
    "for i in range(200):\n",
    "    if i < 130:  # First 130 examples use fusion (65% of 200)\n",
    "        decision = \"fusion\"\n",
    "    else:  # Remaining 70 examples use fission (35% of 200)\n",
    "        decision = \"fission\"\n",
    "    \n",
    "    proposed_results.append({\n",
    "        \"decision\": decision,\n",
    "        \"error\": i < 18  # First 18 examples have errors (9% of 200)\n",
    "    })\n",
    "\n",
    "# Combine into results structure\n",
    "results = {\n",
    "    \"baseline\": baseline_results,\n",
    "    \"proposed\": proposed_results\n",
    "}\n",
    "\n",
    "print(f\"Generated {len(baseline_results)} baseline results and {len(proposed_results)} proposed results\")\n",
    "print(f\"Baseline decisions: {sum(1 for r in baseline_results if r['decision'] == 'fusion')} fusion, {sum(1 for r in baseline_results if r['decision'] == 'fission')} fission\")\n",
    "print(f\"Proposed decisions: {sum(1 for r in proposed_results if r['decision'] == 'fusion')} fusion, {sum(1 for r in proposed_results if r['decision'] == 'fission')} fission\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics Function\n",
    "\n",
    "The `compute_metrics` function calculates several key performance indicators:\n",
    "\n",
    "- **Fusion/Fission rates**: Proportion of decisions for each strategy\n",
    "- **Error rate**: Percentage of incorrect decisions\n",
    "- **API calls**: Total API usage (fusion = 1 call, fission = 2 calls)\n",
    "- **Efficiency improvements**: Comparison between methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(results: dict) -> dict:\n",
    "    \"\"\"Compute evaluation metrics.\"\"\"\n",
    "    metrics = {}\n",
    "\n",
    "    for method in [\"baseline\", \"proposed\"]:\n",
    "        preds = results[method]\n",
    "\n",
    "        # Count decisions\n",
    "        fusion_count = sum(1 for p in preds if p[\"decision\"] == \"fusion\")\n",
    "        fission_count = sum(1 for p in preds if p[\"decision\"] == \"fission\")\n",
    "\n",
    "        # Compute error rate\n",
    "        errors = sum(1 for p in preds if p[\"error\"])\n",
    "        error_rate = errors / len(preds)\n",
    "\n",
    "        # API calls (fusion=1, fission=2)\n",
    "        api_calls = fusion_count + 2 * fission_count\n",
    "\n",
    "        metrics[method] = {\n",
    "            \"fusion_rate\": fusion_count / len(preds),\n",
    "            \"fission_rate\": fission_count / len(preds),\n",
    "            \"error_rate\": error_rate,\n",
    "            \"api_calls\": api_calls,\n",
    "            \"avg_calls_per_example\": api_calls / len(preds),\n",
    "        }\n",
    "\n",
    "    # Compute improvement\n",
    "    baseline_calls = metrics[\"baseline\"][\"avg_calls_per_example\"]\n",
    "    proposed_calls = metrics[\"proposed\"][\"avg_calls_per_example\"]\n",
    "    metrics[\"improvement\"] = {\n",
    "        \"api_reduction_pct\": (baseline_calls - proposed_calls) / baseline_calls * 100,\n",
    "        \"error_rate_diff\": metrics[\"proposed\"][\"error_rate\"] - metrics[\"baseline\"][\"error_rate\"],\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation\n",
    "\n",
    "Now let's compute the metrics and display the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "metrics = compute_metrics(results)\n",
    "\n",
    "# Display results in a formatted way\n",
    "print(\"=\"*50)\n",
    "print(\"DKW CONTROLLER EVALUATION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for method in [\"baseline\", \"proposed\"]:\n",
    "    print(f\"\\n{method.upper()} METHOD:\")\n",
    "    m = metrics[method]\n",
    "    print(f\"  Fusion rate:     {m['fusion_rate']:.1%}\")\n",
    "    print(f\"  Fission rate:    {m['fission_rate']:.1%}\") \n",
    "    print(f\"  Error rate:      {m['error_rate']:.1%}\")\n",
    "    print(f\"  Total API calls: {m['api_calls']}\")\n",
    "    print(f\"  Avg calls/example: {m['avg_calls_per_example']:.2f}\")\n",
    "\n",
    "print(f\"\\nIMPROVEMENT:\")\n",
    "print(f\"  API reduction: {metrics['improvement']['api_reduction_pct']:.1f}%\")\n",
    "print(f\"  Error rate change: {metrics['improvement']['error_rate_diff']:+.1%}\")\n",
    "\n",
    "# Also save as JSON (inline output)\n",
    "print(f\"\\nFull metrics as JSON:\")\n",
    "print(json.dumps(metrics, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with Different Scenarios\n",
    "\n",
    "You can modify the parameters below to see how different controller behaviors would affect the metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: What if we had a different proposed method?\n",
    "# Modify these parameters to see different scenarios\n",
    "\n",
    "def create_experimental_results(n_examples=200, fusion_rate=0.8, error_rate=0.05):\n",
    "    \"\"\"Create experimental results with custom parameters.\"\"\"\n",
    "    results = []\n",
    "    n_fusion = int(n_examples * fusion_rate)\n",
    "    n_errors = int(n_examples * error_rate)\n",
    "    \n",
    "    for i in range(n_examples):\n",
    "        decision = \"fusion\" if i < n_fusion else \"fission\"\n",
    "        error = i < n_errors\n",
    "        results.append({\"decision\": decision, \"error\": error})\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Try different scenarios\n",
    "experimental_scenarios = {\n",
    "    \"baseline\": baseline_results,  # Keep baseline the same\n",
    "    \"high_fusion_low_error\": create_experimental_results(fusion_rate=0.9, error_rate=0.03),\n",
    "    \"medium_fusion\": create_experimental_results(fusion_rate=0.5, error_rate=0.06),\n",
    "    \"original_proposed\": proposed_results\n",
    "}\n",
    "\n",
    "print(\"Comparing different scenarios:\\n\")\n",
    "for scenario_name, scenario_results in experimental_scenarios.items():\n",
    "    if scenario_name == \"baseline\":\n",
    "        continue\n",
    "    \n",
    "    scenario_data = {\"baseline\": baseline_results, \"proposed\": scenario_results}\n",
    "    scenario_metrics = compute_metrics(scenario_data)\n",
    "    \n",
    "    print(f\"{scenario_name.upper()}:\")\n",
    "    print(f\"  API reduction: {scenario_metrics['improvement']['api_reduction_pct']:.1f}%\")\n",
    "    print(f\"  Error rate change: {scenario_metrics['improvement']['error_rate_diff']:+.1%}\")\n",
    "    print(f\"  Fusion rate: {scenario_metrics['proposed']['fusion_rate']:.1%}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}