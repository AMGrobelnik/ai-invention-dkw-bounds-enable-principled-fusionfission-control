{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## ðŸ”§ Experiment with Different Scenarios\n\nYou can easily modify the parameters above to test different scenarios:\n\n1. **Change error rates**: Modify the error assignment in the sample data creation\n2. **Adjust fusion/fission ratios**: Change the decision distributions  \n3. **Vary sample size**: Modify the range in the for loops\n4. **Add new methods**: Extend the results dictionary with additional approaches\n\nTry rerunning the cells above with different parameters to see how the metrics change!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Detailed analysis\nprint(\"ðŸ“Š Detailed Performance Analysis\")\nprint(\"=\" * 50)\n\nprint(f\"\\nðŸ”„ Decision Patterns:\")\nprint(f\"Baseline  - Fusion: {metrics['baseline']['fusion_rate']:.1%}, Fission: {metrics['baseline']['fission_rate']:.1%}\")\nprint(f\"Proposed  - Fusion: {metrics['proposed']['fusion_rate']:.1%}, Fission: {metrics['proposed']['fission_rate']:.1%}\")\n\nprint(f\"\\nâš ï¸  Error Rates:\")\nprint(f\"Baseline  - {metrics['baseline']['error_rate']:.1%}\")\nprint(f\"Proposed  - {metrics['proposed']['error_rate']:.1%}\")\nprint(f\"Difference: +{metrics['improvement']['error_rate_diff']:.1%}\")\n\nprint(f\"\\nðŸ“ž API Efficiency:\")\nprint(f\"Baseline  - {metrics['baseline']['avg_calls_per_example']:.2f} calls/example\")\nprint(f\"Proposed  - {metrics['proposed']['avg_calls_per_example']:.2f} calls/example\") \nprint(f\"Reduction: {metrics['improvement']['api_reduction_pct']:.1f}%\")\n\nprint(f\"\\nðŸ’¡ Key Insights:\")\nprint(f\"â€¢ The proposed method reduces API calls by {metrics['improvement']['api_reduction_pct']:.1f}% through intelligent fusion\")\nprint(f\"â€¢ {metrics['proposed']['fusion_rate']:.0%} of decisions use more efficient fusion approach\")\nprint(f\"â€¢ Slight increase in error rate ({metrics['improvement']['error_rate_diff']:+.1%}) is minimal trade-off\")\n\n# Simple text-based visualization\nprint(f\"\\nðŸ“ˆ API Calls Comparison (per 100 examples):\")\nbaseline_bar = \"â–ˆ\" * int(metrics['baseline']['avg_calls_per_example'] * 10)\nproposed_bar = \"â–ˆ\" * int(metrics['proposed']['avg_calls_per_example'] * 10)\nprint(f\"Baseline:  {baseline_bar} ({metrics['baseline']['avg_calls_per_example']:.1f})\")\nprint(f\"Proposed:  {proposed_bar} ({metrics['proposed']['avg_calls_per_example']:.1f})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Analysis & Insights\n\nLet's analyze the results in more detail and create some visualizations.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Compute metrics\nmetrics = compute_metrics(results)\n\n# Display results (equivalent to writing eval_out.json)\nprint(\"Evaluation Results:\")\nprint(\"=\" * 50)\nprint(json.dumps(metrics, indent=2))\n\n# Main result from original script\nprint(f\"\\nKey Finding:\")\nprint(f\"API reduction: {metrics['improvement']['api_reduction_pct']:.1f}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Run Evaluation\n\nNow let's compute the metrics and display the results.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def compute_metrics(results: dict) -> dict:\n    \"\"\"Compute evaluation metrics.\"\"\"\n    metrics = {}\n\n    for method in [\"baseline\", \"proposed\"]:\n        preds = results[method]\n\n        # Count decisions\n        fusion_count = sum(1 for p in preds if p[\"decision\"] == \"fusion\")\n        fission_count = sum(1 for p in preds if p[\"decision\"] == \"fission\")\n\n        # Compute error rate\n        errors = sum(1 for p in preds if p[\"error\"])\n        error_rate = errors / len(preds)\n\n        # API calls (fusion=1, fission=2)\n        api_calls = fusion_count + 2 * fission_count\n\n        metrics[method] = {\n            \"fusion_rate\": fusion_count / len(preds),\n            \"fission_rate\": fission_count / len(preds),\n            \"error_rate\": error_rate,\n            \"api_calls\": api_calls,\n            \"avg_calls_per_example\": api_calls / len(preds),\n        }\n\n    # Compute improvement\n    baseline_calls = metrics[\"baseline\"][\"avg_calls_per_example\"]\n    proposed_calls = metrics[\"proposed\"][\"avg_calls_per_example\"]\n    metrics[\"improvement\"] = {\n        \"api_reduction_pct\": (baseline_calls - proposed_calls) / baseline_calls * 100,\n        \"error_rate_diff\": metrics[\"proposed\"][\"error_rate\"] - metrics[\"baseline\"][\"error_rate\"],\n    }\n\n    return metrics",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Metrics Calculation Function\n\nThe core evaluation function that computes various performance metrics for both methods.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create sample data that matches the expected results\n# Based on eval_out.json: 200 examples each method\n\n# Baseline: 100% fission, 8% error rate\nbaseline_predictions = []\nfor i in range(200):\n    error = i < 16  # First 16 have errors (8% of 200)\n    baseline_predictions.append({\n        \"decision\": \"fission\",\n        \"error\": error\n    })\n\n# Proposed: 65% fusion, 35% fission, 9% error rate  \nproposed_predictions = []\nfor i in range(200):\n    decision = \"fusion\" if i < 130 else \"fission\"  # First 130 are fusion (65%)\n    error = i < 18  # First 18 have errors (9% of 200)\n    proposed_predictions.append({\n        \"decision\": decision,\n        \"error\": error\n    })\n\n# Combine into results structure that matches original file format\nresults = {\n    \"baseline\": baseline_predictions,\n    \"proposed\": proposed_predictions\n}\n\nprint(f\"Created sample data:\")\nprint(f\"- Baseline: {len(results['baseline'])} predictions\")  \nprint(f\"- Proposed: {len(results['proposed'])} predictions\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Sample Data\n\nThe original script reads from a JSON file containing evaluation results. For this self-contained notebook, we'll inline the sample data that would produce the expected metrics.\n\nThe data structure contains results for two methods:\n- **baseline**: Traditional approach that always uses fission\n- **proposed**: Optimized approach that intelligently chooses between fusion and fission\n\nEach method contains a list of predictions with:\n- `decision`: Either \"fusion\" (1 API call) or \"fission\" (2 API calls)  \n- `error`: Boolean indicating if there was an error",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\"\"\"Evaluation script for DKW Controller.\"\"\"\nimport json\nimport numpy as np",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# DKW Controller Evaluation\n\n**Artifact ID:** evaluation_001  \n**Name:** eval.py\n\nThis notebook evaluates the performance of two methods (baseline vs proposed) for the DKW Controller system. It computes various metrics including fusion/fission rates, error rates, and API call efficiency.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}