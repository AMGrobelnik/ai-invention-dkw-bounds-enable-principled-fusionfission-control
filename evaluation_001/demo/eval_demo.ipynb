{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## How to Modify This Notebook\n\nThis notebook is completely self-contained and can be easily modified:\n\n1. **Change the data**: Modify the data generation logic in the \"Sample Data Setup\" section to test different scenarios\n2. **Adjust metrics**: Add new metrics to the `compute_metrics()` function\n3. **Test different ratios**: Change the fusion/fission rates or error rates in the data generation\n4. **Add visualizations**: Use matplotlib or seaborn to create charts from the metrics\n\n### Key Parameters to Experiment With:\n- Number of examples (currently 200 for each method)\n- Fusion/fission decision ratios\n- Error rates for each method\n- API call costs (currently fusion=1, fission=2)\n\nThe notebook will automatically recalculate all metrics when you modify the input data!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Display detailed results\nprint(\"=\" * 50)\nprint(\"DKW CONTROLLER EVALUATION RESULTS\")\nprint(\"=\" * 50)\n\nprint(f\"\\nðŸ“Š BASELINE METHOD:\")\nprint(f\"   Fusion Rate: {metrics['baseline']['fusion_rate']:.1%}\")\nprint(f\"   Fission Rate: {metrics['baseline']['fission_rate']:.1%}\")\nprint(f\"   Error Rate: {metrics['baseline']['error_rate']:.1%}\")\nprint(f\"   Total API Calls: {metrics['baseline']['api_calls']}\")\nprint(f\"   Avg Calls/Example: {metrics['baseline']['avg_calls_per_example']:.2f}\")\n\nprint(f\"\\nðŸš€ PROPOSED METHOD:\")\nprint(f\"   Fusion Rate: {metrics['proposed']['fusion_rate']:.1%}\")\nprint(f\"   Fission Rate: {metrics['proposed']['fission_rate']:.1%}\")\nprint(f\"   Error Rate: {metrics['proposed']['error_rate']:.1%}\")\nprint(f\"   Total API Calls: {metrics['proposed']['api_calls']}\")\nprint(f\"   Avg Calls/Example: {metrics['proposed']['avg_calls_per_example']:.2f}\")\n\nprint(f\"\\nâœ¨ IMPROVEMENT SUMMARY:\")\nprint(f\"   API Reduction: {metrics['improvement']['api_reduction_pct']:.1f}%\")\nprint(f\"   Error Rate Change: {metrics['improvement']['error_rate_diff']:+.1%}\")\n\n# Main result as in original script\nprint(f\"\\nðŸŽ¯ KEY RESULT: API reduction: {metrics['improvement']['api_reduction_pct']:.1f}%\")\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"COMPLETE JSON OUTPUT:\")\nprint(\"=\" * 50)\nprint(eval_output)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Results Display\n\nPretty-print the evaluation results with detailed breakdown.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Compute metrics using our sample data\nmetrics = compute_metrics(results)\n\n# Save results (this replaces writing to \"eval_out.json\")\neval_output = json.dumps(metrics, indent=2)\nprint(\"Evaluation completed!\")\nprint(\"\\nResults saved (would be written to eval_out.json):\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Run Evaluation\n\nCompute the metrics for both baseline and proposed methods.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def compute_metrics(results: dict) -> dict:\n    \"\"\"Compute evaluation metrics.\"\"\"\n    metrics = {}\n\n    for method in [\"baseline\", \"proposed\"]:\n        preds = results[method]\n\n        # Count decisions\n        fusion_count = sum(1 for p in preds if p[\"decision\"] == \"fusion\")\n        fission_count = sum(1 for p in preds if p[\"decision\"] == \"fission\")\n\n        # Compute error rate\n        errors = sum(1 for p in preds if p[\"error\"])\n        error_rate = errors / len(preds)\n\n        # API calls (fusion=1, fission=2)\n        api_calls = fusion_count + 2 * fission_count\n\n        metrics[method] = {\n            \"fusion_rate\": fusion_count / len(preds),\n            \"fission_rate\": fission_count / len(preds),\n            \"error_rate\": error_rate,\n            \"api_calls\": api_calls,\n            \"avg_calls_per_example\": api_calls / len(preds),\n        }\n\n    # Compute improvement\n    baseline_calls = metrics[\"baseline\"][\"avg_calls_per_example\"]\n    proposed_calls = metrics[\"proposed\"][\"avg_calls_per_example\"]\n    metrics[\"improvement\"] = {\n        \"api_reduction_pct\": (baseline_calls - proposed_calls) / baseline_calls * 100,\n        \"error_rate_diff\": metrics[\"proposed\"][\"error_rate\"] - metrics[\"baseline\"][\"error_rate\"],\n    }\n\n    return metrics\n\nprint(\"Metrics computation function defined!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Metrics Computation Function\n\nThe core evaluation function that computes performance metrics for both methods.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create sample data that matches the expected results\n# This replaces reading from \"../experiment_001/method_out.json\"\n\n# Generate baseline results: 200 examples, all fission, 8% error rate\nbaseline_preds = []\nfor i in range(200):\n    is_error = i < 16  # First 16 examples have errors (8% of 200)\n    baseline_preds.append({\n        \"decision\": \"fission\",  # Baseline always chooses fission\n        \"error\": is_error\n    })\n\n# Generate proposed results: 200 examples, 65% fusion, 35% fission, 9% error rate\nproposed_preds = []\nfor i in range(200):\n    is_error = i < 18  # First 18 examples have errors (9% of 200)\n    decision = \"fusion\" if i < 130 else \"fission\"  # 130 fusion (65%), 70 fission (35%)\n    proposed_preds.append({\n        \"decision\": decision,\n        \"error\": is_error\n    })\n\n# Create the results dictionary that would have been loaded from JSON\nresults = {\n    \"baseline\": baseline_preds,\n    \"proposed\": proposed_preds\n}\n\nprint(f\"Generated data:\")\nprint(f\"- Baseline: {len(results['baseline'])} predictions\")\nprint(f\"- Proposed: {len(results['proposed'])} predictions\")\nprint(f\"- Baseline decisions: {sum(1 for p in baseline_preds if p['decision'] == 'fission')} fission, {sum(1 for p in baseline_preds if p['decision'] == 'fusion')} fusion\")\nprint(f\"- Proposed decisions: {sum(1 for p in proposed_preds if p['decision'] == 'fission')} fission, {sum(1 for p in proposed_preds if p['decision'] == 'fusion')} fusion\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Sample Data Setup\n\nInstead of reading from external JSON files, we'll create inline sample data that represents the experimental results. This data simulates:\n- **Baseline method**: Always chooses fission (more expensive, 2 API calls per decision)\n- **Proposed method**: Intelligently chooses between fusion (1 call) and fission (2 calls)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\"\"\"Evaluation script for DKW Controller.\"\"\"\nimport json\nimport numpy as np\n\nprint(\"Libraries imported successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# DKW Controller Evaluation\n\nThis notebook evaluates the performance of a DKW (Decision-Knowledge-Workflow) controller by comparing baseline and proposed methods. The evaluation focuses on:\n- **Fusion vs Fission decisions**: Different strategies for handling requests\n- **Error rates**: Frequency of incorrect predictions\n- **API call efficiency**: Number of API calls required per example\n\n## Metrics Computed\n- Fusion/Fission rates for each method\n- Error rates comparison\n- API call reduction percentage\n- Average calls per example",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}