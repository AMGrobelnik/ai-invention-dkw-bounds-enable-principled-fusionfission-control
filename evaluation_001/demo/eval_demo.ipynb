{
 "cells": [
  {
   "cell_type": "code",
   "source": "# Experiment with different parameters\ndef create_experimental_data(n_examples=200, proposed_fusion_rate=0.65, \n                            baseline_error_rate=0.08, proposed_error_rate=0.09):\n    \"\"\"Create experimental data with custom parameters.\"\"\"\n    \n    experimental_results = {\n        \"baseline\": [\n            {\"decision\": \"fission\", \"error\": i < int(n_examples * baseline_error_rate)} \n            for i in range(n_examples)\n        ],\n        \"proposed\": [\n            {\n                \"decision\": \"fusion\" if i < int(n_examples * proposed_fusion_rate) else \"fission\", \n                \"error\": i < int(n_examples * proposed_error_rate)\n            } \n            for i in range(n_examples)\n        ]\n    }\n    \n    return experimental_results\n\n# Try different scenarios - modify these values to experiment!\nexperimental_scenarios = {\n    \"Current\": {\"fusion_rate\": 0.65, \"baseline_error\": 0.08, \"proposed_error\": 0.09},\n    \"Conservative\": {\"fusion_rate\": 0.40, \"baseline_error\": 0.08, \"proposed_error\": 0.06},\n    \"Aggressive\": {\"fusion_rate\": 0.85, \"baseline_error\": 0.08, \"proposed_error\": 0.12},\n}\n\nprint(\"Experimental Scenario Comparison:\")\nprint(\"=\" * 80)\n\nfor scenario_name, params in experimental_scenarios.items():\n    exp_data = create_experimental_data(\n        proposed_fusion_rate=params[\"fusion_rate\"],\n        baseline_error_rate=params[\"baseline_error\"], \n        proposed_error_rate=params[\"proposed_error\"]\n    )\n    \n    exp_metrics = compute_metrics(exp_data)\n    \n    print(f\"\\n{scenario_name} Scenario:\")\n    print(f\"  Fusion Rate: {params['fusion_rate']:.1%}\")\n    print(f\"  API Reduction: {exp_metrics['improvement']['api_reduction_pct']:.1f}%\")\n    print(f\"  Error Change: {exp_metrics['improvement']['error_rate_diff']:+.1%}\")\n    print(f\"  Avg Calls/Example: {exp_metrics['proposed']['avg_calls_per_example']:.2f}\")\n\nprint(f\"\\n{'='*80}\")\nprint(\"ðŸ’¡ TIP: Modify the experimental_scenarios dictionary above to test your own scenarios!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Interactive Experimentation\n\nYou can modify the parameters below to experiment with different scenarios and see how they affect the metrics.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create visualizations\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\nfig.suptitle('DKW Controller Performance Analysis', fontsize=16, fontweight='bold')\n\n# 1. Decision Distribution Comparison\nmethods = ['Baseline', 'Proposed']\nfusion_rates = [metrics['baseline']['fusion_rate'], metrics['proposed']['fusion_rate']]\nfission_rates = [metrics['baseline']['fission_rate'], metrics['proposed']['fission_rate']]\n\nx = np.arange(len(methods))\nwidth = 0.35\n\nax1.bar(x - width/2, fusion_rates, width, label='Fusion', alpha=0.8, color='skyblue')\nax1.bar(x + width/2, fission_rates, width, label='Fission', alpha=0.8, color='lightcoral')\nax1.set_xlabel('Method')\nax1.set_ylabel('Decision Rate')\nax1.set_title('Decision Distribution')\nax1.set_xticks(x)\nax1.set_xticklabels(methods)\nax1.legend()\nax1.set_ylim(0, 1.1)\n\n# 2. Error Rate Comparison\nerror_rates = [metrics['baseline']['error_rate'], metrics['proposed']['error_rate']]\nax2.bar(methods, error_rates, alpha=0.8, color=['red', 'orange'])\nax2.set_ylabel('Error Rate')\nax2.set_title('Error Rate Comparison')\nax2.set_ylim(0, max(error_rates) * 1.2)\nfor i, v in enumerate(error_rates):\n    ax2.text(i, v + 0.002, f'{v:.1%}', ha='center', va='bottom')\n\n# 3. API Calls per Example\napi_calls_avg = [metrics['baseline']['avg_calls_per_example'], \n                 metrics['proposed']['avg_calls_per_example']]\nbars = ax3.bar(methods, api_calls_avg, alpha=0.8, color=['lightblue', 'lightgreen'])\nax3.set_ylabel('Average API Calls per Example')\nax3.set_title('API Call Efficiency')\nax3.set_ylim(0, max(api_calls_avg) * 1.2)\nfor i, v in enumerate(api_calls_avg):\n    ax3.text(i, v + 0.05, f'{v:.2f}', ha='center', va='bottom')\n\n# 4. Total API Calls\ntotal_api_calls = [metrics['baseline']['api_calls'], metrics['proposed']['api_calls']]\nax4.bar(methods, total_api_calls, alpha=0.8, color=['coral', 'lightseagreen'])\nax4.set_ylabel('Total API Calls')\nax4.set_title('Total API Call Usage')\nfor i, v in enumerate(total_api_calls):\n    ax4.text(i, v + 5, f'{v}', ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()\n\n# Summary statistics\nprint(f\"\\nðŸŽ¯ KEY FINDINGS:\")\nprint(f\"   â€¢ API calls reduced by {metrics['improvement']['api_reduction_pct']:.1f}%\")\nprint(f\"   â€¢ Proposed method uses {metrics['proposed']['fusion_rate']:.1%} fusion decisions\")\nprint(f\"   â€¢ Error rate changed by {metrics['improvement']['error_rate_diff']:+.1%}\")\nprint(f\"   â€¢ Total API calls: {metrics['baseline']['api_calls']} â†’ {metrics['proposed']['api_calls']}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 4. Visualizations\n\nLet's create some visualizations to better understand the performance differences between the methods.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Compute metrics\nmetrics = compute_metrics(results)\n\n# Display results in a nice format\nprint(\"=\"*60)\nprint(\"           DKW CONTROLLER EVALUATION RESULTS\")\nprint(\"=\"*60)\n\n# Method comparison table\nmethods_data = []\nfor method in [\"baseline\", \"proposed\"]:\n    m = metrics[method]\n    methods_data.append({\n        'Method': method.title(),\n        'Fusion Rate': f\"{m['fusion_rate']:.1%}\",\n        'Fission Rate': f\"{m['fission_rate']:.1%}\",\n        'Error Rate': f\"{m['error_rate']:.1%}\",\n        'API Calls': m['api_calls'],\n        'Avg Calls/Example': f\"{m['avg_calls_per_example']:.2f}\"\n    })\n\ndf = pd.DataFrame(methods_data)\nprint(\"\\nMethod Comparison:\")\nprint(df.to_string(index=False))\n\n# Improvement metrics\nprint(f\"\\n{'='*60}\")\nprint(\"                    IMPROVEMENTS\")\nprint(f\"{'='*60}\")\nprint(f\"API Call Reduction: {metrics['improvement']['api_reduction_pct']:.1f}%\")\nprint(f\"Error Rate Change:  {metrics['improvement']['error_rate_diff']:+.1%}\")\n\n# Save results (equivalent to the original eval_out.json)\neval_output = metrics\nprint(f\"\\nEquivalent to eval_out.json content:\")\nprint(json.dumps(eval_output, indent=2))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Run Evaluation and Display Results\n\nNow let's compute the metrics and display the results in a readable format.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def compute_metrics(results: dict) -> dict:\n    \"\"\"Compute evaluation metrics for both baseline and proposed methods.\"\"\"\n    metrics = {}\n\n    for method in [\"baseline\", \"proposed\"]:\n        preds = results[method]\n\n        # Count decisions\n        fusion_count = sum(1 for p in preds if p[\"decision\"] == \"fusion\")\n        fission_count = sum(1 for p in preds if p[\"decision\"] == \"fission\")\n\n        # Compute error rate\n        errors = sum(1 for p in preds if p[\"error\"])\n        error_rate = errors / len(preds)\n\n        # API calls (fusion=1, fission=2)\n        api_calls = fusion_count + 2 * fission_count\n\n        metrics[method] = {\n            \"fusion_rate\": fusion_count / len(preds),\n            \"fission_rate\": fission_count / len(preds),\n            \"error_rate\": error_rate,\n            \"api_calls\": api_calls,\n            \"avg_calls_per_example\": api_calls / len(preds),\n        }\n\n    # Compute improvement metrics\n    baseline_calls = metrics[\"baseline\"][\"avg_calls_per_example\"]\n    proposed_calls = metrics[\"proposed\"][\"avg_calls_per_example\"]\n    metrics[\"improvement\"] = {\n        \"api_reduction_pct\": (baseline_calls - proposed_calls) / baseline_calls * 100,\n        \"error_rate_diff\": metrics[\"proposed\"][\"error_rate\"] - metrics[\"baseline\"][\"error_rate\"],\n    }\n\n    return metrics\n\nprint(\"Metrics computation function defined successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 2. Metrics Computation Function\n\nThis function analyzes the predictions from both methods and computes key performance metrics:\n\n- **Fusion Rate**: Percentage of examples that chose fusion (1 API call)\n- **Fission Rate**: Percentage of examples that chose fission (2 API calls)  \n- **Error Rate**: Percentage of predictions that resulted in errors\n- **API Calls**: Total number of API calls made\n- **Average Calls per Example**: Efficiency metric for API usage",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\"\"\"Evaluation script for DKW Controller.\"\"\"\nimport json\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Sample data representing the results that would normally be read from method_out.json\n# This data represents predictions from both baseline and proposed methods\nresults = {\n    \"baseline\": [\n        # Baseline always chooses fission (decision=\"fission\"), with 8% error rate\n        {\"decision\": \"fission\", \"error\": i < 16} for i in range(200)\n    ],\n    \"proposed\": [\n        # Proposed method: 65% fusion, 35% fission, with 9% error rate\n        {\"decision\": \"fusion\" if i < 130 else \"fission\", \"error\": i < 18} for i in range(200)\n    ]\n}\n\nprint(\"Sample data loaded successfully!\")\nprint(f\"Baseline examples: {len(results['baseline'])}\")\nprint(f\"Proposed examples: {len(results['proposed'])}\")\nprint(f\"Sample baseline prediction: {results['baseline'][0]}\")\nprint(f\"Sample proposed prediction: {results['proposed'][0]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Setup and Data Import\n\nFirst, let's import the required libraries and set up our sample data. In the original script, this data would be read from `../experiment_001/method_out.json`, but we'll inline it here for a self-contained notebook.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# DKW Controller Evaluation\n\nThis notebook contains an evaluation script for the DKW Controller, analyzing the performance metrics between baseline and proposed methods for fusion/fission decisions.\n\n## Overview\n- **Baseline Method**: Always chooses fission (2 API calls per example)\n- **Proposed Method**: Intelligently chooses between fusion (1 API call) and fission (2 API calls)\n- **Metrics**: Fusion rate, fission rate, error rate, and API call efficiency",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}