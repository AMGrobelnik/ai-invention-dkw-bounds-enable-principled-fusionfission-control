{
 "cells": [
  {
   "cell_type": "code",
   "source": "# Run the experiment\nprint(\"üöÄ Running experiment...\")\nresults = run_experiment(sample_data)\n\nprint(f\"‚úÖ Experiment completed!\")\nprint(f\"   Total examples processed: {len(results['baseline'])}\")\n\n# Basic statistics\nbaseline_errors = sum(1 for r in results['baseline'] if r['error'])\nproposed_errors = sum(1 for r in results['proposed'] if r['error'])\n\nbaseline_fissions = sum(1 for r in results['baseline'] if r['decision'] == 'fission')\nproposed_fissions = sum(1 for r in results['proposed'] if r['decision'] == 'fission')\n\nprint(f\"\\nüìä Results Summary:\")\nprint(f\"   Baseline - Errors: {baseline_errors}, Fissions: {baseline_fissions}\")\nprint(f\"   Proposed - Errors: {proposed_errors}, Fusions: {len(results['proposed']) - proposed_fissions}\")\n\n# Show first few results\nprint(f\"\\nüîç First 10 decisions:\")\nfor i in range(10):\n    b = results['baseline'][i]\n    p = results['proposed'][i]\n    print(f\"   {b['id']}: baseline={b['decision']}, proposed={p['decision']}, error={b['error']}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Optional: Simple visualization using matplotlib\nimport matplotlib.pyplot as plt\n\n# Create a simple comparison chart\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 10))\n\nmethods = ['Baseline', 'Proposed']\n\n# Fusion vs Fission rates\nfusion_rates = [metrics['baseline']['fusion_rate'], metrics['proposed']['fusion_rate']]\nfission_rates = [metrics['baseline']['fission_rate'], metrics['proposed']['fission_rate']]\n\nax1.bar(methods, fusion_rates, color='skyblue', label='Fusion')\nax1.bar(methods, fission_rates, bottom=fusion_rates, color='lightcoral', label='Fission')\nax1.set_ylabel('Decision Rate')\nax1.set_title('Decision Strategy Distribution')\nax1.legend()\n\n# Error rates\nerror_rates = [metrics['baseline']['error_rate'], metrics['proposed']['error_rate']]\nax2.bar(methods, error_rates, color='orange')\nax2.set_ylabel('Error Rate')\nax2.set_title('Error Rate Comparison')\n\n# API calls per example\napi_calls = [metrics['baseline']['avg_calls_per_example'], metrics['proposed']['avg_calls_per_example']]\nax3.bar(methods, api_calls, color='green')\nax3.set_ylabel('Avg API Calls per Example')\nax3.set_title('API Efficiency')\n\n# Improvement summary\nimprovements = [0, metrics['improvement']['api_reduction_pct']]\nax4.bar(['Baseline', 'Improvement'], improvements, color=['gray', 'purple'])\nax4.set_ylabel('API Reduction %')\nax4.set_title('API Call Reduction')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Visualization complete! The proposed method shows significant API call reduction.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Run Experiment\n\nLet's execute the experiment and compare the baseline vs. proposed approach:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def run_experiment(data):\n    \"\"\"Run DKW controller experiment with inline data.\"\"\"\n    controller = DKWController()\n    results = {\"baseline\": [], \"proposed\": []}\n\n    for example in data:\n        # Simulate error occurrence based on difficulty\n        error = np.random.random() < example[\"difficulty\"]\n        controller.add_observation(float(error))\n        decision = controller.decide()\n\n        results[\"proposed\"].append({\n            \"id\": example[\"id\"],\n            \"decision\": decision,\n            \"error\": error,\n            \"difficulty\": example[\"difficulty\"]  # Include for analysis\n        })\n        \n        results[\"baseline\"].append({\n            \"id\": example[\"id\"],\n            \"decision\": \"fission\",  # Always conservative\n            \"error\": error,\n            \"difficulty\": example[\"difficulty\"]\n        })\n\n    return results\n\nprint(\"‚úÖ Experiment function defined\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Optional: Visualization and Further Analysis\n\nYou can extend this notebook with visualizations and deeper analysis. Here are some ideas:\n- Bar charts comparing fusion/fission rates\n- API call efficiency visualization  \n- Error rate analysis\n- Statistical significance testing\n\nThe notebook is now self-contained and can be run without any external files!",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 4. Experiment Function\n\nThe experiment function simulates error occurrence based on difficulty and compares two approaches:\n1. **Baseline**: Always uses conservative \"fission\" mode\n2. **Proposed**: Uses adaptive DKW controller",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Compute metrics\nmetrics = compute_metrics(results)\n\n# Display results\nprint(\"=== DKW Controller Evaluation Results ===\\n\")\n\nfor method in [\"baseline\", \"proposed\"]:\n    print(f\"{method.upper()} METHOD:\")\n    m = metrics[method]\n    print(f\"  Fusion rate:     {m['fusion_rate']:.1%}\")\n    print(f\"  Fission rate:    {m['fission_rate']:.1%}\")\n    print(f\"  Error rate:      {m['error_rate']:.1%}\")\n    print(f\"  Total API calls: {m['api_calls']}\")\n    print(f\"  Avg calls/example: {m['avg_calls_per_example']:.2f}\")\n    print()\n\nprint(\"IMPROVEMENT ANALYSIS:\")\nimp = metrics[\"improvement\"]\nprint(f\"  API reduction:   {imp['api_reduction_pct']:.1f}%\")\nprint(f\"  Error rate diff: {imp['error_rate_diff']:+.1%}\")\n\n# Also save the results (equivalent to the original script's file output)\noutput_data = metrics\nprint(f\"\\n=== Key Result ===\")\nprint(f\"API reduction: {metrics['improvement']['api_reduction_pct']:.1f}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Sample dataset - inline data instead of reading from external JSON file\nsample_data = []\n\n# Create diverse examples with varying difficulty levels\ndifficulty_patterns = [\n    # Easy examples (low error probability)\n    *[{\"id\": f\"easy_{i:03d}\", \"difficulty\": 0.02} for i in range(50)],\n    \n    # Medium examples  \n    *[{\"id\": f\"medium_{i:03d}\", \"difficulty\": 0.08} for i in range(100)],\n    \n    # Hard examples (high error probability)\n    *[{\"id\": f\"hard_{i:03d}\", \"difficulty\": 0.25} for i in range(50)],\n    \n    # Mixed difficulty examples\n    *[{\"id\": f\"mixed_{i:03d}\", \"difficulty\": np.random.uniform(0.05, 0.15)} for i in range(100)],\n]\n\nsample_data = difficulty_patterns\n\nprint(f\"‚úÖ Created dataset with {len(sample_data)} examples\")\nprint(f\"   Difficulty range: {min(ex['difficulty'] for ex in sample_data):.3f} - {max(ex['difficulty'] for ex in sample_data):.3f}\")\n\n# Show first few examples\nprint(\"\\nSample examples:\")\nfor i in range(5):\n    ex = sample_data[i]\n    print(f\"  {ex['id']}: difficulty = {ex['difficulty']:.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Run Evaluation\n\nNow let's compute the metrics and display the results in a readable format.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Sample Dataset\n\nInstead of reading from external files, we'll create sample data inline. This dataset simulates examples with varying difficulty levels that influence error probability.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def compute_metrics(results: dict) -> dict:\n    \"\"\"Compute evaluation metrics for both baseline and proposed methods.\"\"\"\n    metrics = {}\n\n    for method in [\"baseline\", \"proposed\"]:\n        preds = results[method]\n\n        # Count decisions\n        fusion_count = sum(1 for p in preds if p[\"decision\"] == \"fusion\")\n        fission_count = sum(1 for p in preds if p[\"decision\"] == \"fission\")\n\n        # Compute error rate\n        errors = sum(1 for p in preds if p[\"error\"])\n        error_rate = errors / len(preds)\n\n        # API calls (fusion=1, fission=2)\n        api_calls = fusion_count + 2 * fission_count\n\n        metrics[method] = {\n            \"fusion_rate\": fusion_count / len(preds),\n            \"fission_rate\": fission_count / len(preds),\n            \"error_rate\": error_rate,\n            \"api_calls\": api_calls,\n            \"avg_calls_per_example\": api_calls / len(preds),\n        }\n\n    # Compute improvement metrics\n    baseline_calls = metrics[\"baseline\"][\"avg_calls_per_example\"]\n    proposed_calls = metrics[\"proposed\"][\"avg_calls_per_example\"]\n    metrics[\"improvement\"] = {\n        \"api_reduction_pct\": (baseline_calls - proposed_calls) / baseline_calls * 100,\n        \"error_rate_diff\": metrics[\"proposed\"][\"error_rate\"] - metrics[\"baseline\"][\"error_rate\"],\n    }\n\n    return metrics\n\n# Test the function\ntest_metrics = compute_metrics(results)\nprint(\"Function defined successfully!\")\nprint(\"Sample metric:\", test_metrics[\"baseline\"][\"fusion_rate\"])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "@dataclass\nclass DKWController:\n    \"\"\"DKW-guided fusion/fission controller.\"\"\"\n    epsilon_target: float = 0.10  # Target error rate (10%)\n    delta: float = 0.05           # Confidence parameter (5%)\n    min_samples: int = 100        # Minimum samples before decisions\n    hysteresis: float = 0.05      # Hysteresis to prevent oscillation\n\n    samples: list = field(default_factory=list)\n    current_state: str = \"fission\"  # Start conservatively\n\n    def dkw_epsilon(self, n: int) -> float:\n        \"\"\"Compute DKW epsilon for n samples.\"\"\"\n        if n < 2:\n            return 1.0\n        return np.sqrt(np.log(2 / self.delta) / (2 * n))\n\n    def add_observation(self, error: float) -> None:\n        \"\"\"Add error observation for calibration.\"\"\"\n        self.samples.append(error)\n\n    def decide(self) -> str:\n        \"\"\"Make fusion/fission decision with DKW guarantee.\"\"\"\n        n = len(self.samples)\n        if n < self.min_samples:\n            return self.current_state\n\n        epsilon = self.dkw_epsilon(n)\n        empirical_error = np.mean(self.samples[-self.min_samples:])\n        error_upper_bound = empirical_error + epsilon\n\n        if self.current_state == \"fusion\":\n            if error_upper_bound > self.epsilon_target + self.hysteresis:\n                self.current_state = \"fission\"\n        else:  # current_state == \"fission\"\n            if error_upper_bound < self.epsilon_target - self.hysteresis:\n                self.current_state = \"fusion\"\n\n        return self.current_state\n\n# Test the controller instantiation\ncontroller = DKWController()\nprint(f\"‚úÖ Controller created with target error rate: {controller.epsilon_target:.1%}\")\nprint(f\"   Initial state: {controller.current_state}\")\nprint(f\"   Confidence level: {1-controller.delta:.1%}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Evaluation Function\n\nThe core evaluation function computes several key metrics:\n\n- **Fusion/Fission Rates**: Percentage of decisions for each strategy\n- **Error Rate**: Percentage of predictions that resulted in errors\n- **API Calls**: Total API calls (fusion = 1 call, fission = 2 calls)\n- **Average Calls per Example**: Efficiency metric for API usage",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 2. DKW Controller Class\n\nThe `DKWController` implements an adaptive decision-making system that switches between \"fusion\" and \"fission\" modes based on observed error rates. It uses the **Dvoretzky-Kiefer-Wolfowitz inequality** to provide statistical confidence bounds.\n\n**Key Parameters:**\n- `epsilon_target`: Target error rate threshold (10%)\n- `delta`: Confidence level parameter (5%)\n- `min_samples`: Minimum samples before making decisions\n- `hysteresis`: Prevents oscillation between states",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import json\nimport numpy as np\n\n# Synthetic experimental data (normally would be loaded from external files)\n# This data represents the output of both baseline and proposed methods\n# Each prediction has a 'decision' (fusion/fission) and 'error' (boolean) field\n\n# Generate baseline data: 100% fission, 8% error rate, 200 examples\nbaseline_predictions = []\nfor i in range(200):\n    baseline_predictions.append({\n        \"decision\": \"fission\",  # Baseline always uses fission\n        \"error\": i < 16  # First 16 examples have errors (8% error rate)\n    })\n\n# Generate proposed data: 65% fusion, 35% fission, 9% error rate, 200 examples  \nproposed_predictions = []\nfor i in range(200):\n    if i < 130:  # First 130 are fusion (65%)\n        decision = \"fusion\"\n    else:  # Remaining 70 are fission (35%)\n        decision = \"fission\"\n    \n    proposed_predictions.append({\n        \"decision\": decision,\n        \"error\": i < 18  # First 18 examples have errors (9% error rate)\n    })\n\n# Combined results dictionary\nresults = {\n    \"baseline\": baseline_predictions,\n    \"proposed\": proposed_predictions\n}\n\nprint(f\"Baseline examples: {len(results['baseline'])}\")\nprint(f\"Proposed examples: {len(results['proposed'])}\")\nprint(f\"Sample baseline prediction: {results['baseline'][0]}\")\nprint(f\"Sample proposed prediction: {results['proposed'][0]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "\"\"\"DKW Controller Implementation.\"\"\"\nimport json\nimport numpy as np\nfrom dataclasses import dataclass, field\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Set random seed for reproducible results\nnp.random.seed(42)\n\nprint(\"‚úÖ Dependencies imported successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Import Dependencies",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Setup and Data\n\nFirst, let's import the required libraries and define our experimental data. The data represents predictions from both baseline and proposed methods, where each prediction includes a decision type (fusion/fission) and error status.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# DKW Controller Implementation Demo\n## Interactive Notebook for Fusion/Fission Decision Making\n\nThis notebook demonstrates a **DKW-guided fusion/fission controller** that makes adaptive decisions based on error observations with statistical guarantees. The controller uses the Dvoretzky-Kiefer-Wolfowitz (DKW) inequality to provide confidence bounds on empirical error rates.\n\n**Key Features:**\n- Adaptive fusion/fission decision making\n- Statistical confidence bounds using DKW inequality\n- Hysteresis to prevent decision oscillation\n- Self-contained demonstration with sample data",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# DKW Controller Evaluation\n\nThis notebook evaluates the performance of a DKW (Decision, Knowledge, Workflow) Controller by comparing baseline and proposed methods. The evaluation focuses on API call efficiency and error rates between fusion and fission decision strategies.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Display output in the same format as the original script\nprint(\"üìÑ Final results (first 5 examples):\")\nprint(\"=\" * 50)\n\n# Show sample of results in JSON format\nsample_results = {\n    \"baseline\": results[\"baseline\"][:5],\n    \"proposed\": results[\"proposed\"][:5]\n}\n\nprint(json.dumps(sample_results, indent=2))\n\nprint(f\"\\n... (showing 5 of {len(results['baseline'])} total results)\")\n\n# In the original script, this would be saved as:\n# with open(\"method_out.json\", \"w\") as f:\n#     json.dump(results, f, indent=2)\nprint(f\"\\nüíæ In the original script, {len(results['baseline'])} results would be saved to 'method_out.json'\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Summary & Output\n\nThis notebook demonstrates the DKW controller's ability to make statistically-grounded fusion/fission decisions. The controller provides guarantees about error rates while being more adaptive than a purely conservative baseline approach.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# üîß EXPERIMENT WITH DIFFERENT PARAMETERS\n# Modify these values and re-run this cell to see the effects\n\n# Controller parameters\nEPSILON_TARGET = 0.10    # Target error rate (try 0.05, 0.15, 0.20)\nDELTA = 0.05            # Confidence parameter (try 0.01, 0.10)\nMIN_SAMPLES = 50        # Minimum samples before decisions (try 30, 100, 200)\nHYSTERESIS = 0.02       # Hysteresis to prevent oscillation (try 0.01, 0.05, 0.10)\n\nprint(f\"üîß Testing with parameters:\")\nprint(f\"   Target error rate: {EPSILON_TARGET}\")\nprint(f\"   Confidence level: {1-DELTA:.1%}\")\nprint(f\"   Minimum samples: {MIN_SAMPLES}\")\nprint(f\"   Hysteresis: {HYSTERESIS}\")\nprint()\n\n# Create custom controller\ncustom_controller = DKWController(\n    epsilon_target=EPSILON_TARGET,\n    delta=DELTA,\n    min_samples=MIN_SAMPLES,\n    hysteresis=HYSTERESIS\n)\n\n# Run experiment with custom parameters\ncustom_results, final_custom_controller = run_experiment(sample_data)\n\n# Quick comparison\ncustom_df_proposed = pd.DataFrame(custom_results[\"proposed\"])\ncustom_fusion_rate = (custom_df_proposed[\"decision\"] == \"fusion\").mean()\ncustom_error_rate = custom_df_proposed[\"error\"].mean()\n\nprint(f\"üìä Results with custom parameters:\")\nprint(f\"   Fusion rate: {custom_fusion_rate:.1%}\")\nprint(f\"   Error rate: {custom_error_rate:.3f}\")\nprint(f\"   Final state: {final_custom_controller.current_state}\")\nprint(f\"   Samples collected: {len(final_custom_controller.samples)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Interactive Experimentation\n\nTry modifying the controller parameters below and re-running to see how they affect performance!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create visualizations\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n\n# Plot 1: Decision timeline\ndecisions_proposed = [1 if d == \"fusion\" else 0 for d in df_proposed[\"decision\"]]\ndecisions_baseline = [1 if d == \"fusion\" else 0 for d in df_baseline[\"decision\"]]\n\nax1.plot(decisions_proposed, label=\"DKW Controller\", alpha=0.7, linewidth=2)\nax1.plot(decisions_baseline, label=\"Baseline\", alpha=0.7, linewidth=2)\nax1.set_title(\"Decision Timeline (1=Fusion, 0=Fission)\")\nax1.set_xlabel(\"Sample Number\")\nax1.set_ylabel(\"Decision\")\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Plot 2: Running fusion rate\nwindow_size = 20\nrunning_fusion_proposed = pd.Series(decisions_proposed).rolling(window_size).mean()\nrunning_fusion_baseline = pd.Series(decisions_baseline).rolling(window_size).mean()\n\nax2.plot(running_fusion_proposed, label=\"DKW Controller\", alpha=0.7, linewidth=2)\nax2.plot(running_fusion_baseline, label=\"Baseline\", alpha=0.7, linewidth=2)\nax2.set_title(f\"Running Fusion Rate (Window={window_size})\")\nax2.set_xlabel(\"Sample Number\")\nax2.set_ylabel(\"Fusion Rate\")\nax2.legend()\nax2.grid(True, alpha=0.3)\n\n# Plot 3: Error rate over time\nerrors = [1 if e else 0 for e in df_proposed[\"error\"]]\nrunning_error = pd.Series(errors).rolling(window_size).mean()\n\nax3.plot(running_error, label=\"Running Error Rate\", color='red', alpha=0.7, linewidth=2)\nax3.axhline(y=final_controller.epsilon_target, color='orange', linestyle='--', \n            label=f\"Target ({final_controller.epsilon_target})\")\nax3.set_title(f\"Running Error Rate (Window={window_size})\")\nax3.set_xlabel(\"Sample Number\")\nax3.set_ylabel(\"Error Rate\")\nax3.legend()\nax3.grid(True, alpha=0.3)\n\n# Plot 4: DKW bound evolution\nsample_counts = range(1, len(final_controller.samples) + 1)\ndkw_bounds = [final_controller.dkw_epsilon(n) for n in sample_counts]\n\nax4.plot(dkw_bounds, label=\"DKW Œµ\", color='purple', alpha=0.7, linewidth=2)\nax4.set_title(\"DKW Confidence Bound Evolution\")\nax4.set_xlabel(\"Sample Number\")\nax4.set_ylabel(\"Œµ (Confidence Bound)\")\nax4.legend()\nax4.grid(True, alpha=0.3)\nax4.set_yscale('log')\n\nplt.tight_layout()\nplt.show()\n\n# Display final statistics\nprint(\"\\n=== FINAL STATISTICS ===\")\nprint(f\"DKW bound at end: {final_controller.dkw_epsilon(len(final_controller.samples)):.4f}\")\nprint(f\"Empirical error rate: {np.mean(final_controller.samples):.4f}\")\nprint(f\"Upper bound: {np.mean(final_controller.samples) + final_controller.dkw_epsilon(len(final_controller.samples)):.4f}\")\nprint(f\"Target threshold: {final_controller.epsilon_target}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Visualization\n\nLet's visualize how the DKW controller's decisions evolve over time compared to the baseline.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Analyze results\ndef analyze_results(results):\n    \"\"\"Analyze and display experiment results.\"\"\"\n    \n    # Convert to DataFrames for easier analysis\n    df_baseline = pd.DataFrame(results[\"baseline\"])\n    df_proposed = pd.DataFrame(results[\"proposed\"])\n    \n    # Calculate metrics\n    baseline_fusion_rate = (df_baseline[\"decision\"] == \"fusion\").mean()\n    proposed_fusion_rate = (df_proposed[\"decision\"] == \"fusion\").mean()\n    \n    baseline_error_rate = df_baseline[\"error\"].mean()\n    proposed_error_rate = df_proposed[\"error\"].mean()\n    \n    print(\"=== PERFORMANCE COMPARISON ===\")\n    print(f\"Baseline (always fission):\")\n    print(f\"  - Fusion rate: {baseline_fusion_rate:.1%}\")\n    print(f\"  - Error rate: {baseline_error_rate:.3f}\")\n    print()\n    print(f\"Proposed (DKW controller):\")\n    print(f\"  - Fusion rate: {proposed_fusion_rate:.1%}\")\n    print(f\"  - Error rate: {proposed_error_rate:.3f}\")\n    print()\n    print(f\"Improvement in fusion rate: {proposed_fusion_rate - baseline_fusion_rate:.1%}\")\n    \n    return df_baseline, df_proposed\n\ndf_baseline, df_proposed = analyze_results(results)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Results Analysis\n\nLet's analyze the performance of both approaches and visualize the results.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def run_experiment(data):\n    \"\"\"Run DKW controller experiment with inline data.\"\"\"\n    controller = DKWController()\n    results = {\"baseline\": [], \"proposed\": []}\n\n    for example in data:\n        # Simulate error occurrence based on difficulty\n        error = np.random.random() < example[\"difficulty\"]\n        controller.add_observation(float(error))\n        decision = controller.decide()\n\n        results[\"proposed\"].append({\n            \"id\": example[\"id\"],\n            \"decision\": decision,\n            \"error\": error,\n        })\n        results[\"baseline\"].append({\n            \"id\": example[\"id\"],\n            \"decision\": \"fission\",  # Always conservative\n            \"error\": error,\n        })\n\n    return results, controller\n\n# Run the experiment\nresults, final_controller = run_experiment(sample_data)\n\nprint(f\"Experiment completed!\")\nprint(f\"Total samples processed: {len(final_controller.samples)}\")\nprint(f\"Final controller state: {final_controller.current_state}\")\nprint(f\"Final empirical error rate: {np.mean(final_controller.samples):.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Experiment Function\n\nThis function runs the DKW controller experiment, comparing the proposed method against a baseline that always chooses the conservative \"fission\" option.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Inline sample data (replaces reading from JSON file)\nsample_data = [\n    {\"id\": \"example_000\", \"difficulty\": 0.05},\n    {\"id\": \"example_001\", \"difficulty\": 0.03},\n    {\"id\": \"example_002\", \"difficulty\": 0.15},\n    {\"id\": \"example_003\", \"difficulty\": 0.08},\n    {\"id\": \"example_004\", \"difficulty\": 0.12},\n    {\"id\": \"example_005\", \"difficulty\": 0.02},\n    {\"id\": \"example_006\", \"difficulty\": 0.18},\n    {\"id\": \"example_007\", \"difficulty\": 0.06},\n    {\"id\": \"example_008\", \"difficulty\": 0.14},\n    {\"id\": \"example_009\", \"difficulty\": 0.04},\n] * 15  # Repeat to get 150 samples for meaningful statistics\n\nprint(f\"Created {len(sample_data)} sample examples\")\nprint(f\"Difficulty range: {min(ex['difficulty'] for ex in sample_data):.3f} to {max(ex['difficulty'] for ex in sample_data):.3f}\")\nprint(f\"Average difficulty: {np.mean([ex['difficulty'] for ex in sample_data]):.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Sample Data\n\nInstead of reading from external files, we'll create sample data inline. This represents different examples with varying difficulty levels.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "@dataclass\nclass DKWController:\n    \"\"\"DKW-guided fusion/fission controller.\"\"\"\n    epsilon_target: float = 0.10    # Target error rate threshold\n    delta: float = 0.05             # Confidence parameter (95% confidence)\n    min_samples: int = 100           # Minimum samples before making decisions\n    hysteresis: float = 0.05         # Prevents oscillation between states\n\n    samples: list = field(default_factory=list)\n    current_state: str = \"fission\"   # Start conservative\n\n    def dkw_epsilon(self, n: int) -> float:\n        \"\"\"Compute DKW epsilon for n samples.\"\"\"\n        if n < 2:\n            return 1.0\n        return np.sqrt(np.log(2 / self.delta) / (2 * n))\n\n    def add_observation(self, error: float) -> None:\n        \"\"\"Add error observation for calibration.\"\"\"\n        self.samples.append(error)\n\n    def decide(self) -> str:\n        \"\"\"Make fusion/fission decision with DKW guarantee.\"\"\"\n        n = len(self.samples)\n        if n < self.min_samples:\n            return self.current_state\n\n        epsilon = self.dkw_epsilon(n)\n        empirical_error = np.mean(self.samples[-self.min_samples:])\n        error_upper_bound = empirical_error + epsilon\n\n        if self.current_state == \"fusion\":\n            if error_upper_bound > self.epsilon_target + self.hysteresis:\n                self.current_state = \"fission\"\n        else:\n            if error_upper_bound < self.epsilon_target - self.hysteresis:\n                self.current_state = \"fusion\"\n\n        return self.current_state\n\n# Test the controller\ncontroller = DKWController()\nprint(f\"Initial state: {controller.current_state}\")\nprint(f\"Target error rate: {controller.epsilon_target}\")\nprint(f\"Confidence level: {1-controller.delta:.1%}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## DKW Controller Class\n\nThe DKW Controller uses the Dvoretzky-Kiefer-Wolfowitz inequality to provide statistical guarantees when making fusion/fission decisions based on observed error rates.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\"\"\"Required imports and setup.\"\"\"\nimport json\nimport numpy as np\nfrom dataclasses import dataclass, field\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Set random seed for reproducible results\nnp.random.seed(42)\n\nprint(\"‚úÖ All imports loaded successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# DKW Controller Implementation\n\nThis notebook demonstrates a DKW (Dvoretzky-Kiefer-Wolfowitz) guided fusion/fission controller for adaptive decision making with statistical guarantees.\n\n## Overview\n- **DKW Controller**: Uses statistical bounds to make fusion/fission decisions\n- **Self-contained**: All data is inline, no external files needed\n- **Interactive**: Modify parameters and see results immediately",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Save results to JSON file (equivalent to the original script's output)\n# This replaces the original: json.dump(results, f, indent=2)\n\noutput_results = {\n    \"baseline\": [\n        {k: v for k, v in r.items() if k != \"difficulty\"}  # Remove difficulty for clean output\n        for r in results[\"baseline\"]\n    ],\n    \"proposed\": [\n        {k: v for k, v in r.items() if k != \"difficulty\"}  # Remove difficulty for clean output\n        for r in results[\"proposed\"]  \n    ]\n}\n\n# Display first few results (instead of writing to file)\nprint(\"üìÑ Sample output (first 5 results):\")\nprint(\"=\" * 40)\n\nsample_output = {\n    \"baseline\": output_results[\"baseline\"][:3],\n    \"proposed\": output_results[\"proposed\"][:3]\n}\n\nprint(json.dumps(sample_output, indent=2))\n\n# Uncomment the lines below if you want to save to an actual file:\n# with open(\"method_out.json\", \"w\") as f:\n#     json.dump(output_results, f, indent=2)\n# print(\"üíæ Results saved to method_out.json\")\n\nprint(f\"\\\\n‚úÖ Notebook demonstration complete!\")\nprint(f\"üìà Total examples: {len(results['baseline'])}\")\nprint(f\"üîÑ Decision changes: {len(decision_changes)}\")\nprint(f\"üéØ Controller successfully balanced fusion/fission decisions with DKW guarantees!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Save Results (Optional)\n\nThe original script saved results to a JSON file. Here's how you can do the same if needed:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def interactive_experiment(epsilon_target=0.08, delta=0.05, min_samples=80, hysteresis=0.03):\n    \"\"\"Run experiment with custom parameters.\"\"\"\n    print(f\"üß™ Running experiment with custom parameters:\")\n    print(f\"   epsilon_target={epsilon_target}, delta={delta}\")\n    print(f\"   min_samples={min_samples}, hysteresis={hysteresis}\")\n    print()\n    \n    # Create custom controller\n    class CustomDKWController(DKWController):\n        def __init__(self):\n            super().__init__(\n                epsilon_target=epsilon_target,\n                delta=delta, \n                min_samples=min_samples,\n                hysteresis=hysteresis\n            )\n    \n    # Run experiment with custom controller  \n    original_class = DKWController\n    global DKWController\n    DKWController = CustomDKWController\n    \n    try:\n        custom_results, custom_changes = run_experiment(sample_data[:150])  # Smaller subset for speed\n        \n        # Quick analysis\n        custom_errors = sum(1 for r in custom_results[\"proposed\"] if r[\"error\"])\n        custom_error_rate = custom_errors / len(custom_results[\"proposed\"])\n        custom_fusion_count = sum(1 for r in custom_results[\"proposed\"] if r[\"decision\"] == \"fusion\")\n        \n        print(f\"üìä RESULTS:\")\n        print(f\"   Error rate: {custom_error_rate:.1%}\")\n        print(f\"   Fusion rate: {100*custom_fusion_count/len(custom_results['proposed']):.1f}%\")\n        print(f\"   Mode switches: {len(custom_changes)}\")\n        \n        return custom_results, custom_changes\n        \n    finally:\n        DKWController = original_class\n\n# Try some different parameter combinations\nprint(\"=\" * 60)\nprint(\"üéõÔ∏è  PARAMETER EXPLORATION\")\nprint(\"=\" * 60)\n\nprint(\"\\\\n1Ô∏è‚É£ More aggressive (lower error threshold):\")\ninteractive_experiment(epsilon_target=0.06, min_samples=60)\n\nprint(\"\\\\n2Ô∏è‚É£ More conservative (higher error threshold):\")  \ninteractive_experiment(epsilon_target=0.15, min_samples=120)\n\nprint(\"\\\\n3Ô∏è‚É£ Faster adaptation (fewer samples required):\")\ninteractive_experiment(min_samples=50, hysteresis=0.02)\n\nprint(\"\\\\n4Ô∏è‚É£ More stable (higher hysteresis):\")\ninteractive_experiment(hysteresis=0.08, min_samples=100)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Interactive Experiment\n\nNow you can try different controller parameters! Modify the values below and re-run the experiment to see how they affect performance.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create visualizations\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n\n# Plot 1: Decision timeline\nsteps = list(range(len(results[\"proposed\"])))\ndecisions = [1 if r[\"decision\"] == \"fusion\" else 0 for r in results[\"proposed\"]]\nerrors = [1 if r[\"error\"] else 0 for r in results[\"proposed\"]]\n\nax1.plot(steps, decisions, label=\"Decision (1=fusion, 0=fission)\", color=\"blue\", alpha=0.7)\nax1.scatter([i for i, e in enumerate(errors) if e], [decisions[i] for i, e in enumerate(errors) if e], \n           color=\"red\", alpha=0.6, s=20, label=\"Errors\")\nax1.set_xlabel(\"Step\")\nax1.set_ylabel(\"Decision / Error\")\nax1.set_title(\"Decision Timeline with Errors\")\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Plot 2: Cumulative error rate\nbaseline_cumulative = np.cumsum([r[\"error\"] for r in results[\"baseline\"]]) / np.arange(1, len(results[\"baseline\"]) + 1)\nproposed_cumulative = np.cumsum([r[\"error\"] for r in results[\"proposed\"]]) / np.arange(1, len(results[\"proposed\"]) + 1)\n\nax2.plot(steps, baseline_cumulative, label=\"Baseline (always fission)\", color=\"orange\", alpha=0.8)\nax2.plot(steps, proposed_cumulative, label=\"Proposed (DKW controller)\", color=\"blue\", alpha=0.8)\nax2.axhline(y=0.10, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"Target (10%)\")\nax2.set_xlabel(\"Step\")\nax2.set_ylabel(\"Cumulative Error Rate\")\nax2.set_title(\"Cumulative Error Rate Comparison\")\nax2.legend()\nax2.grid(True, alpha=0.3)\n\n# Plot 3: Error rate by difficulty\ndf_proposed = pd.DataFrame(results[\"proposed\"])\ndf_baseline = pd.DataFrame(results[\"baseline\"])\n\ndifficulty_bins = np.linspace(0, 1, 11)\nbin_centers = (difficulty_bins[:-1] + difficulty_bins[1:]) / 2\n\nbaseline_binned = []\nproposed_binned = []\n\nfor i in range(len(difficulty_bins)-1):\n    mask_baseline = (df_baseline[\"difficulty\"] >= difficulty_bins[i]) & (df_baseline[\"difficulty\"] < difficulty_bins[i+1])\n    mask_proposed = (df_proposed[\"difficulty\"] >= difficulty_bins[i]) & (df_proposed[\"difficulty\"] < difficulty_bins[i+1])\n    \n    if mask_baseline.sum() > 0:\n        baseline_binned.append(df_baseline[mask_baseline][\"error\"].mean())\n    else:\n        baseline_binned.append(0)\n        \n    if mask_proposed.sum() > 0:\n        proposed_binned.append(df_proposed[mask_proposed][\"error\"].mean())\n    else:\n        proposed_binned.append(0)\n\nax3.bar(bin_centers - 0.02, baseline_binned, width=0.04, alpha=0.7, color=\"orange\", label=\"Baseline\")\nax3.bar(bin_centers + 0.02, proposed_binned, width=0.04, alpha=0.7, color=\"blue\", label=\"Proposed\")\nax3.set_xlabel(\"Difficulty Level\")\nax3.set_ylabel(\"Error Rate\")\nax3.set_title(\"Error Rate by Difficulty Level\")\nax3.legend()\nax3.grid(True, alpha=0.3)\n\n# Plot 4: Decision distribution over time (rolling window)\nwindow_size = 50\nfusion_rolling = pd.Series(decisions).rolling(window=window_size, min_periods=1).mean()\nax4.plot(steps, fusion_rolling, color=\"purple\", alpha=0.8, linewidth=2)\nax4.axhline(y=0.5, color=\"gray\", linestyle=\"--\", alpha=0.7, label=\"50% fusion\")\nax4.set_xlabel(\"Step\")\nax4.set_ylabel(\"Fusion Rate (rolling avg)\")\nax4.set_title(f\"Fusion Rate Over Time (window={window_size})\")\nax4.legend()\nax4.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## How to Use with Your Own Data\n\nTo use this notebook with your own evaluation data:\n\n1. **Replace the sample data generation**: Modify the second code cell to load your actual data instead of generating sample data.\n\n2. **Expected data format**: Your data should be a dictionary with this structure:\n   ```python\n   {\n       \"baseline\": [\n           {\"decision\": \"fusion\" or \"fission\", \"error\": True or False},\n           # ... more examples\n       ],\n       \"proposed\": [\n           {\"decision\": \"fusion\" or \"fission\", \"error\": True or False},\n           # ... more examples\n       ]\n   }\n   ```\n\n3. **Loading from files**: If you have JSON files, replace the sample data section with:\n   ```python\n   with open(\"your_results_file.json\") as f:\n       results = json.load(f)\n   ```\n\n4. **Customizing metrics**: Modify the `compute_metrics` function if you need different evaluation metrics or have different cost models for fusion/fission operations.\n\nThe rest of the notebook will automatically work with your data!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\n\n# Create visualizations\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 10))\nfig.suptitle('DKW Controller Evaluation Results', fontsize=16, fontweight='bold')\n\n# 1. Decision Types Comparison\nmethods = ['Baseline', 'Proposed']\nfusion_rates = [metrics['baseline']['fusion_rate'], metrics['proposed']['fusion_rate']]\nfission_rates = [metrics['baseline']['fission_rate'], metrics['proposed']['fission_rate']]\n\nx = range(len(methods))\nwidth = 0.35\n\nax1.bar([i - width/2 for i in x], fusion_rates, width, label='Fusion', color='skyblue')\nax1.bar([i + width/2 for i in x], fission_rates, width, label='Fission', color='lightcoral')\nax1.set_ylabel('Rate')\nax1.set_title('Decision Type Distribution')\nax1.set_xticks(x)\nax1.set_xticklabels(methods)\nax1.legend()\n\n# 2. Error Rates\nerror_rates = [metrics['baseline']['error_rate'], metrics['proposed']['error_rate']]\nax2.bar(methods, error_rates, color=['orange', 'red'], alpha=0.7)\nax2.set_ylabel('Error Rate')\nax2.set_title('Error Rate Comparison')\nax2.set_ylim(0, max(error_rates) * 1.2)\n\n# 3. API Calls per Example\napi_calls = [metrics['baseline']['avg_calls_per_example'], metrics['proposed']['avg_calls_per_example']]\nbars = ax3.bar(methods, api_calls, color=['lightblue', 'lightgreen'])\nax3.set_ylabel('Average API Calls per Example')\nax3.set_title('API Efficiency')\n\n# Add value labels on bars\nfor bar, value in zip(bars, api_calls):\n    height = bar.get_height()\n    ax3.text(bar.get_x() + bar.get_width()/2., height,\n             f'{value:.2f}', ha='center', va='bottom')\n\n# 4. Summary metrics\nsummary_labels = ['API Reduction\\n(%)', 'Error Rate\\nDifference (%)']\nsummary_values = [metrics['improvement']['api_reduction_pct'], \n                 metrics['improvement']['error_rate_diff'] * 100]\ncolors = ['green' if v > 0 else 'red' for v in summary_values]\n\nbars = ax4.bar(summary_labels, summary_values, color=colors, alpha=0.7)\nax4.set_title('Improvement Summary')\nax4.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n\n# Add value labels\nfor bar, value in zip(bars, summary_values):\n    height = bar.get_height()\n    ax4.text(bar.get_x() + bar.get_width()/2., height + (1 if height >= 0 else -1),\n             f'{value:.1f}%', ha='center', va='bottom' if height >= 0 else 'top')\n\nplt.tight_layout()\nplt.show()\n\n# Print key insights\nprint(\"KEY INSIGHTS:\")\nprint(f\"‚úì The proposed method achieves {metrics['improvement']['api_reduction_pct']:.1f}% reduction in API calls\")\nprint(f\"‚úì Fusion rate increased from {metrics['baseline']['fusion_rate']:.0%} to {metrics['proposed']['fusion_rate']:.0%}\")\nprint(f\"‚úì Error rate changed by {metrics['improvement']['error_rate_diff']:.1%} (slight increase)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Visualization\n\nLet's create some plots to visualize the controller's behavior over time.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Usage Notes & Customization\n\n### How to use this notebook:\n1. **Self-contained**: This notebook runs without any external files or dependencies\n2. **Customizable**: Modify the `simulated_dataset` to test with your own questions\n3. **Extensible**: Add new fields to the output format by modifying the `collect_data()` function\n\n### Original vs Notebook differences:\n- **Original**: Loads data from HuggingFace datasets library\n- **Notebook**: Uses inline sample data for demonstration\n- **Original**: Saves output to `data_out.json` file  \n- **Notebook**: Displays output directly in cells\n\n### To restore original functionality:\n1. Install dependencies: `pip install datasets`\n2. Uncomment the HuggingFace dataset loading code\n3. Add file writing functionality back if needed",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Analyze results\nbaseline_errors = sum(1 for r in results[\"baseline\"] if r[\"error\"])\nproposed_errors = sum(1 for r in results[\"proposed\"] if r[\"error\"])\n\nbaseline_error_rate = baseline_errors / len(results[\"baseline\"])\nproposed_error_rate = proposed_errors / len(results[\"proposed\"])\n\n# Count fusion vs fission decisions for proposed method\nfusion_count = sum(1 for r in results[\"proposed\"] if r[\"decision\"] == \"fusion\")\nfission_count = sum(1 for r in results[\"proposed\"] if r[\"decision\"] == \"fission\")\n\nprint(\"üìà EXPERIMENT RESULTS\")\nprint(\"=\" * 50)\nprint(f\"Total examples processed: {len(results['baseline'])}\")\nprint()\nprint(\"üî∏ BASELINE (always fission):\")\nprint(f\"   Error rate: {baseline_error_rate:.1%} ({baseline_errors}/{len(results['baseline'])})\")\nprint(f\"   Fusion decisions: 0/{len(results['baseline'])} (0%)\")\nprint()\nprint(\"üîπ PROPOSED (DKW controller):\")\nprint(f\"   Error rate: {proposed_error_rate:.1%} ({proposed_errors}/{len(results['proposed'])})\")\nprint(f\"   Fusion decisions: {fusion_count}/{len(results['proposed'])} ({100*fusion_count/len(results['proposed']):.1f}%)\")\nprint(f\"   Fission decisions: {fission_count}/{len(results['proposed'])} ({100*fission_count/len(results['proposed']):.1f}%)\")\nprint()\nprint(\"üîÑ DECISION CHANGES:\")\nprint(f\"   Mode switches: {len(decision_changes)}\")\nfor change in decision_changes:\n    print(f\"   Step {change['step']}: {change['from']} ‚Üí {change['to']} (upper bound: {change['stats']['upper_bound']:.3f})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Visualization\n\nCreate visualizations to better understand the performance comparison between baseline and proposed methods.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Expected output format (from original data_out.json)\nexpected_output = [\n    {\n        \"id\": \"example_000\",\n        \"question\": \"What is 2+2?\",\n        \"answer\": \"4\",\n        \"difficulty\": 0.15\n    },\n    {\n        \"id\": \"example_001\", \n        \"question\": \"If x=5, what is 2x?\",\n        \"answer\": \"10\",\n        \"difficulty\": 0.22\n    },\n    {\n        \"id\": \"example_002\",\n        \"question\": \"Solve: 3y + 6 = 15\",\n        \"answer\": \"y=3\",\n        \"difficulty\": 0.28\n    }\n]\n\nprint(\"Expected output format:\")\nprint(json.dumps(expected_output, indent=2))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Results Analysis\n\nLet's analyze the performance of our DKW controller compared to the baseline approach.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Save results to JSON file (equivalent to the original script)\nwith open(\"eval_out.json\", \"w\") as f:\n    json.dump(metrics, f, indent=2)\n\nprint(\"Results saved to eval_out.json\")\n\n# Display the JSON content for verification\nprint(\"\\nSaved JSON content:\")\nprint(json.dumps(metrics, indent=2))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def run_experiment(data, verbose=False):\n    \"\"\"Run DKW controller experiment with inline data.\"\"\"\n    controller = DKWController()\n    results = {\"baseline\": [], \"proposed\": []}\n    \n    if verbose:\n        print(\"üöÄ Starting experiment...\")\n        print(f\"Controller settings: target={controller.epsilon_target}, min_samples={controller.min_samples}\")\n    \n    decision_changes = []\n    \n    for i, example in enumerate(data):\n        # Simulate error occurrence based on difficulty\n        error = np.random.random() < example[\"difficulty\"]\n        controller.add_observation(float(error))\n        decision = controller.decide()\n        \n        # Track decision changes for analysis\n        if i > 0 and decision != results[\"proposed\"][-1][\"decision\"]:\n            stats = controller.get_stats()\n            decision_changes.append({\n                \"step\": i,\n                \"from\": results[\"proposed\"][-1][\"decision\"],\n                \"to\": decision,\n                \"stats\": stats\n            })\n\n        results[\"proposed\"].append({\n            \"id\": example[\"id\"],\n            \"decision\": decision,\n            \"error\": error,\n            \"difficulty\": example[\"difficulty\"]\n        })\n        results[\"baseline\"].append({\n            \"id\": example[\"id\"],\n            \"decision\": \"fission\",  # Always conservative\n            \"error\": error,\n            \"difficulty\": example[\"difficulty\"]\n        })\n        \n        if verbose and i % 50 == 0:\n            stats = controller.get_stats()\n            print(f\"  Step {i}: {stats['samples']} samples, error rate: {stats['empirical_error']:.3f}, mode: {stats['current_state']}\")\n\n    if verbose:\n        print(f\"‚úÖ Experiment complete! Decision changes: {len(decision_changes)}\")\n    \n    return results, decision_changes\n\n# Run the experiment\nprint(\"üî¨ Running experiment...\")\nresults, decision_changes = run_experiment(sample_data, verbose=True)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Expected Output Reference\n\nFor comparison, here's the expected output structure that was provided in the original specification:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Save Results\n\nSave the evaluation metrics to a JSON file (as in the original script).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Display the complete processed dataset\nprint(\"Complete processed dataset:\")\nprint(json.dumps(data, indent=2))\n\n# In the original script, this would be saved to a file:\n# with open(\"data_out.json\", \"w\") as f:\n#     json.dump(data, f, indent=2)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Compute the evaluation metrics\nmetrics = compute_metrics(results)\n\n# Display the key result (as in the original script)\nprint(f\"API reduction: {metrics['improvement']['api_reduction_pct']:.1f}%\")\nprint()\n\n# Display detailed metrics in a nice format\nprint(\"Detailed Evaluation Results:\")\nprint(\"=\" * 50)\n\nfor method in [\"baseline\", \"proposed\"]:\n    print(f\"\\n{method.upper()} METHOD:\")\n    m = metrics[method]\n    print(f\"  Fusion rate:           {m['fusion_rate']:.1%}\")\n    print(f\"  Fission rate:          {m['fission_rate']:.1%}\")\n    print(f\"  Error rate:            {m['error_rate']:.1%}\")\n    print(f\"  Total API calls:       {m['api_calls']}\")\n    print(f\"  Avg calls per example: {m['avg_calls_per_example']:.2f}\")\n\nprint(f\"\\nIMPROVEMENT:\")\nimp = metrics['improvement']\nprint(f\"  API reduction:         {imp['api_reduction_pct']:.1f}%\")\nprint(f\"  Error rate difference: {imp['error_rate_diff']:+.1%}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## View Complete Dataset\n\nLet's examine the complete processed dataset structure that would normally be saved to `data_out.json`.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Experiment Function\n\nThe experiment simulates running both the DKW controller (proposed method) and a baseline that always uses fission mode. Errors occur probabilistically based on each example's difficulty level.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Execute the data collection\ndata = collect_data()\n\nprint(f\"Collected {len(data)} examples\")\nprint(\"\\nFirst few examples:\")\nfor item in data[:3]:\n    print(f\"- ID: {item['id']}\")\n    print(f\"  Question: {item['question']}\")\n    print(f\"  Answer: {item['answer']}\")\n    print(f\"  Difficulty: {item['difficulty']:.2f}\")\n    print()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Compute Metrics\n\nRun the evaluation on our sample data and display the results.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Sample dataset - inlined instead of reading from file\n# This replaces the original: with open(\"../dataset_001/data_out.json\") as f: data = json.load(f)\n\nsample_data = [\n    {\"id\": f\"example_{i:03d}\", \"difficulty\": 0.05 + 0.15 * np.random.random()}\n    for i in range(200)\n]\n\n# Add some high-difficulty examples to test mode switching\nfor i in range(50):\n    sample_data.append({\n        \"id\": f\"hard_example_{i:03d}\", \n        \"difficulty\": 0.3 + 0.4 * np.random.random()\n    })\n\nprint(f\"üìä Created {len(sample_data)} sample examples\")\nprint(f\"üìà Difficulty range: {min(ex['difficulty'] for ex in sample_data):.3f} - {max(ex['difficulty'] for ex in sample_data):.3f}\")\n\n# Show first few examples\nprint(\"\\nüîç First 5 examples:\")\nfor i, ex in enumerate(sample_data[:5]):\n    print(f\"  {ex['id']}: difficulty = {ex['difficulty']:.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Execute Data Collection\n\nLet's run the data collection function and examine the results.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def compute_metrics(results: dict) -> dict:\n    \"\"\"Compute evaluation metrics.\"\"\"\n    metrics = {}\n\n    for method in [\"baseline\", \"proposed\"]:\n        preds = results[method]\n\n        # Count decisions\n        fusion_count = sum(1 for p in preds if p[\"decision\"] == \"fusion\")\n        fission_count = sum(1 for p in preds if p[\"decision\"] == \"fission\")\n\n        # Compute error rate\n        errors = sum(1 for p in preds if p[\"error\"])\n        error_rate = errors / len(preds)\n\n        # API calls (fusion=1, fission=2)\n        api_calls = fusion_count + 2 * fission_count\n\n        metrics[method] = {\n            \"fusion_rate\": fusion_count / len(preds),\n            \"fission_rate\": fission_count / len(preds),\n            \"error_rate\": error_rate,\n            \"api_calls\": api_calls,\n            \"avg_calls_per_example\": api_calls / len(preds),\n        }\n\n    # Compute improvement\n    baseline_calls = metrics[\"baseline\"][\"avg_calls_per_example\"]\n    proposed_calls = metrics[\"proposed\"][\"avg_calls_per_example\"]\n    metrics[\"improvement\"] = {\n        \"api_reduction_pct\": (baseline_calls - proposed_calls) / baseline_calls * 100,\n        \"error_rate_diff\": metrics[\"proposed\"][\"error_rate\"] - metrics[\"baseline\"][\"error_rate\"],\n    }\n\n    return metrics\n\nprint(\"Evaluation function defined successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def collect_data() -> List[Dict[str, Any]]:\n    \"\"\"Collect benchmark data for DKW controller evaluation.\"\"\"\n    \n    # In the original script, this would be:\n    # ds = load_dataset(\"gsm8k\", \"main\", split=\"test[:200]\")\n    # Here we use our simulated dataset instead\n    ds = simulated_dataset\n\n    data = []\n    for i, example in enumerate(ds):\n        processed_item = {\n            \"id\": f\"example_{i:03d}\",\n            \"question\": example[\"question\"],\n            \"answer\": example[\"answer\"],\n            \"difficulty\": len(example[\"question\"]) / 100,  # Simple proxy for difficulty\n        }\n        data.append(processed_item)\n\n    return data",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Sample Data\n\nInstead of reading from external files, we'll create sample data inline. The data represents examples with varying difficulty levels that influence error probability.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Evaluation Function\n\nThe `compute_metrics` function calculates performance metrics for both baseline and proposed methods, including fusion/fission rates, error rates, API call counts, and improvement percentages.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Data Processing Function\n\nThe `collect_data()` function processes the raw dataset and adds additional metadata like difficulty scoring based on question length.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "@dataclass\nclass DKWController:\n    \"\"\"DKW-guided fusion/fission controller.\"\"\"\n    epsilon_target: float = 0.10\n    delta: float = 0.05\n    min_samples: int = 100\n    hysteresis: float = 0.05\n\n    samples: list = field(default_factory=list)\n    current_state: str = \"fission\"\n\n    def dkw_epsilon(self, n: int) -> float:\n        \"\"\"Compute DKW epsilon for n samples.\"\"\"\n        if n < 2:\n            return 1.0\n        return np.sqrt(np.log(2 / self.delta) / (2 * n))\n\n    def add_observation(self, error: float) -> None:\n        \"\"\"Add error observation for calibration.\"\"\"\n        self.samples.append(error)\n\n    def decide(self) -> str:\n        \"\"\"Make fusion/fission decision with DKW guarantee.\"\"\"\n        n = len(self.samples)\n        if n < self.min_samples:\n            return self.current_state\n\n        epsilon = self.dkw_epsilon(n)\n        empirical_error = np.mean(self.samples[-self.min_samples:])\n        error_upper_bound = empirical_error + epsilon\n\n        if self.current_state == \"fusion\":\n            if error_upper_bound > self.epsilon_target + self.hysteresis:\n                self.current_state = \"fission\"\n        else:\n            if error_upper_bound < self.epsilon_target - self.hysteresis:\n                self.current_state = \"fusion\"\n\n        return self.current_state\n    \n    def get_stats(self):\n        \"\"\"Get current controller statistics.\"\"\"\n        n = len(self.samples)\n        if n == 0:\n            return {\"samples\": 0, \"empirical_error\": 0, \"epsilon\": 1.0, \"upper_bound\": 1.0}\n        \n        empirical_error = np.mean(self.samples[-self.min_samples:]) if n >= self.min_samples else np.mean(self.samples)\n        epsilon = self.dkw_epsilon(n)\n        \n        return {\n            \"samples\": n,\n            \"empirical_error\": empirical_error,\n            \"epsilon\": epsilon,\n            \"upper_bound\": empirical_error + epsilon,\n            \"current_state\": self.current_state\n        }\n\nprint(\"‚úÖ DKWController class defined!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Simulated GSM8K dataset samples (normally loaded from HuggingFace)\nsimulated_dataset = [\n    {\n        \"question\": \"What is 2+2?\",\n        \"answer\": \"4\"\n    },\n    {\n        \"question\": \"If x=5, what is 2x?\", \n        \"answer\": \"10\"\n    },\n    {\n        \"question\": \"Solve: 3y + 6 = 15\",\n        \"answer\": \"y=3\"\n    },\n    {\n        \"question\": \"A store sells apples for $3 per pound. How much do 4 pounds cost?\",\n        \"answer\": \"$12\"\n    },\n    {\n        \"question\": \"If a rectangle has length 8 and width 6, what is its area?\",\n        \"answer\": \"48\"\n    }\n]\n\nprint(f\"Loaded {len(simulated_dataset)} sample questions\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import json\nimport numpy as np\n\n# Sample data that produces the expected evaluation metrics\n# This replaces reading from \"../experiment_001/method_out.json\"\n\n# Generate sample baseline data: 100% fission, 8% error rate\nbaseline_data = []\nfor i in range(200):\n    error = i < 16  # First 16 examples have errors (8% error rate)\n    baseline_data.append({\n        \"decision\": \"fission\",  # 100% fission rate\n        \"error\": error\n    })\n\n# Generate sample proposed data: 65% fusion, 35% fission, 9% error rate\nproposed_data = []\nfor i in range(200):\n    decision = \"fusion\" if i < 130 else \"fission\"  # 65% fusion, 35% fission\n    error = i < 18  # First 18 examples have errors (9% error rate)\n    proposed_data.append({\n        \"decision\": decision,\n        \"error\": error\n    })\n\n# Combine into the expected format\nresults = {\n    \"baseline\": baseline_data,\n    \"proposed\": proposed_data\n}\n\nprint(f\"Generated sample data:\")\nprint(f\"- Baseline: {len(results['baseline'])} examples\")\nprint(f\"- Proposed: {len(results['proposed'])} examples\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## DKW Controller Class\n\nThe `DKWController` uses the Dvoretzky-Kiefer-Wolfowitz inequality to provide statistical guarantees on error rate estimates. \n\n### Key Parameters:\n- **`epsilon_target`**: Target error rate threshold (default: 0.10)\n- **`delta`**: Confidence level parameter for DKW bound (default: 0.05)  \n- **`min_samples`**: Minimum samples before making decisions (default: 100)\n- **`hysteresis`**: Prevents rapid mode switching (default: 0.05)\n\n### DKW Inequality:\nFor n samples, the true error rate is within `empirical_error ¬± epsilon` with probability ‚â• 1-Œ¥, where:\n```\nepsilon = sqrt(log(2/Œ¥) / (2*n))\n```",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Sample Dataset\n\nSince this is a self-contained notebook, we'll simulate the GSM8K dataset with sample data instead of loading from HuggingFace. In the original script, this would be loaded using `load_dataset(\"gsm8k\", \"main\", split=\"test[:200]\")`.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Sample Data\n\nThe original script reads from `../experiment_001/method_out.json`. For this self-contained notebook, we'll inline the sample data that would produce the expected evaluation results.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\"\"\"Dataset collection script for DKW benchmark.\"\"\"\nimport json\nfrom typing import List, Dict, Any\n\n# Note: In the original script, this would be: from datasets import load_dataset\n# For this self-contained notebook, we'll use inline data instead",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "\"\"\"DKW Controller Implementation - Imports and Setup\"\"\"\nimport json\nimport numpy as np\nfrom dataclasses import dataclass, field\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\nprint(\"üì¶ All packages imported successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# DKW Controller Evaluation\n\nThis notebook contains the evaluation script for the DKW Controller, converted from `eval.py` into an interactive format. The notebook analyzes the performance of two methods (baseline and proposed) by computing various metrics including API call reduction and error rates.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Import Dependencies\n\nFirst, let's import the required libraries for data processing.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# DKW Controller Implementation - Interactive Demo\n\nThis notebook demonstrates a **DKW-guided fusion/fission controller** implementation. The controller uses the Dvoretzky-Kiefer-Wolfowitz (DKW) inequality to make statistically guaranteed decisions between fusion and fission modes based on observed error rates.\n\n## Overview\n- **Fusion mode**: Aggressive strategy that may have higher error rates but better performance\n- **Fission mode**: Conservative strategy with lower error rates but potentially reduced performance\n- **DKW guarantee**: Statistical bound ensuring our error estimates are reliable\n\nThe controller switches between modes based on observed error rates with statistical confidence bounds.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# Dataset Collection for DKW Benchmark\n\nThis notebook demonstrates the dataset collection script for DKW controller evaluation. The script processes benchmark data from the GSM8K dataset and formats it for evaluation purposes.\n\n**Original Artifact:** data.py  \n**Purpose:** Collect and format benchmark data for mathematical reasoning tasks",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Interactive parameter exploration\n# Modify these parameters to see different scenarios\n\ndef create_custom_scenario(n_examples=200, \n                          proposed_fusion_rate=0.65, \n                          baseline_error_rate=0.08,\n                          proposed_error_rate=0.09):\n    \"\"\"Create a custom evaluation scenario.\"\"\"\n    \n    # Baseline: always fission\n    baseline_data = []\n    for i in range(n_examples):\n        error = i < int(n_examples * baseline_error_rate)\n        baseline_data.append({\n            \"decision\": \"fission\",\n            \"error\": error\n        })\n    \n    # Proposed: mix of fusion and fission  \n    proposed_data = []\n    fusion_count = int(n_examples * proposed_fusion_rate)\n    for i in range(n_examples):\n        decision = \"fusion\" if i < fusion_count else \"fission\"\n        error = i < int(n_examples * proposed_error_rate)\n        proposed_data.append({\n            \"decision\": decision,\n            \"error\": error\n        })\n    \n    custom_results = {\n        \"baseline\": baseline_data,\n        \"proposed\": proposed_data\n    }\n    \n    return compute_metrics(custom_results)\n\n# Try different scenarios\nprint(\"=== SCENARIO 1: Higher Fusion Rate ===\")\nscenario1 = create_custom_scenario(proposed_fusion_rate=0.80)\nprint(f\"API Reduction: {scenario1['improvement']['api_reduction_pct']:.1f}%\")\n\nprint(\"\\n=== SCENARIO 2: Lower Error Rate ===\")  \nscenario2 = create_custom_scenario(proposed_error_rate=0.05)\nprint(f\"API Reduction: {scenario2['improvement']['api_reduction_pct']:.1f}%\")\nprint(f\"Error Rate Diff: {scenario2['improvement']['error_rate_diff']:+.1%}\")\n\nprint(\"\\n=== SCENARIO 3: Conservative Approach ===\")\nscenario3 = create_custom_scenario(proposed_fusion_rate=0.40, proposed_error_rate=0.06)\nprint(f\"API Reduction: {scenario3['improvement']['api_reduction_pct']:.1f}%\")\nprint(f\"Error Rate Diff: {scenario3['improvement']['error_rate_diff']:+.1%}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Customization & Usage\n\n### Modifying the Sample Data\nYou can easily modify the `sample_dataset` variable above to include your own questions and answers. Just maintain the format:\n\n```python\nsample_dataset = [\n    {\n        \"question\": \"Your question here\",\n        \"answer\": \"Your answer here\"\n    }\n    # Add more examples...\n]\n```\n\n### Using Real HuggingFace Data\nTo use the actual GSM8k dataset from HuggingFace:\n\n1. Install the datasets library: `pip install datasets`\n2. Uncomment the import: `from datasets import load_dataset`\n3. Use the `collect_data_from_huggingface()` function\n\n### Difficulty Metric\nThe difficulty score is calculated as `question_length / 100`. You can modify this calculation in the `collect_data()` function to use more sophisticated metrics.\n\n### Next Steps\nThis processed data can now be used for:\n- DKW benchmark evaluation\n- Mathematical reasoning model testing\n- Performance analysis and comparison",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Interactive Exploration\n\nTry modifying the parameters below to see how different scenarios affect the results.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Display the formatted data (equivalent to what would be saved in data_out.json)\nformatted_output = json.dumps(collected_data, indent=2)\nprint(formatted_output)\n\nprint(f\"\\nüíæ In the original script, this data would be saved to 'data_out.json'\")\nprint(f\"üéØ The data is now ready for DKW benchmark evaluation!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Display detailed metrics in a formatted way\nimport json\n\nprint(\"=== BASELINE METHOD ===\")\nbaseline = metrics[\"baseline\"]\nprint(f\"Fusion Rate:     {baseline['fusion_rate']:.1%}\")\nprint(f\"Fission Rate:    {baseline['fission_rate']:.1%}\")\nprint(f\"Error Rate:      {baseline['error_rate']:.1%}\")\nprint(f\"Total API Calls: {baseline['api_calls']}\")\nprint(f\"Avg Calls/Example: {baseline['avg_calls_per_example']:.2f}\")\n\nprint(\"\\n=== PROPOSED METHOD ===\")\nproposed = metrics[\"proposed\"]\nprint(f\"Fusion Rate:     {proposed['fusion_rate']:.1%}\")\nprint(f\"Fission Rate:    {proposed['fission_rate']:.1%}\")\nprint(f\"Error Rate:      {proposed['error_rate']:.1%}\")\nprint(f\"Total API Calls: {proposed['api_calls']}\")\nprint(f\"Avg Calls/Example: {proposed['avg_calls_per_example']:.2f}\")\n\nprint(\"\\n=== IMPROVEMENT ===\")\nimprovement = metrics[\"improvement\"]\nprint(f\"API Reduction:   {improvement['api_reduction_pct']:.1f}%\")\nprint(f\"Error Rate Diff: {improvement['error_rate_diff']:+.1%}\")\n\nprint(\"\\n=== COMPLETE METRICS (JSON) ===\")\nprint(json.dumps(metrics, indent=2))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Execute the data collection\nprint(\"üöÄ Starting data collection process...\\n\")\n\n# Collect and process the data\ncollected_data = collect_data()\n\n# Display the results\nprint(f\"\\nüìà Collection Summary:\")\nprint(f\"   ‚Ä¢ Total examples: {len(collected_data)}\")\nprint(f\"   ‚Ä¢ Average difficulty: {sum(item['difficulty'] for item in collected_data) / len(collected_data):.3f}\")\n\n# Instead of writing to file, we'll display the data inline\nprint(f\"\\nüìã Collected Data Structure:\")\nprint(\"-\" * 50)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Detailed Results\n\nLet's examine the detailed metrics for both methods.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Execute Data Collection\n\nNow let's run the data collection process and see the results. The function will process our sample data and format it for benchmark evaluation.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Compute metrics using our sample data\nmetrics = compute_metrics(results)\n\n# Display the main result\nprint(f\"API reduction: {metrics['improvement']['api_reduction_pct']:.1f}%\")\n\n# Save results (optional - replaces writing to JSON file)\neval_output = metrics\nprint(\"\\nEvaluation completed successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def collect_data(dataset=None):\n    \"\"\"Collect benchmark data for DKW controller evaluation.\"\"\"\n    \n    # Use inline sample data if no dataset provided (self-contained mode)\n    if dataset is None:\n        dataset = sample_dataset\n        print(\"üîÑ Using inline sample data for self-contained execution\")\n    \n    # Process the dataset\n    data = []\n    for i, example in enumerate(dataset):\n        data.append({\n            \"id\": f\"example_{i:03d}\",\n            \"question\": example[\"question\"],\n            \"answer\": example[\"answer\"],\n            \"difficulty\": len(example[\"question\"]) / 100,  # Simple proxy based on question length\n        })\n    \n    print(f\"‚úÖ Processed {len(data)} examples successfully\")\n    return data\n\n# Alternative function that would work with HuggingFace datasets (commented for reference)\ndef collect_data_from_huggingface():\n    \"\"\"\n    Original function that loads from HuggingFace (requires 'datasets' package):\n    \n    from datasets import load_dataset\n    ds = load_dataset(\"gsm8k\", \"main\", split=\"test[:200]\")\n    return collect_data(ds)\n    \"\"\"\n    pass\n\nprint(\"üîß Data collection functions defined successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Run Evaluation\n\nNow let's compute the metrics and display the results.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def compute_metrics(results: dict) -> dict:\n    \"\"\"Compute evaluation metrics.\"\"\"\n    metrics = {}\n\n    for method in [\"baseline\", \"proposed\"]:\n        preds = results[method]\n\n        # Count decisions\n        fusion_count = sum(1 for p in preds if p[\"decision\"] == \"fusion\")\n        fission_count = sum(1 for p in preds if p[\"decision\"] == \"fission\")\n\n        # Compute error rate\n        errors = sum(1 for p in preds if p[\"error\"])\n        error_rate = errors / len(preds)\n\n        # API calls (fusion=1, fission=2)\n        api_calls = fusion_count + 2 * fission_count\n\n        metrics[method] = {\n            \"fusion_rate\": fusion_count / len(preds),\n            \"fission_rate\": fission_count / len(preds),\n            \"error_rate\": error_rate,\n            \"api_calls\": api_calls,\n            \"avg_calls_per_example\": api_calls / len(preds),\n        }\n\n    # Compute improvement\n    baseline_calls = metrics[\"baseline\"][\"avg_calls_per_example\"]\n    proposed_calls = metrics[\"proposed\"][\"avg_calls_per_example\"]\n    metrics[\"improvement\"] = {\n        \"api_reduction_pct\": (baseline_calls - proposed_calls) / baseline_calls * 100,\n        \"error_rate_diff\": metrics[\"proposed\"][\"error_rate\"] - metrics[\"baseline\"][\"error_rate\"],\n    }\n\n    return metrics",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Data Collection Function\n\nThe `collect_data()` function processes the raw dataset and formats it for DKW benchmark evaluation. It:\n\n1. Takes mathematical questions and answers\n2. Assigns unique IDs to each example\n3. Calculates a difficulty metric based on question length\n4. Returns structured data ready for benchmark testing",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Sample data that mimics HuggingFace GSM8k dataset format\n# This represents what would be loaded from: load_dataset(\"gsm8k\", \"main\", split=\"test[:200]\")\nsample_dataset = [\n    {\n        \"question\": \"What is 2+2?\",\n        \"answer\": \"4\"\n    },\n    {\n        \"question\": \"If x=5, what is 2x?\", \n        \"answer\": \"10\"\n    },\n    {\n        \"question\": \"Solve: 3y + 6 = 15\",\n        \"answer\": \"y=3\"\n    }\n]\n\nprint(f\"üìä Sample dataset loaded with {len(sample_dataset)} examples\")\nprint(\"\\nüîç Preview of first example:\")\nprint(f\"Question: {sample_dataset[0]['question']}\")\nprint(f\"Answer: {sample_dataset[0]['answer']}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Metrics Computation Function\n\nThis function analyzes the results and computes key performance metrics for both methods.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create sample data that matches the expected evaluation results\n# 200 examples total for each method\n\n# Baseline: 100% fission, 8% error rate  \nbaseline_data = []\nfor i in range(200):\n    error = i < 16  # First 16 examples have errors (8% error rate)\n    baseline_data.append({\n        \"decision\": \"fission\",\n        \"error\": error\n    })\n\n# Proposed: 65% fusion, 35% fission, 9% error rate\nproposed_data = []\nfor i in range(200):\n    if i < 130:  # First 130 examples use fusion (65%)\n        decision = \"fusion\"\n    else:  # Last 70 examples use fission (35%)\n        decision = \"fission\"\n    \n    error = i < 18  # First 18 examples have errors (9% error rate)\n    proposed_data.append({\n        \"decision\": decision,\n        \"error\": error\n    })\n\n# Combined results dictionary (replaces reading from JSON file)\nresults = {\n    \"baseline\": baseline_data,\n    \"proposed\": proposed_data\n}\n\nprint(f\"Created sample data:\")\nprint(f\"- Baseline: {len(results['baseline'])} examples\")\nprint(f\"- Proposed: {len(results['proposed'])} examples\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Sample Data (Inline)\n\nFor self-contained execution, we'll use sample data that represents what would normally be loaded from the HuggingFace GSM8k dataset. This data includes mathematical reasoning questions with their answers.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Sample Data\n\nInstead of reading from external JSON files, we'll define the sample data inline. This represents the results from 200 test examples for both baseline and proposed methods.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\"\"\"Dataset collection script for DKW benchmark.\"\"\"\nimport json\n# Note: In a real environment, you would need: pip install datasets\n# from datasets import load_dataset\n\n# For this self-contained demo, we'll use inline sample data\nprint(\"‚úÖ Imports loaded successfully!\")\nprint(\"üìù Note: This notebook uses inline sample data for self-contained execution\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# DKW Benchmark Dataset Collection\n\n**Artifact:** dataset_001 - data.py\n\nThis notebook demonstrates the dataset collection script for DKW benchmark evaluation. It processes mathematical reasoning questions from the GSM8k dataset and formats them for benchmark testing.\n\n## Features\n- Loads data from HuggingFace GSM8k dataset\n- Processes and formats questions with answers\n- Calculates difficulty metrics\n- Self-contained execution with sample data",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\"\"\"Evaluation script for DKW Controller.\"\"\"\nimport json\nimport numpy as np",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# DKW Controller Evaluation\n\nThis notebook evaluates the performance of a proposed method against a baseline for the DKW Controller system. \n\nThe evaluation compares two approaches:\n- **Baseline**: Always uses fission (2 API calls per example)\n- **Proposed**: Intelligently chooses between fusion (1 API call) and fission (2 API calls)\n\nKey metrics computed:\n- Fusion/Fission rates\n- Error rates \n- API call efficiency\n- Performance improvement",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}