{
 "cells": [
  {
   "cell_type": "code",
   "source": "# Interactive parameter exploration\n# Modify these parameters to see different scenarios\n\ndef create_custom_scenario(n_examples=200, \n                          proposed_fusion_rate=0.65, \n                          baseline_error_rate=0.08,\n                          proposed_error_rate=0.09):\n    \"\"\"Create a custom evaluation scenario.\"\"\"\n    \n    # Baseline: always fission\n    baseline_data = []\n    for i in range(n_examples):\n        error = i < int(n_examples * baseline_error_rate)\n        baseline_data.append({\n            \"decision\": \"fission\",\n            \"error\": error\n        })\n    \n    # Proposed: mix of fusion and fission  \n    proposed_data = []\n    fusion_count = int(n_examples * proposed_fusion_rate)\n    for i in range(n_examples):\n        decision = \"fusion\" if i < fusion_count else \"fission\"\n        error = i < int(n_examples * proposed_error_rate)\n        proposed_data.append({\n            \"decision\": decision,\n            \"error\": error\n        })\n    \n    custom_results = {\n        \"baseline\": baseline_data,\n        \"proposed\": proposed_data\n    }\n    \n    return compute_metrics(custom_results)\n\n# Try different scenarios\nprint(\"=== SCENARIO 1: Higher Fusion Rate ===\")\nscenario1 = create_custom_scenario(proposed_fusion_rate=0.80)\nprint(f\"API Reduction: {scenario1['improvement']['api_reduction_pct']:.1f}%\")\n\nprint(\"\\n=== SCENARIO 2: Lower Error Rate ===\")  \nscenario2 = create_custom_scenario(proposed_error_rate=0.05)\nprint(f\"API Reduction: {scenario2['improvement']['api_reduction_pct']:.1f}%\")\nprint(f\"Error Rate Diff: {scenario2['improvement']['error_rate_diff']:+.1%}\")\n\nprint(\"\\n=== SCENARIO 3: Conservative Approach ===\")\nscenario3 = create_custom_scenario(proposed_fusion_rate=0.40, proposed_error_rate=0.06)\nprint(f\"API Reduction: {scenario3['improvement']['api_reduction_pct']:.1f}%\")\nprint(f\"Error Rate Diff: {scenario3['improvement']['error_rate_diff']:+.1%}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Customization & Usage\n\n### Modifying the Sample Data\nYou can easily modify the `sample_dataset` variable above to include your own questions and answers. Just maintain the format:\n\n```python\nsample_dataset = [\n    {\n        \"question\": \"Your question here\",\n        \"answer\": \"Your answer here\"\n    }\n    # Add more examples...\n]\n```\n\n### Using Real HuggingFace Data\nTo use the actual GSM8k dataset from HuggingFace:\n\n1. Install the datasets library: `pip install datasets`\n2. Uncomment the import: `from datasets import load_dataset`\n3. Use the `collect_data_from_huggingface()` function\n\n### Difficulty Metric\nThe difficulty score is calculated as `question_length / 100`. You can modify this calculation in the `collect_data()` function to use more sophisticated metrics.\n\n### Next Steps\nThis processed data can now be used for:\n- DKW benchmark evaluation\n- Mathematical reasoning model testing\n- Performance analysis and comparison",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Interactive Exploration\n\nTry modifying the parameters below to see how different scenarios affect the results.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Display the formatted data (equivalent to what would be saved in data_out.json)\nformatted_output = json.dumps(collected_data, indent=2)\nprint(formatted_output)\n\nprint(f\"\\nüíæ In the original script, this data would be saved to 'data_out.json'\")\nprint(f\"üéØ The data is now ready for DKW benchmark evaluation!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Display detailed metrics in a formatted way\nimport json\n\nprint(\"=== BASELINE METHOD ===\")\nbaseline = metrics[\"baseline\"]\nprint(f\"Fusion Rate:     {baseline['fusion_rate']:.1%}\")\nprint(f\"Fission Rate:    {baseline['fission_rate']:.1%}\")\nprint(f\"Error Rate:      {baseline['error_rate']:.1%}\")\nprint(f\"Total API Calls: {baseline['api_calls']}\")\nprint(f\"Avg Calls/Example: {baseline['avg_calls_per_example']:.2f}\")\n\nprint(\"\\n=== PROPOSED METHOD ===\")\nproposed = metrics[\"proposed\"]\nprint(f\"Fusion Rate:     {proposed['fusion_rate']:.1%}\")\nprint(f\"Fission Rate:    {proposed['fission_rate']:.1%}\")\nprint(f\"Error Rate:      {proposed['error_rate']:.1%}\")\nprint(f\"Total API Calls: {proposed['api_calls']}\")\nprint(f\"Avg Calls/Example: {proposed['avg_calls_per_example']:.2f}\")\n\nprint(\"\\n=== IMPROVEMENT ===\")\nimprovement = metrics[\"improvement\"]\nprint(f\"API Reduction:   {improvement['api_reduction_pct']:.1f}%\")\nprint(f\"Error Rate Diff: {improvement['error_rate_diff']:+.1%}\")\n\nprint(\"\\n=== COMPLETE METRICS (JSON) ===\")\nprint(json.dumps(metrics, indent=2))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Execute the data collection\nprint(\"üöÄ Starting data collection process...\\n\")\n\n# Collect and process the data\ncollected_data = collect_data()\n\n# Display the results\nprint(f\"\\nüìà Collection Summary:\")\nprint(f\"   ‚Ä¢ Total examples: {len(collected_data)}\")\nprint(f\"   ‚Ä¢ Average difficulty: {sum(item['difficulty'] for item in collected_data) / len(collected_data):.3f}\")\n\n# Instead of writing to file, we'll display the data inline\nprint(f\"\\nüìã Collected Data Structure:\")\nprint(\"-\" * 50)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Detailed Results\n\nLet's examine the detailed metrics for both methods.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Execute Data Collection\n\nNow let's run the data collection process and see the results. The function will process our sample data and format it for benchmark evaluation.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Compute metrics using our sample data\nmetrics = compute_metrics(results)\n\n# Display the main result\nprint(f\"API reduction: {metrics['improvement']['api_reduction_pct']:.1f}%\")\n\n# Save results (optional - replaces writing to JSON file)\neval_output = metrics\nprint(\"\\nEvaluation completed successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def collect_data(dataset=None):\n    \"\"\"Collect benchmark data for DKW controller evaluation.\"\"\"\n    \n    # Use inline sample data if no dataset provided (self-contained mode)\n    if dataset is None:\n        dataset = sample_dataset\n        print(\"üîÑ Using inline sample data for self-contained execution\")\n    \n    # Process the dataset\n    data = []\n    for i, example in enumerate(dataset):\n        data.append({\n            \"id\": f\"example_{i:03d}\",\n            \"question\": example[\"question\"],\n            \"answer\": example[\"answer\"],\n            \"difficulty\": len(example[\"question\"]) / 100,  # Simple proxy based on question length\n        })\n    \n    print(f\"‚úÖ Processed {len(data)} examples successfully\")\n    return data\n\n# Alternative function that would work with HuggingFace datasets (commented for reference)\ndef collect_data_from_huggingface():\n    \"\"\"\n    Original function that loads from HuggingFace (requires 'datasets' package):\n    \n    from datasets import load_dataset\n    ds = load_dataset(\"gsm8k\", \"main\", split=\"test[:200]\")\n    return collect_data(ds)\n    \"\"\"\n    pass\n\nprint(\"üîß Data collection functions defined successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Run Evaluation\n\nNow let's compute the metrics and display the results.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def compute_metrics(results: dict) -> dict:\n    \"\"\"Compute evaluation metrics.\"\"\"\n    metrics = {}\n\n    for method in [\"baseline\", \"proposed\"]:\n        preds = results[method]\n\n        # Count decisions\n        fusion_count = sum(1 for p in preds if p[\"decision\"] == \"fusion\")\n        fission_count = sum(1 for p in preds if p[\"decision\"] == \"fission\")\n\n        # Compute error rate\n        errors = sum(1 for p in preds if p[\"error\"])\n        error_rate = errors / len(preds)\n\n        # API calls (fusion=1, fission=2)\n        api_calls = fusion_count + 2 * fission_count\n\n        metrics[method] = {\n            \"fusion_rate\": fusion_count / len(preds),\n            \"fission_rate\": fission_count / len(preds),\n            \"error_rate\": error_rate,\n            \"api_calls\": api_calls,\n            \"avg_calls_per_example\": api_calls / len(preds),\n        }\n\n    # Compute improvement\n    baseline_calls = metrics[\"baseline\"][\"avg_calls_per_example\"]\n    proposed_calls = metrics[\"proposed\"][\"avg_calls_per_example\"]\n    metrics[\"improvement\"] = {\n        \"api_reduction_pct\": (baseline_calls - proposed_calls) / baseline_calls * 100,\n        \"error_rate_diff\": metrics[\"proposed\"][\"error_rate\"] - metrics[\"baseline\"][\"error_rate\"],\n    }\n\n    return metrics",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Data Collection Function\n\nThe `collect_data()` function processes the raw dataset and formats it for DKW benchmark evaluation. It:\n\n1. Takes mathematical questions and answers\n2. Assigns unique IDs to each example\n3. Calculates a difficulty metric based on question length\n4. Returns structured data ready for benchmark testing",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Sample data that mimics HuggingFace GSM8k dataset format\n# This represents what would be loaded from: load_dataset(\"gsm8k\", \"main\", split=\"test[:200]\")\nsample_dataset = [\n    {\n        \"question\": \"What is 2+2?\",\n        \"answer\": \"4\"\n    },\n    {\n        \"question\": \"If x=5, what is 2x?\", \n        \"answer\": \"10\"\n    },\n    {\n        \"question\": \"Solve: 3y + 6 = 15\",\n        \"answer\": \"y=3\"\n    }\n]\n\nprint(f\"üìä Sample dataset loaded with {len(sample_dataset)} examples\")\nprint(\"\\nüîç Preview of first example:\")\nprint(f\"Question: {sample_dataset[0]['question']}\")\nprint(f\"Answer: {sample_dataset[0]['answer']}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Metrics Computation Function\n\nThis function analyzes the results and computes key performance metrics for both methods.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create sample data that matches the expected evaluation results\n# 200 examples total for each method\n\n# Baseline: 100% fission, 8% error rate  \nbaseline_data = []\nfor i in range(200):\n    error = i < 16  # First 16 examples have errors (8% error rate)\n    baseline_data.append({\n        \"decision\": \"fission\",\n        \"error\": error\n    })\n\n# Proposed: 65% fusion, 35% fission, 9% error rate\nproposed_data = []\nfor i in range(200):\n    if i < 130:  # First 130 examples use fusion (65%)\n        decision = \"fusion\"\n    else:  # Last 70 examples use fission (35%)\n        decision = \"fission\"\n    \n    error = i < 18  # First 18 examples have errors (9% error rate)\n    proposed_data.append({\n        \"decision\": decision,\n        \"error\": error\n    })\n\n# Combined results dictionary (replaces reading from JSON file)\nresults = {\n    \"baseline\": baseline_data,\n    \"proposed\": proposed_data\n}\n\nprint(f\"Created sample data:\")\nprint(f\"- Baseline: {len(results['baseline'])} examples\")\nprint(f\"- Proposed: {len(results['proposed'])} examples\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Sample Data (Inline)\n\nFor self-contained execution, we'll use sample data that represents what would normally be loaded from the HuggingFace GSM8k dataset. This data includes mathematical reasoning questions with their answers.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Sample Data\n\nInstead of reading from external JSON files, we'll define the sample data inline. This represents the results from 200 test examples for both baseline and proposed methods.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\"\"\"Dataset collection script for DKW benchmark.\"\"\"\nimport json\n# Note: In a real environment, you would need: pip install datasets\n# from datasets import load_dataset\n\n# For this self-contained demo, we'll use inline sample data\nprint(\"‚úÖ Imports loaded successfully!\")\nprint(\"üìù Note: This notebook uses inline sample data for self-contained execution\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# DKW Benchmark Dataset Collection\n\n**Artifact:** dataset_001 - data.py\n\nThis notebook demonstrates the dataset collection script for DKW benchmark evaluation. It processes mathematical reasoning questions from the GSM8k dataset and formats them for benchmark testing.\n\n## Features\n- Loads data from HuggingFace GSM8k dataset\n- Processes and formats questions with answers\n- Calculates difficulty metrics\n- Self-contained execution with sample data",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\"\"\"Evaluation script for DKW Controller.\"\"\"\nimport json\nimport numpy as np",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# DKW Controller Evaluation\n\nThis notebook evaluates the performance of a proposed method against a baseline for the DKW Controller system. \n\nThe evaluation compares two approaches:\n- **Baseline**: Always uses fission (2 API calls per example)\n- **Proposed**: Intelligently chooses between fusion (1 API call) and fission (2 API calls)\n\nKey metrics computed:\n- Fusion/Fission rates\n- Error rates \n- API call efficiency\n- Performance improvement",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}