{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## Customization and Usage\n\nThis notebook is completely self-contained and ready to run. To customize it:\n\n1. **Modify the data**: Edit the sample data generation in the \"Sample Data\" cell to reflect your actual prediction results\n2. **Add visualizations**: Consider adding matplotlib/seaborn plots to visualize the results\n3. **Extend metrics**: Add additional evaluation metrics in the `compute_metrics` function\n4. **Save results**: Uncomment the file saving code in the last cell to write results to disk\n\n### Key Insights from Current Results:\n- The proposed method achieved a **32.5% reduction** in API calls compared to baseline\n- This was accomplished by shifting from 100% fission decisions to 65% fusion + 35% fission  \n- Trade-off: Slightly higher error rate (+1 percentage point) but significant efficiency gains",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Display metrics in JSON format (equivalent to eval_out.json)\nimport json\n\nprint(\"Metrics (JSON format):\")\nprint(json.dumps(metrics, indent=2))\n\n# Optional: Save to file if desired (uncomment the lines below)\n# with open(\"eval_out.json\", \"w\") as f:\n#     json.dump(metrics, f, indent=2)\n# print(\"\\nMetrics saved to eval_out.json\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Metrics Output (JSON Format)\n\nThe following cell shows the complete metrics in JSON format (equivalent to what was originally written to `eval_out.json`):",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Compute the metrics\nmetrics = compute_metrics(results)\n\n# Display the main result (equivalent to the original script's print statement)\nprint(f\"API reduction: {metrics['improvement']['api_reduction_pct']:.1f}%\")\n\n# Display all computed metrics in a formatted way\nprint(\"\\n\" + \"=\"*50)\nprint(\"DETAILED EVALUATION RESULTS\")\nprint(\"=\"*50)\n\nfor method in [\"baseline\", \"proposed\"]:\n    print(f\"\\n{method.upper()} METHOD:\")\n    print(f\"  Fusion rate:     {metrics[method]['fusion_rate']:.3f} ({metrics[method]['fusion_rate']*100:.1f}%)\")\n    print(f\"  Fission rate:    {metrics[method]['fission_rate']:.3f} ({metrics[method]['fission_rate']*100:.1f}%)\")\n    print(f\"  Error rate:      {metrics[method]['error_rate']:.3f} ({metrics[method]['error_rate']*100:.1f}%)\")\n    print(f\"  Total API calls: {metrics[method]['api_calls']}\")\n    print(f\"  Avg calls/example: {metrics[method]['avg_calls_per_example']:.2f}\")\n\nprint(f\"\\nIMPROVEMENT:\")\nprint(f\"  API reduction:   {metrics['improvement']['api_reduction_pct']:.1f}%\")\nprint(f\"  Error rate diff: {metrics['improvement']['error_rate_diff']:+.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Compute Metrics and Results\n\nExecute the evaluation and display the computed metrics:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def compute_metrics(results: dict) -> dict:\n    \"\"\"Compute evaluation metrics.\"\"\"\n    metrics = {}\n\n    for method in [\"baseline\", \"proposed\"]:\n        preds = results[method]\n\n        # Count decisions\n        fusion_count = sum(1 for p in preds if p[\"decision\"] == \"fusion\")\n        fission_count = sum(1 for p in preds if p[\"decision\"] == \"fission\")\n\n        # Compute error rate\n        errors = sum(1 for p in preds if p[\"error\"])\n        error_rate = errors / len(preds)\n\n        # API calls (fusion=1, fission=2)\n        api_calls = fusion_count + 2 * fission_count\n\n        metrics[method] = {\n            \"fusion_rate\": fusion_count / len(preds),\n            \"fission_rate\": fission_count / len(preds),\n            \"error_rate\": error_rate,\n            \"api_calls\": api_calls,\n            \"avg_calls_per_example\": api_calls / len(preds),\n        }\n\n    # Compute improvement\n    baseline_calls = metrics[\"baseline\"][\"avg_calls_per_example\"]\n    proposed_calls = metrics[\"proposed\"][\"avg_calls_per_example\"]\n    metrics[\"improvement\"] = {\n        \"api_reduction_pct\": (baseline_calls - proposed_calls) / baseline_calls * 100,\n        \"error_rate_diff\": metrics[\"proposed\"][\"error_rate\"] - metrics[\"baseline\"][\"error_rate\"],\n    }\n\n    return metrics",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Evaluation Function\n\nThe `compute_metrics` function calculates key performance metrics for both baseline and proposed methods:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Sample prediction data (originally from ../experiment_001/method_out.json)\n# This data is structured to produce the exact metrics shown in eval_out.json\n\n# Generate baseline predictions: 200 examples, all fission, 8% error rate\nbaseline_predictions = []\nfor i in range(200):\n    baseline_predictions.append({\n        \"decision\": \"fission\",\n        \"error\": i < 16  # First 16 examples have errors (8% of 200)\n    })\n\n# Generate proposed predictions: 200 examples, 65% fusion/35% fission, 9% error rate\nproposed_predictions = []\nfor i in range(200):\n    if i < 130:  # First 130 are fusion (65% of 200)\n        decision = \"fusion\"\n    else:  # Remaining 70 are fission (35% of 200)\n        decision = \"fission\"\n    \n    proposed_predictions.append({\n        \"decision\": decision,\n        \"error\": i < 18  # First 18 examples have errors (9% of 200)\n    })\n\n# Combine into results structure\nresults = {\n    \"baseline\": baseline_predictions,\n    \"proposed\": proposed_predictions\n}\n\nprint(f\"Baseline predictions: {len(results['baseline'])} examples\")\nprint(f\"Proposed predictions: {len(results['proposed'])} examples\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Sample Data\n\nThe following cell contains sample prediction data that replicates the scenario described in the original script. In the original code, this data would be loaded from `../experiment_001/method_out.json`, but here we've inlined it for self-contained execution.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\"\"\"Evaluation script for DKW Controller.\"\"\"\nimport json\nimport numpy as np",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# DKW Controller Evaluation\n\n**Artifact ID:** evaluation_001  \n**Original File:** eval.py\n\nThis notebook evaluates the performance of a DKW Controller system, comparing baseline and proposed methods in terms of API call efficiency and error rates.\n\n## Overview\n- **Fusion decisions**: Single API call\n- **Fission decisions**: Two API calls  \n- **Metrics**: Fusion/fission rates, error rates, API call efficiency",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}