{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# DKW Controller Evaluation\n",
    "\n",
    "This notebook evaluates the performance of the DKW Controller, comparing baseline and proposed methods for decision-making between fusion and fission operations.\n",
    "\n",
    "**Artifact ID:** evaluation_001  \n",
    "**Original file:** eval.py\n",
    "\n",
    "**Key Metrics:**\n",
    "- **Fusion Rate**: Proportion of decisions that chose fusion (1 API call)\n",
    "- **Fission Rate**: Proportion of decisions that chose fission (2 API calls)  \n",
    "- **Error Rate**: Proportion of predictions that resulted in errors\n",
    "- **API Efficiency**: Average API calls per example and overall reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluation script for DKW Controller.\"\"\"\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-section",
   "metadata": {},
   "source": [
    "## Sample Data\n",
    "\n",
    "Instead of reading from external JSON files, we'll create sample data inline that represents the evaluation results from both methods. Each prediction contains:\n",
    "- `decision`: Either \"fusion\" (1 API call) or \"fission\" (2 API calls)\n",
    "- `error`: Boolean indicating if the prediction resulted in an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sample-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data that matches the expected metrics\n",
    "# Baseline: 200 examples, all fission, 8% error rate\n",
    "baseline_data = []\n",
    "for i in range(200):\n",
    "    error = i < 16  # First 16 examples have errors (8% error rate)\n",
    "    baseline_data.append({\n",
    "        \"decision\": \"fission\",\n",
    "        \"error\": error\n",
    "    })\n",
    "\n",
    "# Proposed: 200 examples, 65% fusion/35% fission, 9% error rate  \n",
    "proposed_data = []\n",
    "for i in range(200):\n",
    "    decision = \"fusion\" if i < 130 else \"fission\"  # First 130 are fusion (65%)\n",
    "    error = i < 18  # First 18 examples have errors (9% error rate)\n",
    "    proposed_data.append({\n",
    "        \"decision\": decision,\n",
    "        \"error\": error\n",
    "    })\n",
    "\n",
    "# Combine into the expected results structure\n",
    "results = {\n",
    "    \"baseline\": baseline_data,\n",
    "    \"proposed\": proposed_data\n",
    "}\n",
    "\n",
    "print(f\"Created {len(results['baseline'])} baseline examples\")\n",
    "print(f\"Created {len(results['proposed'])} proposed examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metrics-section",
   "metadata": {},
   "source": [
    "## Metrics Computation\n",
    "\n",
    "The `compute_metrics` function analyzes the prediction results and calculates key performance indicators for both methods. It computes rates, API call efficiency, and improvement metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compute-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(results: dict) -> dict:\n",
    "    \"\"\"Compute evaluation metrics.\"\"\"\n",
    "    metrics = {}\n",
    "\n",
    "    for method in [\"baseline\", \"proposed\"]:\n",
    "        preds = results[method]\n",
    "\n",
    "        # Count decisions\n",
    "        fusion_count = sum(1 for p in preds if p[\"decision\"] == \"fusion\")\n",
    "        fission_count = sum(1 for p in preds if p[\"decision\"] == \"fission\")\n",
    "\n",
    "        # Compute error rate\n",
    "        errors = sum(1 for p in preds if p[\"error\"])\n",
    "        error_rate = errors / len(preds)\n",
    "\n",
    "        # API calls (fusion=1, fission=2)\n",
    "        api_calls = fusion_count + 2 * fission_count\n",
    "\n",
    "        metrics[method] = {\n",
    "            \"fusion_rate\": fusion_count / len(preds),\n",
    "            \"fission_rate\": fission_count / len(preds),\n",
    "            \"error_rate\": error_rate,\n",
    "            \"api_calls\": api_calls,\n",
    "            \"avg_calls_per_example\": api_calls / len(preds),\n",
    "        }\n",
    "\n",
    "    # Compute improvement\n",
    "    baseline_calls = metrics[\"baseline\"][\"avg_calls_per_example\"]\n",
    "    proposed_calls = metrics[\"proposed\"][\"avg_calls_per_example\"]\n",
    "    metrics[\"improvement\"] = {\n",
    "        \"api_reduction_pct\": (baseline_calls - proposed_calls) / baseline_calls * 100,\n",
    "        \"error_rate_diff\": metrics[\"proposed\"][\"error_rate\"] - metrics[\"baseline\"][\"error_rate\"],\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-section",
   "metadata": {},
   "source": [
    "## Run Evaluation\n",
    "\n",
    "Now we'll compute the metrics and display the results in a formatted way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "metrics = compute_metrics(results)\n",
    "\n",
    "# Display results in a formatted way\n",
    "print(\"=\"*50)\n",
    "print(\"DKW CONTROLLER EVALUATION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nBASELINE METHOD:\")\n",
    "print(f\"  Fusion Rate:       {metrics['baseline']['fusion_rate']:.1%}\")\n",
    "print(f\"  Fission Rate:      {metrics['baseline']['fission_rate']:.1%}\")\n",
    "print(f\"  Error Rate:        {metrics['baseline']['error_rate']:.1%}\")\n",
    "print(f\"  Total API Calls:   {metrics['baseline']['api_calls']:,}\")\n",
    "print(f\"  Avg Calls/Example: {metrics['baseline']['avg_calls_per_example']:.2f}\")\n",
    "\n",
    "print(\"\\nPROPOSED METHOD:\")\n",
    "print(f\"  Fusion Rate:       {metrics['proposed']['fusion_rate']:.1%}\")\n",
    "print(f\"  Fission Rate:      {metrics['proposed']['fission_rate']:.1%}\")\n",
    "print(f\"  Error Rate:        {metrics['proposed']['error_rate']:.1%}\")\n",
    "print(f\"  Total API Calls:   {metrics['proposed']['api_calls']:,}\")\n",
    "print(f\"  Avg Calls/Example: {metrics['proposed']['avg_calls_per_example']:.2f}\")\n",
    "\n",
    "print(\"\\nIMPROVEMENT:\")\n",
    "print(f\"  API Reduction:     {metrics['improvement']['api_reduction_pct']:.1f}%\")\n",
    "print(f\"  Error Rate Change: {metrics['improvement']['error_rate_diff']:+.1%}\")\n",
    "\n",
    "# Show key result\n",
    "print(f\"\\nðŸŽ‰ API reduction: {metrics['improvement']['api_reduction_pct']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "json-section",
   "metadata": {},
   "source": [
    "## Optional: Export Results\n",
    "\n",
    "The original script saved results to a JSON file. Here's the equivalent output for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "json-output",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the metrics as JSON (equivalent to what would be saved to eval_out.json)\n",
    "print(\"Metrics JSON Output:\")\n",
    "print(\"=\" * 20)\n",
    "print(json.dumps(metrics, indent=2))\n",
    "\n",
    "# If you want to save to a file, uncomment the following:\n",
    "# with open(\"eval_out.json\", \"w\") as f:\n",
    "#     json.dump(metrics, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usage-section",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "\n",
    "This notebook is **completely self-contained** and ready to run! Here are some ways you can customize it:\n",
    "\n",
    "### Modify Sample Data:\n",
    "- Change the number of examples: adjust the `range(200)` values\n",
    "- Modify error rates: change the threshold values (e.g., `i < 16` for 8% error rate)\n",
    "- Adjust fusion/fission ratios: modify the decision logic\n",
    "\n",
    "### Add Real Data:\n",
    "- Replace the sample data generation with real prediction results\n",
    "- Load data from your own JSON files\n",
    "- Connect to your evaluation pipeline\n",
    "\n",
    "### Extend Analysis:\n",
    "- Add visualization (matplotlib/seaborn)\n",
    "- Include confidence intervals\n",
    "- Add statistical significance testing\n",
    "- Create comparison charts\n",
    "\n",
    "### Running the Notebook:\n",
    "1. No additional packages needed beyond standard Python libraries\n",
    "2. Run all cells in order\n",
    "3. No external files required - everything is self-contained!\n",
    "\n",
    "---\n",
    "*Original script: eval.py | Converted to interactive Jupyter notebook*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}