{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## Customization\n\nYou can modify the sample data above to test different scenarios. For example:\n- Change the fusion/fission ratios for the proposed method\n- Adjust error rates for both methods  \n- Experiment with different sample sizes\n\nSimply modify the data generation code and re-run the evaluation cells to see how different configurations affect the performance metrics.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Display detailed results\nprint(\"=\" * 60)\nprint(\"DKW CONTROLLER EVALUATION RESULTS\")\nprint(\"=\" * 60)\n\nprint(\"\\\\nBASELINE METHOD:\")\nprint(\"-\" * 20)\nbaseline = metrics[\"baseline\"]\nprint(f\"  Fusion Rate:        {baseline['fusion_rate']:.1%}\")\nprint(f\"  Fission Rate:       {baseline['fission_rate']:.1%}\")  \nprint(f\"  Error Rate:         {baseline['error_rate']:.1%}\")\nprint(f\"  Total API Calls:    {baseline['api_calls']:,}\")\nprint(f\"  Avg Calls/Example:  {baseline['avg_calls_per_example']:.2f}\")\n\nprint(\"\\\\nPROPOSED METHOD:\")\nprint(\"-\" * 20)\nproposed = metrics[\"proposed\"]\nprint(f\"  Fusion Rate:        {proposed['fusion_rate']:.1%}\")\nprint(f\"  Fission Rate:       {proposed['fission_rate']:.1%}\")\nprint(f\"  Error Rate:         {proposed['error_rate']:.1%}\")\nprint(f\"  Total API Calls:    {proposed['api_calls']:,}\")\nprint(f\"  Avg Calls/Example:  {proposed['avg_calls_per_example']:.2f}\")\n\nprint(\"\\\\nIMPROVEMENT ANALYSIS:\")\nprint(\"-\" * 20)\nimprovement = metrics[\"improvement\"]\nprint(f\"  API Reduction:      {improvement['api_reduction_pct']:.1f}%\")\nprint(f\"  Error Rate Change:  {improvement['error_rate_diff']:+.1%}\")\n\n# Calculate absolute improvements\ncalls_saved = baseline['api_calls'] - proposed['api_calls']\nprint(f\"  Total Calls Saved:  {calls_saved:,}\")\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Detailed Results Display\n\nLet's examine the detailed metrics for better understanding of the performance differences.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Compute evaluation metrics\nmetrics = compute_metrics(results)\n\n# Display the main improvement result (as in original script)\nprint(f\"API reduction: {metrics['improvement']['api_reduction_pct']:.1f}%\")\n\n# Save results (equivalent to writing eval_out.json)\nwith open(\"eval_out.json\", \"w\") as f:\n    json.dump(metrics, f, indent=2)\n    \nprint(\"\\\\nResults saved to eval_out.json\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Run Evaluation\n\nNow let's compute the metrics for both methods and display the results.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def compute_metrics(results: dict) -> dict:\n    \"\"\"Compute evaluation metrics.\"\"\"\n    metrics = {}\n\n    for method in [\"baseline\", \"proposed\"]:\n        preds = results[method]\n\n        # Count decisions\n        fusion_count = sum(1 for p in preds if p[\"decision\"] == \"fusion\")\n        fission_count = sum(1 for p in preds if p[\"decision\"] == \"fission\")\n\n        # Compute error rate\n        errors = sum(1 for p in preds if p[\"error\"])\n        error_rate = errors / len(preds)\n\n        # API calls (fusion=1, fission=2)\n        api_calls = fusion_count + 2 * fission_count\n\n        metrics[method] = {\n            \"fusion_rate\": fusion_count / len(preds),\n            \"fission_rate\": fission_count / len(preds),\n            \"error_rate\": error_rate,\n            \"api_calls\": api_calls,\n            \"avg_calls_per_example\": api_calls / len(preds),\n        }\n\n    # Compute improvement\n    baseline_calls = metrics[\"baseline\"][\"avg_calls_per_example\"]\n    proposed_calls = metrics[\"proposed\"][\"avg_calls_per_example\"]\n    metrics[\"improvement\"] = {\n        \"api_reduction_pct\": (baseline_calls - proposed_calls) / baseline_calls * 100,\n        \"error_rate_diff\": metrics[\"proposed\"][\"error_rate\"] - metrics[\"baseline\"][\"error_rate\"],\n    }\n\n    return metrics",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Evaluation Metrics Function\n\nThe `compute_metrics` function calculates key performance indicators:\n- **Fusion/Fission rates**: Percentage of each decision type\n- **Error rate**: Percentage of predictions that resulted in errors  \n- **API calls**: Total calls needed (fusion=1 call, fission=2 calls)\n- **Improvement**: Comparison between baseline and proposed methods",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create sample evaluation results data\n# This simulates the data that would be read from ../experiment_001/method_out.json\n\n# Baseline: 100% fission decisions, 8% error rate\nbaseline_predictions = []\nfor i in range(200):\n    error = i < 16  # First 16 have errors (8%)\n    baseline_predictions.append({\n        \"decision\": \"fission\",\n        \"error\": error\n    })\n\n# Proposed: 65% fusion, 35% fission, 9% error rate  \nproposed_predictions = []\nfor i in range(200):\n    if i < 130:  # First 130 are fusion (65%)\n        decision = \"fusion\"\n    else:  # Remaining 70 are fission (35%)\n        decision = \"fission\"\n    \n    error = i < 18  # First 18 have errors (9%)\n    proposed_predictions.append({\n        \"decision\": decision,\n        \"error\": error\n    })\n\n# Combined results data structure\nresults = {\n    \"baseline\": baseline_predictions,\n    \"proposed\": proposed_predictions\n}\n\nprint(f\"Created {len(results['baseline'])} baseline predictions\")\nprint(f\"Created {len(results['proposed'])} proposed predictions\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Sample Data\n\nBelow we define the evaluation results data that would normally be read from external JSON files. This data contains predictions from both baseline and proposed methods, including their decisions (fusion/fission) and error status.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import json\nimport numpy as np",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Setup and Imports\n\nFirst, let's import the necessary libraries for our evaluation.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# DKW Controller Evaluation\n\nThis notebook provides an interactive evaluation of the DKW Controller, comparing baseline and proposed methods for API call optimization. The evaluation measures fusion/fission decision rates, error rates, and API call efficiency.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}