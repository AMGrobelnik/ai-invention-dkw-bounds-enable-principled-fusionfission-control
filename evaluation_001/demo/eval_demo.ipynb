{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-cell",
   "metadata": {},
   "source": [
    "# DKW Controller Evaluation\n",
    "\n",
    "This notebook contains an evaluation script for the DKW Controller, comparing baseline and proposed methods in terms of:\n",
    "- Decision rates (fusion vs fission)\n",
    "- Error rates  \n",
    "- API call efficiency\n",
    "- Performance improvements\n",
    "\n",
    "The notebook is self-contained with inline sample data for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluation script for DKW Controller.\"\"\"\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-title-cell",
   "metadata": {},
   "source": [
    "## Sample Data\n",
    "\n",
    "Instead of reading from external JSON files, we'll create inline sample data that represents evaluation results for 200 test examples each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data that matches the expected evaluation results\n",
    "# 200 examples per method to produce the metrics shown in eval_out.json\n",
    "\n",
    "# Baseline method: 100% fission decisions, 8% error rate\n",
    "baseline_data = []\n",
    "for i in range(200):\n",
    "    baseline_data.append({\n",
    "        \"decision\": \"fission\",  # All baseline decisions are fission\n",
    "        \"error\": i < 16  # First 16 examples have errors (8% error rate)\n",
    "    })\n",
    "\n",
    "# Proposed method: 65% fusion, 35% fission, 9% error rate  \n",
    "proposed_data = []\n",
    "for i in range(200):\n",
    "    if i < 130:  # First 130 examples use fusion (65%)\n",
    "        decision = \"fusion\"\n",
    "    else:  # Remaining 70 examples use fission (35%)\n",
    "        decision = \"fission\"\n",
    "    \n",
    "    proposed_data.append({\n",
    "        \"decision\": decision,\n",
    "        \"error\": i < 18  # First 18 examples have errors (9% error rate)\n",
    "    })\n",
    "\n",
    "# Combine into the expected data structure\n",
    "results = {\n",
    "    \"baseline\": baseline_data,\n",
    "    \"proposed\": proposed_data\n",
    "}\n",
    "\n",
    "print(f\"Created sample data:\")\n",
    "print(f\"- Baseline: {len(results['baseline'])} examples\")\n",
    "print(f\"- Proposed: {len(results['proposed'])} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "function-title-cell",
   "metadata": {},
   "source": [
    "## Evaluation Function\n",
    "\n",
    "The `compute_metrics` function analyzes the results and calculates key performance indicators for both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "function-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(results: dict) -> dict:\n",
    "    \"\"\"Compute evaluation metrics.\"\"\"\n",
    "    metrics = {}\n",
    "\n",
    "    for method in [\"baseline\", \"proposed\"]:\n",
    "        preds = results[method]\n",
    "\n",
    "        # Count decisions\n",
    "        fusion_count = sum(1 for p in preds if p[\"decision\"] == \"fusion\")\n",
    "        fission_count = sum(1 for p in preds if p[\"decision\"] == \"fission\")\n",
    "\n",
    "        # Compute error rate\n",
    "        errors = sum(1 for p in preds if p[\"error\"])\n",
    "        error_rate = errors / len(preds)\n",
    "\n",
    "        # API calls (fusion=1, fission=2)\n",
    "        api_calls = fusion_count + 2 * fission_count\n",
    "\n",
    "        metrics[method] = {\n",
    "            \"fusion_rate\": fusion_count / len(preds),\n",
    "            \"fission_rate\": fission_count / len(preds),\n",
    "            \"error_rate\": error_rate,\n",
    "            \"api_calls\": api_calls,\n",
    "            \"avg_calls_per_example\": api_calls / len(preds),\n",
    "        }\n",
    "\n",
    "    # Compute improvement\n",
    "    baseline_calls = metrics[\"baseline\"][\"avg_calls_per_example\"]\n",
    "    proposed_calls = metrics[\"proposed\"][\"avg_calls_per_example\"]\n",
    "    metrics[\"improvement\"] = {\n",
    "        \"api_reduction_pct\": (baseline_calls - proposed_calls) / baseline_calls * 100,\n",
    "        \"error_rate_diff\": metrics[\"proposed\"][\"error_rate\"] - metrics[\"baseline\"][\"error_rate\"],\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-title-cell",
   "metadata": {},
   "source": [
    "## Run Evaluation\n",
    "\n",
    "Compute the metrics and display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics using our sample data\n",
    "metrics = compute_metrics(results)\n",
    "\n",
    "# Display key improvement metric\n",
    "print(f\"API reduction: {metrics['improvement']['api_reduction_pct']:.1f}%\")\n",
    "print(f\"Error rate change: {metrics['improvement']['error_rate_diff']:.3f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-title-cell",
   "metadata": {},
   "source": [
    "## Detailed Results\n",
    "\n",
    "Let's examine the complete metrics breakdown for both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed metrics in a readable format\n",
    "print(\"=\" * 60)\n",
    "print(\"DETAILED EVALUATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for method in [\"baseline\", \"proposed\"]:\n",
    "    print(f\"\\n{method.upper()} METHOD:\")\n",
    "    print(f\"  Fusion rate:        {metrics[method]['fusion_rate']:.2%}\")\n",
    "    print(f\"  Fission rate:       {metrics[method]['fission_rate']:.2%}\")\n",
    "    print(f\"  Error rate:         {metrics[method]['error_rate']:.2%}\")\n",
    "    print(f\"  Total API calls:    {metrics[method]['api_calls']:,}\")\n",
    "    print(f\"  Avg calls/example:  {metrics[method]['avg_calls_per_example']:.2f}\")\n",
    "\n",
    "print(f\"\\nIMPROVEMENT SUMMARY:\")\n",
    "print(f\"  API reduction:      {metrics['improvement']['api_reduction_pct']:.1f}%\")\n",
    "print(f\"  Error rate change:  {metrics['improvement']['error_rate_diff']:+.3f}\")\n",
    "\n",
    "# Save results to match original script behavior (optional)\n",
    "output_data = metrics\n",
    "print(f\"\\nResults computed successfully!\")\n",
    "print(f\"Equivalent to saving to 'eval_out.json':\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "json-output-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the JSON output that would be saved to eval_out.json\n",
    "print(json.dumps(metrics, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "customization-cell",
   "metadata": {},
   "source": [
    "## Customization\n",
    "\n",
    "To modify this evaluation:\n",
    "\n",
    "1. **Change the sample data**: Edit the data generation cell to use your own experimental results\n",
    "2. **Adjust metrics**: Modify the `compute_metrics` function to add new evaluation criteria\n",
    "3. **Add visualizations**: Use matplotlib/seaborn to create charts from the metrics\n",
    "4. **Scale the analysis**: Increase the number of test examples or add new methods\n",
    "\n",
    "This notebook is completely self-contained and doesn't require any external files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}