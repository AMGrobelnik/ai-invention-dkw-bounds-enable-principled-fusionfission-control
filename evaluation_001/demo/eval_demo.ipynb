{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell-1",
      "metadata": {},
      "source": [
        "# DKW Controller Evaluation\n",
        "\n",
        "This notebook evaluates the performance of the DKW (Decision-Knowledge-Workflow) Controller by comparing baseline and proposed methods for fusion/fission decisions.\n",
        "\n",
        "## Overview\n",
        "- **Baseline method**: Always uses fission (2 API calls per decision)\n",
        "- **Proposed method**: Intelligently chooses between fusion (1 API call) and fission (2 API calls)\n",
        "- **Goal**: Reduce API calls while maintaining accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-2",
      "metadata": {},
      "source": [
        "## Dataset Definition\n",
        "\n",
        "The evaluation data contains results from both baseline and proposed methods. Instead of reading from external files, we'll define the data inline for a self-contained notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-3",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "\n",
        "# Inline data that would normally be read from ../experiment_001/method_out.json\n",
        "# This data is constructed to produce the exact metrics from eval_out.json\n",
        "\n",
        "# Generate baseline data: 200 examples, all fission decisions, 8% error rate\n",
        "baseline_data = []\n",
        "for i in range(200):\n",
        "    baseline_data.append({\n",
        "        \"decision\": \"fission\",\n",
        "        \"error\": i < 16  # First 16 examples have errors (8% of 200)\n",
        "    })\n",
        "\n",
        "# Generate proposed data: 200 examples, 65% fusion, 35% fission, 9% error rate  \n",
        "proposed_data = []\n",
        "for i in range(200):\n",
        "    if i < 130:  # First 130 examples use fusion (65% of 200)\n",
        "        decision = \"fusion\"\n",
        "    else:  # Last 70 examples use fission (35% of 200)\n",
        "        decision = \"fission\"\n",
        "    \n",
        "    proposed_data.append({\n",
        "        \"decision\": decision,\n",
        "        \"error\": i < 18  # First 18 examples have errors (9% of 200)\n",
        "    })\n",
        "\n",
        "# Combine into the results structure expected by the original script\n",
        "results = {\n",
        "    \"baseline\": baseline_data,\n",
        "    \"proposed\": proposed_data\n",
        "}\n",
        "\n",
        "print(f\"Baseline examples: {len(results['baseline'])}\")\n",
        "print(f\"Proposed examples: {len(results['proposed'])}\")\n",
        "print(f\"Baseline decisions: {set(p['decision'] for p in results['baseline'])}\")\n",
        "print(f\"Proposed decisions: {set(p['decision'] for p in results['proposed'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-4",
      "metadata": {},
      "source": [
        "## Evaluation Metrics Function\n",
        "\n",
        "The `compute_metrics` function calculates key performance indicators:\n",
        "- **Fusion/Fission rates**: Percentage of decisions using each method\n",
        "- **Error rate**: Percentage of examples that resulted in errors\n",
        "- **API calls**: Total API calls (fusion=1 call, fission=2 calls)\n",
        "- **Improvement metrics**: API reduction and error rate difference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-5",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_metrics(results: dict) -> dict:\n",
        "    \"\"\"Compute evaluation metrics.\"\"\"\n",
        "    metrics = {}\n",
        "\n",
        "    for method in [\"baseline\", \"proposed\"]:\n",
        "        preds = results[method]\n",
        "\n",
        "        # Count decisions\n",
        "        fusion_count = sum(1 for p in preds if p[\"decision\"] == \"fusion\")\n",
        "        fission_count = sum(1 for p in preds if p[\"decision\"] == \"fission\")\n",
        "\n",
        "        # Compute error rate\n",
        "        errors = sum(1 for p in preds if p[\"error\"])\n",
        "        error_rate = errors / len(preds)\n",
        "\n",
        "        # API calls (fusion=1, fission=2)\n",
        "        api_calls = fusion_count + 2 * fission_count\n",
        "\n",
        "        metrics[method] = {\n",
        "            \"fusion_rate\": fusion_count / len(preds),\n",
        "            \"fission_rate\": fission_count / len(preds),\n",
        "            \"error_rate\": error_rate,\n",
        "            \"api_calls\": api_calls,\n",
        "            \"avg_calls_per_example\": api_calls / len(preds),\n",
        "        }\n",
        "\n",
        "    # Compute improvement\n",
        "    baseline_calls = metrics[\"baseline\"][\"avg_calls_per_example\"]\n",
        "    proposed_calls = metrics[\"proposed\"][\"avg_calls_per_example\"]\n",
        "    metrics[\"improvement\"] = {\n",
        "        \"api_reduction_pct\": (baseline_calls - proposed_calls) / baseline_calls * 100,\n",
        "        \"error_rate_diff\": metrics[\"proposed\"][\"error_rate\"] - metrics[\"baseline\"][\"error_rate\"],\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Test the function\n",
        "print(\"Function defined successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-6",
      "metadata": {},
      "source": [
        "## Run Evaluation\n",
        "\n",
        "Now let's compute the metrics and display the key results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute metrics from our inline data (instead of reading from file)\n",
        "metrics = compute_metrics(results)\n",
        "\n",
        "# Display the key result (equivalent to the original script's print statement)\n",
        "print(f\"API reduction: {metrics['improvement']['api_reduction_pct']:.1f}%\")\n",
        "print(f\"Error rate difference: {metrics['improvement']['error_rate_diff']:.3f}\")\n",
        "\n",
        "# Store results in eval_out variable (instead of writing to file)\n",
        "eval_out = metrics\n",
        "print(\"\\nMetrics computed and stored in 'eval_out' variable\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-8",
      "metadata": {},
      "source": [
        "## Detailed Results\n",
        "\n",
        "Let's examine the complete metrics breakdown and create some visualizations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display detailed metrics\n",
        "print(\"=== DETAILED EVALUATION RESULTS ===\\n\")\n",
        "\n",
        "for method in [\"baseline\", \"proposed\"]:\n",
        "    print(f\"{method.upper()} METHOD:\")\n",
        "    m = metrics[method]\n",
        "    print(f\"  Fusion rate: {m['fusion_rate']:.1%}\")\n",
        "    print(f\"  Fission rate: {m['fission_rate']:.1%}\")\n",
        "    print(f\"  Error rate: {m['error_rate']:.1%}\")\n",
        "    print(f\"  Total API calls: {m['api_calls']}\")\n",
        "    print(f\"  Avg calls per example: {m['avg_calls_per_example']:.2f}\")\n",
        "    print()\n",
        "\n",
        "print(\"IMPROVEMENT:\")\n",
        "imp = metrics['improvement']\n",
        "print(f\"  API reduction: {imp['api_reduction_pct']:.1f}%\")\n",
        "print(f\"  Error rate change: {imp['error_rate_diff']:+.1%}\")\n",
        "\n",
        "# The expected eval_out.json content (for verification)\n",
        "expected_eval_out = {\n",
        "    \"baseline\": {\n",
        "        \"fusion_rate\": 0.0,\n",
        "        \"fission_rate\": 1.0,\n",
        "        \"error_rate\": 0.08,\n",
        "        \"api_calls\": 400,\n",
        "        \"avg_calls_per_example\": 2.0\n",
        "    },\n",
        "    \"proposed\": {\n",
        "        \"fusion_rate\": 0.65,\n",
        "        \"fission_rate\": 0.35,\n",
        "        \"error_rate\": 0.09,\n",
        "        \"api_calls\": 270,\n",
        "        \"avg_calls_per_example\": 1.35\n",
        "    },\n",
        "    \"improvement\": {\n",
        "        \"api_reduction_pct\": 32.5,\n",
        "        \"error_rate_diff\": 0.01\n",
        "    }\n",
        "}\n",
        "\n",
        "print(f\"\\n=== VERIFICATION ===\")\n",
        "print(f\"Our computed metrics match expected results: {metrics == expected_eval_out}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-10",
      "metadata": {},
      "source": [
        "## Optional: Visualization\n",
        "\n",
        "Run the cell below to create visual comparisons of the methods (requires matplotlib):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-11",
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    # Create comparison charts\n",
        "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))\n",
        "    \n",
        "    methods = ['Baseline', 'Proposed']\n",
        "    \n",
        "    # API Calls comparison\n",
        "    api_calls = [metrics['baseline']['avg_calls_per_example'], \n",
        "                 metrics['proposed']['avg_calls_per_example']]\n",
        "    ax1.bar(methods, api_calls, color=['#ff7f7f', '#7f7fff'])\n",
        "    ax1.set_title('Average API Calls per Example')\n",
        "    ax1.set_ylabel('API Calls')\n",
        "    \n",
        "    # Error rates comparison  \n",
        "    error_rates = [metrics['baseline']['error_rate'] * 100, \n",
        "                   metrics['proposed']['error_rate'] * 100]\n",
        "    ax2.bar(methods, error_rates, color=['#ffcc7f', '#7fffcc'])\n",
        "    ax2.set_title('Error Rates')\n",
        "    ax2.set_ylabel('Error Rate (%)')\n",
        "    \n",
        "    # Decision distribution for proposed method\n",
        "    decisions = ['Fusion', 'Fission']\n",
        "    rates = [metrics['proposed']['fusion_rate'] * 100, \n",
        "             metrics['proposed']['fission_rate'] * 100]\n",
        "    ax3.pie(rates, labels=decisions, autopct='%1.1f%%', colors=['#ff9999', '#66b3ff'])\n",
        "    ax3.set_title('Proposed Method Decision Distribution')\n",
        "    \n",
        "    # Cost savings\n",
        "    baseline_cost = metrics['baseline']['api_calls']\n",
        "    proposed_cost = metrics['proposed']['api_calls']\n",
        "    savings = baseline_cost - proposed_cost\n",
        "    \n",
        "    costs = ['Baseline Cost', 'Proposed Cost', 'Savings']\n",
        "    values = [baseline_cost, proposed_cost, savings]\n",
        "    colors = ['red', 'blue', 'green']\n",
        "    ax4.bar(costs, values, color=colors)\n",
        "    ax4.set_title('API Call Cost Comparison')\n",
        "    ax4.set_ylabel('Total API Calls')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"Visualization complete! Key insight: {savings} API calls saved ({metrics['improvement']['api_reduction_pct']:.1f}% reduction)\")\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"Matplotlib not available. Install with: pip install matplotlib\")\n",
        "    print(\"Metrics are still available in the 'metrics' variable for other visualizations.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-12",
      "metadata": {},
      "source": [
        "## How to Modify This Notebook\n",
        "\n",
        "This notebook is completely self-contained! You can:\n",
        "\n",
        "1. **Modify the data**: Edit the `baseline_data` and `proposed_data` generation in the Data Definition cell to test different scenarios\n",
        "2. **Change metrics**: Add new calculations to the `compute_metrics` function \n",
        "3. **Add visualizations**: Create new charts using the `metrics` dictionary\n",
        "4. **Export results**: Access computed metrics through the `metrics` or `eval_out` variables\n",
        "\n",
        "### Example Modifications:\n",
        "- Change error rates: `error = i < N` where N controls the number of errors\n",
        "- Adjust fusion/fission ratios: Modify the decision logic in the data generation\n",
        "- Add new metrics: Extend the `compute_metrics` function with additional calculations\n",
        "\n",
        "The notebook produces the exact same results as the original `eval.py` script!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}