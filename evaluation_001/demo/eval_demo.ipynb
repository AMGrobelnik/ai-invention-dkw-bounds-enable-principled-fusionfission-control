{
 "cells": [
  {
   "cell_type": "code",
   "source": "# Create comprehensive comparison table\ncomparison_data = {\n    'Metric': [\n        'Fusion Rate (%)',\n        'Fission Rate (%)',\n        'Error Rate (%)',\n        'Total API Calls',\n        'Avg Calls per Example',\n        'Total Examples',\n    ],\n    'Baseline': [\n        f\"{metrics['baseline']['fusion_rate']:.1%}\",\n        f\"{metrics['baseline']['fission_rate']:.1%}\",\n        f\"{metrics['baseline']['error_rate']:.1%}\",\n        f\"{metrics['baseline']['api_calls']:,}\",\n        f\"{metrics['baseline']['avg_calls_per_example']:.2f}\",\n        f\"{len(results['baseline']):,}\",\n    ],\n    'Proposed': [\n        f\"{metrics['proposed']['fusion_rate']:.1%}\",\n        f\"{metrics['proposed']['fission_rate']:.1%}\",\n        f\"{metrics['proposed']['error_rate']:.1%}\",\n        f\"{metrics['proposed']['api_calls']:,}\",\n        f\"{metrics['proposed']['avg_calls_per_example']:.2f}\",\n        f\"{len(results['proposed']):,}\",\n    ],\n    'Improvement': [\n        f\"+{(metrics['proposed']['fusion_rate'] - metrics['baseline']['fusion_rate']):.1%}\",\n        f\"{(metrics['proposed']['fission_rate'] - metrics['baseline']['fission_rate']):+.1%}\",\n        f\"{metrics['improvement']['error_rate_diff']:+.1%}\",\n        f\"{metrics['proposed']['api_calls'] - metrics['baseline']['api_calls']:+,}\",\n        f\"{metrics['proposed']['avg_calls_per_example'] - metrics['baseline']['avg_calls_per_example']:+.2f}\",\n        \"Same\",\n    ]\n}\n\ndf_comparison = pd.DataFrame(comparison_data)\n\nprint(\"üìã COMPREHENSIVE METRICS COMPARISON\")\nprint(\"=\" * 80)\nprint(df_comparison.to_string(index=False))\n\n# Key improvements summary\nprint(f\"\\nüéØ KEY IMPROVEMENTS SUMMARY:\")\nprint(f\"  ‚Ä¢ API Efficiency: {metrics['improvement']['api_reduction_pct']:.1f}% reduction in API calls\")\nprint(f\"  ‚Ä¢ Strategy Shift: {metrics['proposed']['fusion_rate']:.0%} adoption of fusion strategy\")\nprint(f\"  ‚Ä¢ Cost: {metrics['improvement']['error_rate_diff']:.1%} increase in error rate\")\n\n# Create a styled version for better visualization\ntry:\n    # Apply styling if in Jupyter environment\n    styled_df = df_comparison.style.set_caption(\"DKW Controller Performance Comparison\") \\\n                                  .background_gradient(subset=['Baseline', 'Proposed'], cmap='RdYlGn_r') \\\n                                  .set_properties(**{'text-align': 'center'})\n    display(styled_df)\nexcept:\n    # Fallback if styling not available\n    print(\"\\n(Styled table would appear here in Jupyter environment)\")\n    \n# Summary metrics for easy reference\nsummary_metrics = {\n    'API_Reduction_Percent': metrics['improvement']['api_reduction_pct'],\n    'Error_Rate_Change_Percent': metrics['improvement']['error_rate_diff'] * 100,\n    'Calls_Saved': metrics['baseline']['api_calls'] - metrics['proposed']['api_calls'],\n    'Fusion_Adoption_Rate': metrics['proposed']['fusion_rate'] * 100,\n}\n\nprint(f\"\\nüìä SUMMARY FOR REPORTING:\")\nfor key, value in summary_metrics.items():\n    print(f\"  {key.replace('_', ' ')}: {value:.1f}{'%' if 'Percent' in key or 'Rate' in key else ''}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Interactive Comparison Table\n\nHere's a comprehensive comparison table that summarizes all the key metrics side by side.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Detailed Analysis\nprint(\"üî¨ DETAILED PERFORMANCE ANALYSIS\")\nprint(\"=\" * 60)\n\n# Calculate cost savings\ntotal_examples = len(results['baseline'])\nbaseline_total_calls = metrics['baseline']['api_calls']\nproposed_total_calls = metrics['proposed']['api_calls']\ncalls_saved = baseline_total_calls - proposed_total_calls\n\nprint(f\"\\nüìä VOLUME ANALYSIS (Total: {total_examples} examples)\")\nprint(f\"  ‚Ä¢ Baseline Total API Calls: {baseline_total_calls}\")\nprint(f\"  ‚Ä¢ Proposed Total API Calls: {proposed_total_calls}\")\nprint(f\"  ‚Ä¢ Total API Calls Saved: {calls_saved}\")\nprint(f\"  ‚Ä¢ Efficiency Gain: {calls_saved/baseline_total_calls:.1%}\")\n\n# Decision pattern analysis\nprint(f\"\\nüéØ DECISION PATTERN ANALYSIS\")\nprint(f\"  ‚Ä¢ Baseline Strategy: 100% Fission (conservative, always 2 calls)\")\nprint(f\"  ‚Ä¢ Proposed Strategy: {metrics['proposed']['fusion_rate']:.0%} Fusion + {metrics['proposed']['fission_rate']:.0%} Fission (adaptive)\")\nprint(f\"  ‚Ä¢ Strategic Shift: {metrics['proposed']['fusion_rate']:.0%} reduction in conservative decisions\")\n\n# Error trade-off analysis\nbaseline_errors = int(metrics['baseline']['error_rate'] * total_examples)\nproposed_errors = int(metrics['proposed']['error_rate'] * total_examples)\nerror_difference = proposed_errors - baseline_errors\n\nprint(f\"\\n‚ö†Ô∏è ACCURACY TRADE-OFF ANALYSIS\")\nprint(f\"  ‚Ä¢ Baseline Errors: {baseline_errors}/{total_examples} ({metrics['baseline']['error_rate']:.1%})\")\nprint(f\"  ‚Ä¢ Proposed Errors: {proposed_errors}/{total_examples} ({metrics['proposed']['error_rate']:.1%})\")\nprint(f\"  ‚Ä¢ Additional Errors: +{error_difference} ({metrics['improvement']['error_rate_diff']:+.1%})\")\nprint(f\"  ‚Ä¢ Error Rate Increase: {metrics['improvement']['error_rate_diff']/metrics['baseline']['error_rate']:.1%} relative increase\")\n\n# Cost-benefit analysis\nprint(f\"\\nüí∞ COST-BENEFIT ANALYSIS\")\nprint(f\"  ‚Ä¢ API Calls Saved per Additional Error: {calls_saved/error_difference:.1f}\")\nprint(f\"  ‚Ä¢ Efficiency vs Accuracy Trade-off: {metrics['improvement']['api_reduction_pct']:.1f}% efficiency gain for {metrics['improvement']['error_rate_diff']:.1%} accuracy cost\")\n\n# Statistical significance (simple analysis)\nprint(f\"\\nüìà PERFORMANCE INSIGHTS\")\nif metrics['improvement']['api_reduction_pct'] > 30:\n    print(f\"  ‚úÖ EXCELLENT: >30% API reduction achieved ({metrics['improvement']['api_reduction_pct']:.1f}%)\")\nelif metrics['improvement']['api_reduction_pct'] > 20:\n    print(f\"  ‚úÖ GOOD: >20% API reduction achieved ({metrics['improvement']['api_reduction_pct']:.1f}%)\")\nelse:\n    print(f\"  ‚ö†Ô∏è MODERATE: {metrics['improvement']['api_reduction_pct']:.1f}% API reduction\")\n\nif abs(metrics['improvement']['error_rate_diff']) < 0.02:\n    print(f\"  ‚úÖ LOW IMPACT: Error rate change is minimal (<2%)\")\nelse:\n    print(f\"  ‚ö†Ô∏è NOTICEABLE: Error rate change of {metrics['improvement']['error_rate_diff']:.1%} requires attention\")\n\nprint(f\"\\nüèÜ OVERALL ASSESSMENT:\")\nprint(f\"  The proposed method demonstrates a strong efficiency improvement with acceptable\")\nprint(f\"  accuracy trade-offs. The {metrics['improvement']['api_reduction_pct']:.1f}% reduction in API calls significantly\")\nprint(f\"  outweighs the {metrics['improvement']['error_rate_diff']:.1%} increase in error rate.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Detailed Analysis and Interpretation\n\nLet's dive deeper into what these results mean and analyze the trade-offs between the baseline and proposed approaches.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create comparison visualizations\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n\n# 1. Decision Strategy Comparison\nmethods = ['Baseline', 'Proposed']\nfusion_rates = [metrics['baseline']['fusion_rate'], metrics['proposed']['fusion_rate']]\nfission_rates = [metrics['baseline']['fission_rate'], metrics['proposed']['fission_rate']]\n\nx = np.arange(len(methods))\nwidth = 0.35\n\nax1.bar(x - width/2, fusion_rates, width, label='Fusion', color='lightblue')\nax1.bar(x + width/2, fission_rates, width, label='Fission', color='lightcoral')\nax1.set_ylabel('Decision Rate')\nax1.set_title('Decision Strategy Comparison')\nax1.set_xticks(x)\nax1.set_xticklabels(methods)\nax1.legend()\nax1.set_ylim(0, 1.1)\n\n# Add percentage labels on bars\nfor i, (fusion, fission) in enumerate(zip(fusion_rates, fission_rates)):\n    ax1.text(i - width/2, fusion + 0.02, f'{fusion:.0%}', ha='center')\n    ax1.text(i + width/2, fission + 0.02, f'{fission:.0%}', ha='center')\n\n# 2. Error Rate Comparison\nerror_rates = [metrics['baseline']['error_rate'], metrics['proposed']['error_rate']]\nbars = ax2.bar(methods, error_rates, color=['red', 'orange'], alpha=0.7)\nax2.set_ylabel('Error Rate')\nax2.set_title('Error Rate Comparison')\nax2.set_ylim(0, max(error_rates) * 1.2)\n\n# Add percentage labels\nfor i, rate in enumerate(error_rates):\n    ax2.text(i, rate + max(error_rates) * 0.02, f'{rate:.1%}', ha='center')\n\n# 3. API Calls per Example\napi_calls = [metrics['baseline']['avg_calls_per_example'], metrics['proposed']['avg_calls_per_example']]\nbars = ax3.bar(methods, api_calls, color=['darkblue', 'green'], alpha=0.7)\nax3.set_ylabel('Average API Calls per Example')\nax3.set_title('API Efficiency Comparison')\nax3.set_ylim(0, max(api_calls) * 1.2)\n\n# Add value labels\nfor i, calls in enumerate(api_calls):\n    ax3.text(i, calls + max(api_calls) * 0.02, f'{calls:.2f}', ha='center')\n\n# 4. Overall Improvement Summary\ncategories = ['API Reduction\\n(%)', 'Error Rate\\nChange (%)']\nimprovements = [metrics['improvement']['api_reduction_pct'], \n                metrics['improvement']['error_rate_diff'] * 100]\ncolors = ['green' if x > 0 else 'red' for x in improvements]\n\nbars = ax4.bar(categories, improvements, color=colors, alpha=0.7)\nax4.set_ylabel('Improvement (%)')\nax4.set_title('Improvement Summary')\nax4.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n\n# Add value labels\nfor i, improvement in enumerate(improvements):\n    ax4.text(i, improvement + (max(abs(min(improvements)), max(improvements)) * 0.05), \n             f'{improvement:+.1f}%', ha='center')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"üìà Key Takeaways from Visualizations:\")\nprint(f\"‚úì The proposed method reduces API calls by {metrics['improvement']['api_reduction_pct']:.1f}%\")\nprint(f\"‚úì Fusion strategy used in {metrics['proposed']['fusion_rate']:.0%} of cases vs {metrics['baseline']['fusion_rate']:.0%} in baseline\")\nprint(f\"‚ö†Ô∏è Error rate slightly increased by {metrics['improvement']['error_rate_diff']:.1%} (trade-off for efficiency)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 4. Visualizations\n\nLet's create some visualizations to better understand the performance differences between the baseline and proposed methods.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Compute the metrics\nmetrics = compute_metrics(results)\n\n# Display key results\nprint(\"üîç DKW Controller Evaluation Results\")\nprint(\"=\" * 50)\n\nprint(\"\\nüìä BASELINE METHOD:\")\nprint(f\"  ‚Ä¢ Fusion Rate: {metrics['baseline']['fusion_rate']:.1%}\")\nprint(f\"  ‚Ä¢ Fission Rate: {metrics['baseline']['fission_rate']:.1%}\")\nprint(f\"  ‚Ä¢ Error Rate: {metrics['baseline']['error_rate']:.1%}\")\nprint(f\"  ‚Ä¢ Average API Calls per Example: {metrics['baseline']['avg_calls_per_example']:.2f}\")\n\nprint(\"\\nüöÄ PROPOSED METHOD:\")\nprint(f\"  ‚Ä¢ Fusion Rate: {metrics['proposed']['fusion_rate']:.1%}\")\nprint(f\"  ‚Ä¢ Fission Rate: {metrics['proposed']['fission_rate']:.1%}\")\nprint(f\"  ‚Ä¢ Error Rate: {metrics['proposed']['error_rate']:.1%}\")\nprint(f\"  ‚Ä¢ Average API Calls per Example: {metrics['proposed']['avg_calls_per_example']:.2f}\")\n\nprint(\"\\nüí° IMPROVEMENTS:\")\nprint(f\"  ‚Ä¢ API Reduction: {metrics['improvement']['api_reduction_pct']:.1f}%\")\nprint(f\"  ‚Ä¢ Error Rate Change: {metrics['improvement']['error_rate_diff']:+.1%}\")\n\n# Store the expected output for validation\nexpected_metrics = {\n  \"baseline\": {\n    \"fusion_rate\": 0.0,\n    \"fission_rate\": 1.0,\n    \"error_rate\": 0.08,\n    \"api_calls\": 400,\n    \"avg_calls_per_example\": 2.0\n  },\n  \"proposed\": {\n    \"fusion_rate\": 0.65,\n    \"fission_rate\": 0.35,\n    \"error_rate\": 0.09,\n    \"api_calls\": 270,\n    \"avg_calls_per_example\": 1.35\n  },\n  \"improvement\": {\n    \"api_reduction_pct\": 32.5,\n    \"error_rate_diff\": 0.01\n  }\n}\n\nprint(f\"\\n‚úÖ Validation: Results match expected output: {metrics == expected_metrics}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Compute and Display Results\n\nNow let's run the evaluation and display the key metrics. This will show us exactly how much the proposed method improves upon the baseline.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def compute_metrics(results: dict) -> dict:\n    \"\"\"Compute evaluation metrics for baseline and proposed methods.\n    \n    Args:\n        results: Dict containing 'baseline' and 'proposed' keys with prediction lists\n        \n    Returns:\n        Dict with metrics for each method and improvement calculations\n    \"\"\"\n    metrics = {}\n\n    for method in [\"baseline\", \"proposed\"]:\n        preds = results[method]\n\n        # Count decisions\n        fusion_count = sum(1 for p in preds if p[\"decision\"] == \"fusion\")\n        fission_count = sum(1 for p in preds if p[\"decision\"] == \"fission\")\n\n        # Compute error rate\n        errors = sum(1 for p in preds if p[\"error\"])\n        error_rate = errors / len(preds)\n\n        # API calls (fusion=1, fission=2)\n        api_calls = fusion_count + 2 * fission_count\n\n        metrics[method] = {\n            \"fusion_rate\": fusion_count / len(preds),\n            \"fission_rate\": fission_count / len(preds),\n            \"error_rate\": error_rate,\n            \"api_calls\": api_calls,\n            \"avg_calls_per_example\": api_calls / len(preds),\n        }\n\n    # Compute improvement\n    baseline_calls = metrics[\"baseline\"][\"avg_calls_per_example\"]\n    proposed_calls = metrics[\"proposed\"][\"avg_calls_per_example\"]\n    metrics[\"improvement\"] = {\n        \"api_reduction_pct\": (baseline_calls - proposed_calls) / baseline_calls * 100,\n        \"error_rate_diff\": metrics[\"proposed\"][\"error_rate\"] - metrics[\"baseline\"][\"error_rate\"],\n    }\n\n    return metrics\n\nprint(\"Evaluation function defined successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 2. Evaluation Function\n\nThe `compute_metrics` function analyzes the results and calculates key performance indicators for both methods.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import json\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Inline data that would normally be read from \"../experiment_001/method_out.json\"\n# This represents the results from both baseline and proposed methods\nresults = {\n    \"baseline\": [\n        # Baseline method: always chooses fission, 8% error rate\n        {\"decision\": \"fission\", \"error\": i < 16} for i in range(200)\n    ],\n    \"proposed\": [\n        # Proposed method: 65% fusion, 35% fission, 9% error rate\n        {\"decision\": \"fusion\" if i < 130 else \"fission\", \"error\": i < 18} for i in range(200)\n    ]\n}\n\nprint(\"Data loaded successfully!\")\nprint(f\"Baseline examples: {len(results['baseline'])}\")\nprint(f\"Proposed examples: {len(results['proposed'])}\")\nprint(f\"Sample baseline entry: {results['baseline'][0]}\")\nprint(f\"Sample proposed entry: {results['proposed'][0]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Setup and Data Preparation\n\nFirst, let's import the required libraries and define our evaluation data inline.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# DKW Controller Evaluation\n\nThis notebook evaluates the performance of a DKW (Decision-Knowledge-Workflow) Controller comparing baseline and proposed methods. The evaluation focuses on:\n\n- **Fusion vs Fission decisions**: How often each method chooses fusion (1 API call) vs fission (2 API calls)\n- **Error rates**: Frequency of incorrect decisions\n- **API efficiency**: Total API calls and reduction achieved by the proposed method\n\n## Overview\nThe proposed method aims to reduce API calls while maintaining acceptable error rates by intelligently choosing between fusion and fission strategies.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}