{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## Analysis Summary\n\n**Key Findings:**\n\n1. **API Efficiency**: The proposed method achieves a **32.5% reduction** in API calls compared to baseline\n   - Baseline: 2.0 calls per example (all fission decisions)\n   - Proposed: 1.35 calls per example (mix of fusion/fission)\n\n2. **Decision Strategy**: The proposed method uses a mixed strategy:\n   - 65% fusion decisions (more efficient, 1 API call each)\n   - 35% fission decisions (less efficient, 2 API calls each) \n\n3. **Accuracy Trade-off**: There's a small increase in error rate (1% higher) but significant API savings\n\n4. **Overall**: The proposed method successfully balances efficiency and accuracy, making it practical for production use where API costs are a concern.\n\n**To modify this notebook**: Update the data generation section to experiment with different decision strategies and error rates.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Display full metrics in formatted JSON (equivalent to eval_out.json)\nprint(\"Complete evaluation metrics:\")\nprint(\"=\" * 40)\nprint(json.dumps(metrics, indent=2))\n\n# Also save to file (optional, as in original script)\nwith open(\"eval_out.json\", \"w\") as f:\n    json.dump(metrics, f, indent=2)\n    \nprint(f\"\\nMetrics also saved to 'eval_out.json'\")    ",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Detailed Results\n\nHere are the complete evaluation metrics (equivalent to the original `eval_out.json` output):",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Compute evaluation metrics\nmetrics = compute_metrics(results)\n\n# Display the key result that the original script printed\nprint(f\"API reduction: {metrics['improvement']['api_reduction_pct']:.1f}%\")\nprint(f\"Error rate difference: {metrics['improvement']['error_rate_diff']:.2f}\")\nprint()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Run Analysis\n\nNow we'll compute the evaluation metrics using our sample data and display the results. This replaces the original script's file I/O operations with direct computation.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def compute_metrics(results: dict) -> dict:\n    \"\"\"Compute evaluation metrics.\"\"\"\n    metrics = {}\n\n    for method in [\"baseline\", \"proposed\"]:\n        preds = results[method]\n\n        # Count decisions\n        fusion_count = sum(1 for p in preds if p[\"decision\"] == \"fusion\")\n        fission_count = sum(1 for p in preds if p[\"decision\"] == \"fission\")\n\n        # Compute error rate\n        errors = sum(1 for p in preds if p[\"error\"])\n        error_rate = errors / len(preds)\n\n        # API calls (fusion=1, fission=2)\n        api_calls = fusion_count + 2 * fission_count\n\n        metrics[method] = {\n            \"fusion_rate\": fusion_count / len(preds),\n            \"fission_rate\": fission_count / len(preds),\n            \"error_rate\": error_rate,\n            \"api_calls\": api_calls,\n            \"avg_calls_per_example\": api_calls / len(preds),\n        }\n\n    # Compute improvement\n    baseline_calls = metrics[\"baseline\"][\"avg_calls_per_example\"]\n    proposed_calls = metrics[\"proposed\"][\"avg_calls_per_example\"]\n    metrics[\"improvement\"] = {\n        \"api_reduction_pct\": (baseline_calls - proposed_calls) / baseline_calls * 100,\n        \"error_rate_diff\": metrics[\"proposed\"][\"error_rate\"] - metrics[\"baseline\"][\"error_rate\"],\n    }\n\n    return metrics",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Evaluation Metrics Function\n\nThe `compute_metrics` function analyzes prediction results and calculates key performance indicators:\n\n**For each method (baseline/proposed):**\n- **Fusion rate**: Proportion of decisions that were \"fusion\" (1 API call each)\n- **Fission rate**: Proportion of decisions that were \"fission\" (2 API calls each) \n- **Error rate**: Proportion of predictions that were incorrect\n- **Total API calls**: fusion_count × 1 + fission_count × 2\n- **Average calls per example**: Total API calls divided by number of examples\n\n**Overall improvement metrics:**\n- **API reduction percentage**: How much the proposed method reduces API usage vs baseline\n- **Error rate difference**: Change in error rate (proposed - baseline)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create sample data that matches the original eval_out.json results\n# Baseline: 200 examples, all fission, 8% error rate\nbaseline_predictions = []\nfor i in range(200):\n    error = i < 16  # First 16 examples have errors (8% of 200)\n    baseline_predictions.append({\n        \"decision\": \"fission\",\n        \"error\": error\n    })\n\n# Proposed: 200 examples, 65% fusion (130), 35% fission (70), 9% error rate\nproposed_predictions = []\nerror_indices = set(range(18))  # First 18 examples have errors (9% of 200)\n\n# Add fusion decisions (130 examples)\nfor i in range(130):\n    proposed_predictions.append({\n        \"decision\": \"fusion\",\n        \"error\": i in error_indices\n    })\n\n# Add fission decisions (70 examples)  \nfor i in range(130, 200):\n    proposed_predictions.append({\n        \"decision\": \"fission\",\n        \"error\": i in error_indices\n    })\n\n# Combined results data\nresults = {\n    \"baseline\": baseline_predictions,\n    \"proposed\": proposed_predictions\n}\n\nprint(f\"Created sample data:\")\nprint(f\"- Baseline: {len(results['baseline'])} predictions\")\nprint(f\"- Proposed: {len(results['proposed'])} predictions\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Experimental Data\n\nThe original script read from `../experiment_001/method_out.json`. For this self-contained notebook, we'll inline sample data that produces the same results as shown in the original `eval_out.json`.\n\nThe data contains prediction results for both baseline and proposed methods, where each prediction has:\n- `decision`: Either \"fusion\" (1 API call) or \"fission\" (2 API calls)  \n- `error`: Boolean indicating if the prediction was incorrect",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\"\"\"Import required libraries\"\"\"\nimport json\nimport numpy as np",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# DKW Controller Evaluation\n\nThis notebook evaluates the performance of DKW Controller methods, comparing baseline and proposed approaches in terms of API efficiency and error rates.\n\nThe original script analyzed results from experiments and computed key performance metrics including:\n- Fusion/fission decision rates\n- Error rates\n- API call efficiency\n- Performance improvements",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}